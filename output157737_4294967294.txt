./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	data_augmentation: True
	lr: 0.001
	mlp_depth: 3
	mlp_dropout: 0.1
	mlp_width: 512
	momentum: 0.1
	nonlinear_classifier: False
	num_domain_tokens: 16
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "a photo of a"
Number of context words (tokens): 4
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          step          step_time    
0.5644876325  0.5300353357  0.5590588235  0.5630885122  0.5727341965  0.5609756098  0.5949648278  0.6281481481  0.0000000000  5.9294605255  0             2.0259728432 
0.8727915194  0.8374558304  0.6931764706  0.7325800377  0.7063975628  0.6966463415  0.7919289152  0.8177777778  8.4805653710  4.7530968372  300           0.7149231315 
0.9010600707  0.8798586572  0.7138823529  0.7419962335  0.7273419650  0.7073170732  0.8119215106  0.8355555556  16.961130742  4.4295756427  600           0.7162063479 
0.9779151943  0.9717314488  0.7223529412  0.7495291902  0.7452399086  0.7347560976  0.8415401703  0.8503703704  25.441696113  4.2891494870  900           0.7164682563 
0.8992932862  0.8657243816  0.7303529412  0.7514124294  0.7475247525  0.7271341463  0.8163643095  0.8311111111  33.922261484  4.1964141726  1200          0.7175593241 
0.9620141343  0.9363957597  0.7303529412  0.7551789077  0.7600913938  0.7423780488  0.8204368752  0.8385185185  42.402826855  4.2136428404  1500          0.7173263717 
0.9734982332  0.9681978799  0.7430588235  0.7532956685  0.7779893374  0.7560975610  0.8422806368  0.8533333333  50.883392226  4.1277682416  1800          0.7167587066 
0.9531802120  0.9434628975  0.7435294118  0.7664783427  0.7776085301  0.7667682927  0.8289522399  0.8459259259  59.363957597  4.1491638335  2100          0.7171884696 
0.9734982332  0.9681978799  0.7440000000  0.7570621469  0.7768469155  0.7530487805  0.8248796742  0.8400000000  67.844522968  4.1668648942  2400          0.7169266033 
0.9876325088  0.9752650177  0.7524705882  0.7627118644  0.7882711348  0.7621951220  0.8556090337  0.8622222222  76.325088339  4.1302856882  2700          0.7163836185 
0.9885159011  0.9717314488  0.7543529412  0.7664783427  0.7936024372  0.7652439024  0.8567197334  0.8711111111  84.805653710  4.0930238072  3000          0.7174957983 
0.9920494700  0.9823321555  0.7524705882  0.7758945386  0.7962680883  0.7728658537  0.8611625324  0.8800000000  93.286219081  4.0932966105  3300          0.7157281923 
0.9858657244  0.9823321555  0.7515294118  0.7683615819  0.7901751714  0.7637195122  0.8422806368  0.8666666667  101.76678445  4.0903584886  3600          0.7165126435 
0.9717314488  0.9646643110  0.7534117647  0.7758945386  0.7871287129  0.7667682927  0.8422806368  0.8651851852  110.24734982  4.1536710660  3900          0.7161819736 
0.9823321555  0.9752650177  0.7458823529  0.7702448211  0.7977913176  0.7682926829  0.8489448352  0.8666666667  118.72791519  4.0682469583  4200          0.7166453671 
0.9964664311  0.9964664311  0.7534117647  0.7777777778  0.7993145468  0.7728658537  0.8681969641  0.8800000000  127.20848056  4.0832339994  4500          0.7164529483 
0.9849823322  0.9823321555  0.7548235294  0.7758945386  0.7966488957  0.7621951220  0.8467234358  0.8681481481  135.68904593  4.0725257643  4800          0.7160185504 
0.9425795053  0.9257950530  0.7557647059  0.7721280603  0.7909367860  0.7515243902  0.8319141059  0.8548148148  141.34275618  4.0396251106  5000          0.7165052617 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	data_augmentation: True
	lr: 0.001
	mlp_depth: 3
	mlp_dropout: 0.1
	mlp_width: 512
	momentum: 0.1
	nonlinear_classifier: False
	num_domain_tokens: 16
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "a photo of a"
Number of context words (tokens): 4
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          step          step_time    
0.6183745583  0.5865724382  0.5538823529  0.5574387947  0.5906321401  0.5731707317  0.6079229915  0.6340740741  0.0000000000  6.2620544434  0             1.9485824108 
0.9973498233  1.0000000000  0.5948235294  0.5649717514  0.7433358720  0.7240853659  0.7952610144  0.8103703704  8.4805653710  4.6555050047  300           0.7143615715 
0.9982332155  1.0000000000  0.5967058824  0.5555555556  0.7425742574  0.7347560976  0.7848944835  0.7866666667  16.961130742  4.1576935061  600           0.7142019582 
0.9982332155  1.0000000000  0.5774117647  0.5461393597  0.7212490480  0.7103658537  0.7541651240  0.7525925926  25.441696113  3.9733901652  900           0.7144863717 
0.9982332155  0.9964664311  0.5689411765  0.5480225989  0.7147753237  0.7103658537  0.7430581266  0.7392592593  33.922261484  3.9580506070  1200          0.7147843091 
0.9982332155  0.9964664311  0.5774117647  0.5555555556  0.7086824067  0.7073170732  0.7367641614  0.7362962963  42.402826855  3.8897530723  1500          0.7148904332 
0.9982332155  0.9964664311  0.5595294118  0.5273069680  0.6980198020  0.6920731707  0.7275083302  0.7333333333  50.883392226  3.8886557690  1800          0.7148712428 
0.9982332155  0.9964664311  0.5557647059  0.5216572505  0.6843107388  0.6783536585  0.6901147723  0.6948148148  59.363957597  3.8287623779  2100          0.7155190969 
0.9982332155  0.9964664311  0.5590588235  0.5386064030  0.7109672506  0.6951219512  0.7315808960  0.7362962963  67.844522968  3.8557884296  2400          0.7148774576 
0.9982332155  0.9964664311  0.5675294118  0.5404896422  0.7269611577  0.7118902439  0.7500925583  0.7451851852  76.325088339  3.8570388675  2700          0.7147521814 
0.9991166078  0.9964664311  0.5680000000  0.5404896422  0.6949733435  0.6966463415  0.6708626435  0.6622222222  84.805653710  3.8191768138  3000          0.7157819939 
0.9991166078  0.9964664311  0.5830588235  0.5630885122  0.7128712871  0.6920731707  0.7145501666  0.7140740741  93.286219081  3.7968408068  3300          0.7139655725 
0.9991166078  0.9964664311  0.5689411765  0.5461393597  0.7136329018  0.7088414634  0.7175120326  0.7244444444  101.76678445  3.7850890525  3600          0.7143240142 
0.9991166078  0.9964664311  0.5618823529  0.5291902072  0.6987814166  0.6951219512  0.6978896705  0.7022222222  110.24734982  3.7819208789  3900          0.7139095108 
0.9991166078  0.9964664311  0.5736470588  0.5517890772  0.7235338919  0.6996951220  0.7349129952  0.7392592593  118.72791519  3.7887548772  4200          0.7143805035 
0.9991166078  0.9964664311  0.5708235294  0.5517890772  0.7155369383  0.6996951220  0.7175120326  0.7125925926  127.20848056  3.7722945611  4500          0.7145753161 
0.9991166078  0.9964664311  0.5741176471  0.5442561205  0.7010662605  0.6966463415  0.6927064050  0.6829629630  135.68904593  3.7680976160  4800          0.7146455940 
0.9991166078  0.9964664311  0.5717647059  0.5461393597  0.7140137091  0.7012195122  0.7267678638  0.7200000000  141.34275618  3.7674508107  5000          0.7150363231 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	data_augmentation: True
	lr: 0.001
	mlp_depth: 3
	mlp_dropout: 0.1
	mlp_width: 512
	momentum: 0.1
	nonlinear_classifier: False
	num_domain_tokens: 16
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "a photo of a"
Number of context words (tokens): 4
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          step          step_time    
0.6289752650  0.5830388693  0.5698823529  0.5743879473  0.5833968012  0.5670731707  0.6108848575  0.6311111111  0.0000000000  6.2902970314  0             2.0986247063 
0.9982332155  1.0000000000  0.6847058824  0.6854990584  0.6801218583  0.6661585366  0.7908182155  0.8148148148  8.4805653710  4.6542726636  300           0.7141220323 
0.9982332155  1.0000000000  0.6715294118  0.6553672316  0.6686976390  0.6722560976  0.8048870789  0.8222222222  16.961130742  4.1054066396  600           0.7137295016 
0.9982332155  1.0000000000  0.6710588235  0.6647834275  0.6793602437  0.6676829268  0.8104405776  0.8370370370  25.441696113  3.9619431814  900           0.7140912398 
0.9982332155  1.0000000000  0.6776470588  0.6817325800  0.6744097487  0.6661585366  0.8137726768  0.8429629630  33.922261484  3.9197977233  1200          0.7145212110 
0.9982332155  1.0000000000  0.6301176471  0.5932203390  0.6831683168  0.6798780488  0.8026656794  0.8340740741  42.402826855  3.9070857056  1500          0.7147569132 
0.9982332155  1.0000000000  0.6051764706  0.5725047081  0.6751713633  0.6692073171  0.7689744539  0.7955555556  50.883392226  3.8745920444  1800          0.7149933712 
0.9982332155  1.0000000000  0.6771764706  0.6949152542  0.6785986291  0.6676829268  0.8259903739  0.8459259259  59.363957597  3.8788049173  2100          0.7147625693 
0.9982332155  1.0000000000  0.6268235294  0.6082862524  0.6721249048  0.6615853659  0.7885968160  0.8014814815  67.844522968  3.8519536797  2400          0.7152314631 
0.9982332155  1.0000000000  0.6357647059  0.6271186441  0.6759329779  0.6753048780  0.7930396150  0.8148148148  76.325088339  3.8867682036  2700          0.7154335388 
0.9982332155  1.0000000000  0.6423529412  0.6233521657  0.6770753998  0.6768292683  0.7982228804  0.8251851852  84.805653710  3.8411490218  3000          0.7147806247 
0.9982332155  1.0000000000  0.6296470588  0.6082862524  0.6660319878  0.6570121951  0.7630507220  0.7837037037  93.286219081  3.8276441614  3300          0.7156715878 
0.9982332155  1.0000000000  0.6061176471  0.5838041431  0.6820258949  0.6646341463  0.7541651240  0.7570370370  101.76678445  3.8293296528  3600          0.7151932867 
0.9982332155  1.0000000000  0.6141176471  0.5819209040  0.6645087586  0.6600609756  0.7537948908  0.7540740741  110.24734982  3.8350527994  3900          0.7158758434 
0.9982332155  1.0000000000  0.6178823529  0.5894538606  0.6443259711  0.6509146341  0.7219548315  0.7303703704  118.72791519  3.8221445274  4200          0.7150679564 
0.9982332155  1.0000000000  0.6202352941  0.6139359699  0.6450875857  0.6554878049  0.7341725287  0.7437037037  127.20848056  3.8216344237  4500          0.7147232596 
0.9982332155  1.0000000000  0.6009411765  0.5781544256  0.6408987053  0.6402439024  0.7108478341  0.7111111111  135.68904593  3.8204364157  4800          0.7148088264 
0.9982332155  1.0000000000  0.5934117647  0.5725047081  0.6648895659  0.6417682927  0.7138097001  0.7348148148  141.34275618  3.8576069272  5000          0.7150501418 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	data_augmentation: True
	lr: 0.001
	mlp_depth: 3
	mlp_dropout: 0.1
	mlp_width: 512
	momentum: 0.1
	nonlinear_classifier: False
	num_domain_tokens: 16
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "a photo of a"
Number of context words (tokens): 4
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          step          step_time    
0.6095406360  0.5583038869  0.5698823529  0.5781544256  0.5841584158  0.5701219512  0.6060718252  0.6222222222  0.0000000000  6.3484177589  0             1.8395144939 
0.9991166078  1.0000000000  0.6047058824  0.6007532957  0.7003046458  0.6935975610  0.7878563495  0.7925925926  8.4805653710  4.6495194674  300           0.7149995391 
0.9982332155  0.9964664311  0.5444705882  0.5423728814  0.6226199543  0.6173780488  0.7293594965  0.7362962963  16.961130742  4.0997812533  600           0.7140839847 
0.9991166078  1.0000000000  0.5345882353  0.4990583804  0.6020563595  0.6128048780  0.6760459089  0.6800000000  25.441696113  3.9961274425  900           0.7151708539 
0.9991166078  1.0000000000  0.5524705882  0.5273069680  0.6054836253  0.6082317073  0.6449463162  0.6459259259  33.922261484  3.9721499546  1200          0.7149098134 
0.9991166078  1.0000000000  0.5670588235  0.5367231638  0.6058644326  0.6036585366  0.6482784154  0.6488888889  42.402826855  3.9782744169  1500          0.7144708061 
0.9982332155  1.0000000000  0.5741176471  0.5725047081  0.6325209444  0.6387195122  0.6734542762  0.6829629630  50.883392226  3.9309957631  1800          0.7147739482 
0.9982332155  0.9964664311  0.5675294118  0.5423728814  0.6142421935  0.6112804878  0.6330988523  0.6340740741  59.363957597  3.9389084109  2100          0.7150038401 
0.9982332155  0.9964664311  0.5675294118  0.5461393597  0.6077684692  0.6112804878  0.6327286190  0.6385185185  67.844522968  3.9221689200  2400          0.7146905414 
0.9982332155  1.0000000000  0.5868235294  0.5687382298  0.6332825590  0.6326219512  0.6427249167  0.6474074074  76.325088339  3.9546997301  2700          0.7146387903 
0.9982332155  1.0000000000  0.5548235294  0.5329566855  0.6260472201  0.6128048780  0.6264346538  0.6325925926  84.805653710  3.9009910472  3000          0.7147435609 
0.9982332155  1.0000000000  0.5750588235  0.5593220339  0.6633663366  0.6448170732  0.7189929656  0.7274074074  93.286219081  3.9091715360  3300          0.7142658647 
0.9982332155  1.0000000000  0.5642352941  0.5574387947  0.6260472201  0.6356707317  0.6516105146  0.6459259259  101.76678445  3.8961533030  3600          0.7151343505 
0.9982332155  1.0000000000  0.5637647059  0.5329566855  0.6157654227  0.6112804878  0.6182895224  0.6281481481  110.24734982  3.8990264948  3900          0.7140391819 
0.9982332155  1.0000000000  0.5510588235  0.5329566855  0.6264280274  0.6234756098  0.6282858201  0.6385185185  118.72791519  3.9082644224  4200          0.7139798713 
0.9982332155  1.0000000000  0.5548235294  0.5254237288  0.6180502666  0.6143292683  0.6116253240  0.6237037037  127.20848056  3.8834996867  4500          0.7142287175 
0.9982332155  1.0000000000  0.5421176471  0.5235404896  0.6165270373  0.6219512195  0.6149574232  0.6148148148  135.68904593  3.8868958966  4800          0.7145143557 
0.9982332155  1.0000000000  0.5364705882  0.5197740113  0.6054836253  0.6067073171  0.5923731951  0.6044444444  141.34275618  3.8709662580  5000          0.7141918242 
