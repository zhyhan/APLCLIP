./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
closs         dloss         env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kloss         step          step_time    
0.3675505221  1.0683833361  0.9984301413  1.0000000000  0.7188940092  0.6981132075  0.7769126608  0.7621951220  0.8828562027  0.9020771513  0.0000000000  0.6104874611  0             9.3857059479 
0.1464679483  0.2574053510  1.0000000000  1.0000000000  0.6749057394  0.6452830189  0.9444820582  0.8902439024  0.9644619941  0.9525222552  7.5353218210  0.1296511820  300           1.3400924516 
0.0659083068  0.4612839205  1.0000000000  1.0000000000  0.6711353163  0.6452830189  0.9678402167  0.8963414634  0.9789404409  0.9554896142  15.070643642  0.1057303466  600           1.3405988971 
0.0538277706  0.7524743258  1.0000000000  1.0000000000  0.6715542522  0.6490566038  0.9793500339  0.9085365854  0.9828891083  0.9465875371  22.605965463  0.1031269505  900           1.3412836305 
0.0462246482  0.9026295386  1.0000000000  1.0000000000  0.6723921240  0.6490566038  0.9820582261  0.9085365854  0.9881539980  0.9465875371  30.141287284  0.1029461769  1200          1.3408607507 
0.0357267951  0.9485913306  1.0000000000  1.0000000000  0.6728110599  0.6452830189  0.9888287068  0.9024390244  0.9914445541  0.9436201780  37.676609105  0.0886702641  1500          1.3419115980 
0.0279809397  0.9701694586  1.0000000000  1.0000000000  0.6715542522  0.6452830189  0.9905213270  0.9054878049  0.9934188878  0.9465875371  45.211930926  0.0822718604  1800          1.3422920624 
0.0212167512  0.9798346543  1.0000000000  1.0000000000  0.6711353163  0.6452830189  0.9922139472  0.9054878049  0.9940769990  0.9436201780  52.747252747  0.0674229129  2100          1.3561596211 
0.0192824916  0.9867208437  1.0000000000  1.0000000000  0.6711353163  0.6452830189  0.9952606635  0.9054878049  0.9957222771  0.9406528190  60.282574568  0.0603182600  2400          1.3494994378 
0.0152584619  0.9925288218  1.0000000000  1.0000000000  0.6698785086  0.6415094340  0.9959377116  0.9054878049  0.9970384995  0.9406528190  67.817896389  0.0542698183  2700          1.3416482512 
0.0122212086  0.9922021492  1.0000000000  1.0000000000  0.6682027650  0.6415094340  0.9962762356  0.9024390244  0.9967094439  0.9406528190  75.353218210  0.0523684315  3000          1.3433866564 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
closs         dloss         env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kloss         step          step_time    
0.5181087255  1.1157500744  0.9976452119  1.0000000000  0.7406786762  0.7283018868  0.7149627624  0.6981707317  0.8749588680  0.9020771513  0.0000000000  0.5501352549  0             11.745154380 
0.1999986258  0.1640606185  1.0000000000  1.0000000000  0.8894009217  0.8113207547  0.8182125931  0.7774390244  0.9532741033  0.9406528190  7.5353218210  0.1506822969  300           1.3420983227 
0.0946639722  0.4135130861  1.0000000000  1.0000000000  0.9250104734  0.8113207547  0.8219363575  0.7865853659  0.9723593287  0.9525222552  15.070643642  0.1294805535  600           1.3420164426 
0.0763611433  0.7920903955  1.0000000000  1.0000000000  0.9413489736  0.8339622642  0.8239675017  0.7957317073  0.9802566634  0.9495548961  22.605965463  0.1294784588  900           1.3429221916 
0.0595822509  0.9090595818  1.0000000000  1.0000000000  0.9560117302  0.8264150943  0.8192281652  0.7987804878  0.9835472195  0.9525222552  30.141287284  0.1155568286  1200          1.3435615961 
0.0446101581  0.9517908114  1.0000000000  1.0000000000  0.9669040637  0.8264150943  0.8199052133  0.8018292683  0.9884830536  0.9525222552  37.676609105  0.1047645372  1500          1.3446456099 
0.0345548281  0.9693491975  1.0000000000  1.0000000000  0.9727691663  0.8264150943  0.8182125931  0.7957317073  0.9904573873  0.9495548961  45.211930926  0.0870724263  1800          1.3445238034 
0.0261732834  0.9743317763  1.0000000000  1.0000000000  0.9773774612  0.8264150943  0.8192281652  0.7957317073  0.9927607766  0.9465875371  52.747252747  0.0808498968  2100          1.7470794837 
0.0238050696  0.9872237607  1.0000000000  1.0000000000  0.9828236280  0.8264150943  0.8171970210  0.7957317073  0.9940769990  0.9465875371  60.282574568  0.0745407394  2400          1.3437295985 
0.0199668586  0.9930120911  1.0000000000  1.0000000000  0.9861751152  0.8301886792  0.8155044008  0.7896341463  0.9947351102  0.9465875371  67.817896389  0.0638761439  2700          1.3439138063 
0.0138283625  0.9937817736  1.0000000000  1.0000000000  0.9891076665  0.8339622642  0.8138117806  0.7896341463  0.9947351102  0.9436201780  75.353218210  0.0587588399  3000          1.3439140368 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
closs         dloss         env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kloss         step          step_time    
0.7067578435  1.2055222988  0.9984301413  1.0000000000  0.7385839966  0.7471698113  0.7542315504  0.7347560976  0.8736426456  0.8931750742  0.0000000000  0.5528984070  0             11.787979841 
0.2143694861  0.1522071438  1.0000000000  1.0000000000  0.8894009217  0.8075471698  0.9427894381  0.8993902439  0.8861467588  0.9139465875  7.5353218210  0.1674836553  300           1.3430371205 
0.1026520065  0.3728878651  1.0000000000  1.0000000000  0.9287808965  0.8188679245  0.9695328368  0.8902439024  0.8821980915  0.8991097923  15.070643642  0.1430324989  600           1.3435330637 
0.0728621964  0.6237324953  1.0000000000  1.0000000000  0.9472140762  0.8150943396  0.9769803656  0.8963414634  0.8812109246  0.9080118694  22.605965463  0.1315682768  900           1.3436488008 
0.0671129622  0.8369135697  1.0000000000  1.0000000000  0.9589442815  0.8188679245  0.9813811781  0.8993902439  0.8845014808  0.9080118694  30.141287284  0.1319472321  1200          1.3445599127 
0.0496962070  0.8855863565  1.0000000000  1.0000000000  0.9664851278  0.8188679245  0.9857819905  0.8993902439  0.8808818690  0.9020771513  37.676609105  0.1181763329  1500          1.3449670998 
0.0395318014  0.9055384111  1.0000000000  1.0000000000  0.9736070381  0.8226415094  0.9867975626  0.8993902439  0.8759460349  0.8991097923  45.211930926  0.1020771683  1800          1.3439439185 
0.0339516004  0.9138847307  1.0000000000  1.0000000000  0.9786342690  0.8188679245  0.9888287068  0.9024390244  0.8706811451  0.8961424332  52.747252747  0.0976478122  2100          1.3449645782 
0.0253531626  0.9255687602  1.0000000000  1.0000000000  0.9836614998  0.8188679245  0.9911983751  0.9024390244  0.8673905890  0.8872403561  60.282574568  0.0821028816  2400          1.3443457866 
0.0226311171  0.9289950214  1.0000000000  1.0000000000  0.9878508588  0.8150943396  0.9928909953  0.8993902439  0.8660743666  0.8902077151  67.817896389  0.0791141282  2700          1.3455014292 
0.0209279450  0.9420874951  1.0000000000  1.0000000000  0.9916212819  0.8150943396  0.9935680433  0.8993902439  0.8617966436  0.8872403561  75.353218210  0.0762901637  3000          1.3457146311 
