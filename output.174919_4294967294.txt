./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kl_loss       step          step_time    
0.7974038124  1.1246479750  0.8320366133  0.8512396694  0.6989055739  0.6949541284  0.8958958959  0.8848758465  0.8972463029  0.9149425287  0.0000000000  1.1136577129  0             13.557541608 
0.3891418635  0.4093686226  0.8727688787  0.8553719008  0.8875031815  0.8119266055  0.9679679680  0.9480812641  0.9558898521  0.9310344828  4.3935926773  0.3971101972  300           1.2666125798 
0.1848860513  0.7161244080  0.8723112128  0.8512396694  0.9256808348  0.8486238532  0.9789789790  0.9593679458  0.9739928608  0.9402298851  8.7871853547  0.2108494437  600           1.2671555694 
0.1130941688  0.8199976468  0.8732265446  0.8512396694  0.9432425554  0.8600917431  0.9852352352  0.9616252822  0.9811320755  0.9425287356  13.180778032  0.1502556383  900           1.2671397074 
0.0957921766  0.9412339147  0.8768878719  0.8553719008  0.9569865106  0.8692660550  0.9884884885  0.9638826185  0.9869964304  0.9425287356  17.574370709  0.1329870980  1200          1.2667807269 
0.0724588760  0.9867774645  0.8791762014  0.8595041322  0.9638584882  0.8715596330  0.9909909910  0.9661399549  0.9905660377  0.9402298851  21.967963386  0.1062579541  1500          1.2700680971 
0.0596610068  1.0343222576  0.8773455378  0.8595041322  0.9704759481  0.8738532110  0.9922422422  0.9706546275  0.9920958695  0.9379310345  26.361556064  0.0810766880  1800          1.2687865202 
0.0500570001  1.0471528963  0.8782608696  0.8595041322  0.9748027488  0.8738532110  0.9937437437  0.9729119639  0.9933707292  0.9356321839  30.755148741  0.0752536193  2100          1.2698537922 
0.0422199122  1.0569890187  0.8787185355  0.8595041322  0.9760753372  0.8761467890  0.9952452452  0.9729119639  0.9941356451  0.9356321839  35.148741418  0.0582319767  2400          1.2697640777 
0.0356841252  1.0679424940  0.8805491991  0.8595041322  0.9791295495  0.8784403670  0.9959959960  0.9729119639  0.9943906170  0.9333333333  39.542334096  0.0496263649  2700          1.2700541798 
0.0362807947  1.0644395487  0.8768878719  0.8512396694  0.9809111733  0.8807339450  0.9957457457  0.9729119639  0.9946455890  0.9356321839  43.935926773  0.0443768412  3000          1.2702355814 
0.0311790386  1.0690531997  0.8759725400  0.8512396694  0.9811656910  0.8876146789  0.9964964965  0.9751693002  0.9951555329  0.9356321839  48.329519450  0.0404571472  3300          1.2699552139 
0.0307656072  1.0752619449  0.8732265446  0.8512396694  0.9816747264  0.8876146789  0.9967467467  0.9751693002  0.9951555329  0.9333333333  52.723112128  0.0387521076  3600          1.2707133953 
0.0300232603  1.0687110776  0.8745995423  0.8471074380  0.9829473148  0.8876146789  0.9967467467  0.9751693002  0.9954105048  0.9356321839  57.116704805  0.0396188374  3900          1.2698880593 
0.0265317351  1.0730062763  0.8755148741  0.8388429752  0.9844744210  0.8853211009  0.9969969970  0.9729119639  0.9961754207  0.9333333333  61.510297482  0.0309937905  4200          1.2749714049 
0.0265427939  1.0701405271  0.8723112128  0.8388429752  0.9847289387  0.8830275229  0.9974974975  0.9729119639  0.9961754207  0.9333333333  65.903890160  0.0286659883  4500          1.2695685005 
0.0291142184  1.0719974343  0.8713958810  0.8471074380  0.9847289387  0.8830275229  0.9977477477  0.9729119639  0.9961754207  0.9356321839  70.297482837  0.0350143944  4800          1.2700056068 
0.0245784401  1.0778377676  0.8713958810  0.8512396694  0.9852379740  0.8876146789  0.9977477477  0.9751693002  0.9961754207  0.9379310345  73.226544622  0.0261098133  5000          1.2702795422 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kl_loss       step          step_time    
0.5174990296  1.1512644291  0.8320366133  0.8471074380  0.6927971494  0.6834862385  0.8948948949  0.8848758465  0.8952065273  0.9149425287  0.0000000000  0.9722533226  0             1.9925055504 
0.2742005396  0.5209726091  0.9592677346  0.8677685950  0.7526088063  0.7316513761  0.9669669670  0.9525959368  0.9566547680  0.9379310345  4.3935926773  0.3598098167  300           1.2676965396 
0.1235996688  0.6872713832  0.9807780320  0.8760330579  0.7541359124  0.7362385321  0.9804804805  0.9571106095  0.9755226925  0.9402298851  8.7871853547  0.1989621191  600           1.2673550534 
0.0851192956  0.8035731558  0.9862700229  0.8884297521  0.7528633240  0.7362385321  0.9864864865  0.9638826185  0.9836817950  0.9402298851  13.180778032  0.1482016466  900           1.2682011588 
0.0690514602  0.9240443665  0.9899313501  0.8884297521  0.7554085009  0.7545871560  0.9897397397  0.9683972912  0.9890362060  0.9425287356  17.574370709  0.1210167090  1200          1.2690724897 
0.0508524768  0.9775815177  0.9926773455  0.8842975207  0.7571901247  0.7614678899  0.9924924925  0.9706546275  0.9913309536  0.9448275862  21.967963386  0.1061816736  1500          1.2707155053 
0.0424771507  1.0241384325  0.9931350114  0.8760330579  0.7571901247  0.7568807339  0.9929929930  0.9706546275  0.9923508414  0.9448275862  26.361556064  0.0855413049  1800          1.2694380657 
0.0352868153  1.0396026166  0.9940503432  0.8801652893  0.7556630186  0.7545871560  0.9947447447  0.9683972912  0.9931157573  0.9425287356  30.755148741  0.0736923424  2100          1.2696763492 
0.0313112962  1.0495408189  0.9945080092  0.8842975207  0.7538813948  0.7522935780  0.9949949950  0.9661399549  0.9941356451  0.9448275862  35.148741418  0.0517821116  2400          1.2691027840 
0.0240891971  1.0541099125  0.9954233410  0.8842975207  0.7531178417  0.7431192661  0.9954954955  0.9683972912  0.9949005609  0.9425287356  39.542334096  0.0465269294  2700          1.2698812604 
0.0245191321  1.0637098887  0.9958810069  0.8884297521  0.7523542886  0.7385321101  0.9962462462  0.9661399549  0.9949005609  0.9379310345  43.935926773  0.0462271145  3000          1.2706156985 
0.0198548688  1.0642476120  0.9958810069  0.8884297521  0.7546449478  0.7362385321  0.9964964965  0.9706546275  0.9949005609  0.9379310345  48.329519450  0.0342089394  3300          1.2692355728 
0.0221183663  1.0643396868  0.9958810069  0.8884297521  0.7554085009  0.7385321101  0.9967467467  0.9683972912  0.9954105048  0.9402298851  52.723112128  0.0326730462  3600          1.2679808267 
0.0163363768  1.0631972311  0.9958810069  0.8925619835  0.7536268771  0.7270642202  0.9969969970  0.9706546275  0.9954105048  0.9379310345  57.116704805  0.0287212178  3900          1.2696060157 
0.0171567421  1.0666401261  0.9958810069  0.8925619835  0.7531178417  0.7293577982  0.9969969970  0.9683972912  0.9956654768  0.9333333333  61.510297482  0.0305193074  4200          1.2701113749 
0.0158252259  1.0669121937  0.9958810069  0.8966942149  0.7541359124  0.7316513761  0.9972472472  0.9706546275  0.9956654768  0.9310344828  65.903890160  0.0316128591  4500          1.2701414108 
0.0150084306  1.0697635180  0.9963386728  0.8966942149  0.7541359124  0.7316513761  0.9972472472  0.9706546275  0.9959204488  0.9287356322  70.297482837  0.0246780825  4800          1.2694808189 
0.0157855378  1.0712970731  0.9963386728  0.8884297521  0.7528633240  0.7339449541  0.9974974975  0.9729119639  0.9961754207  0.9287356322  73.226544622  0.0243034588  5000          1.2697125661 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kl_loss       step          step_time    
0.8603045344  1.1533626318  0.8334096110  0.8471074380  0.6978875032  0.6811926606  0.8998998999  0.8893905192  0.8967363590  0.9172413793  0.0000000000  1.3315154314  0             2.0452589989 
0.3919942090  0.4062708359  0.9569794050  0.8760330579  0.8885212522  0.8211009174  0.9234234234  0.9209932280  0.9576746558  0.9287356322  4.3935926773  0.3964415388  300           1.2688175909 
0.1736317584  0.6609801387  0.9789473684  0.8801652893  0.9244082464  0.8417431193  0.9226726727  0.9232505643  0.9737378888  0.9356321839  8.7871853547  0.2043758611  600           1.2684697715 
0.1228422409  0.8165060288  0.9876430206  0.8966942149  0.9419699669  0.8623853211  0.9221721722  0.9209932280  0.9816420194  0.9379310345  13.180778032  0.1573297702  900           1.2693887083 
0.0818511291  0.9115677230  0.9894736842  0.8884297521  0.9539322983  0.8646788991  0.9194194194  0.9209932280  0.9869964304  0.9425287356  17.574370709  0.1253234396  1200          1.2692292786 
0.0677783756  0.9674533929  0.9922196796  0.9008264463  0.9620768643  0.8738532110  0.9184184184  0.9142212190  0.9900560938  0.9471264368  21.967963386  0.1020448653  1500          1.2691669305 
0.0573956586  1.0175328888  0.9935926773  0.9008264463  0.9686943243  0.8853211009  0.9181681682  0.9187358916  0.9920958695  0.9448275862  26.361556064  0.0839401077  1800          1.2695418135 
0.0469521559  1.0306508559  0.9940503432  0.8925619835  0.9727666073  0.8876146789  0.9174174174  0.9187358916  0.9928607853  0.9402298851  30.755148741  0.0692256231  2100          1.2692010824 
0.0427504808  1.0481833305  0.9945080092  0.8925619835  0.9763298549  0.8876146789  0.9166666667  0.9164785553  0.9933707292  0.9425287356  35.148741418  0.0573202300  2400          1.2703560646 
0.0396437087  1.0570938526  0.9954233410  0.8925619835  0.9778569611  0.8876146789  0.9169169169  0.9142212190  0.9936257012  0.9448275862  39.542334096  0.0537004566  2700          1.2688280201 
0.0308039147  1.0589271784  0.9954233410  0.8966942149  0.9804021379  0.8922018349  0.9166666667  0.9142212190  0.9938806731  0.9448275862  43.935926773  0.0382990968  3000          1.2702752177 
0.0353058971  1.0637161660  0.9958810069  0.8966942149  0.9821837618  0.8967889908  0.9154154154  0.9142212190  0.9941356451  0.9425287356  48.329519450  0.0375384287  3300          1.2715750901 
0.0329649992  1.0619451584  0.9958810069  0.8966942149  0.9829473148  0.8944954128  0.9136636637  0.9119638826  0.9946455890  0.9425287356  52.723112128  0.0382848714  3600          1.2717055885 
0.0257418795  1.0681025507  0.9958810069  0.8925619835  0.9829473148  0.8922018349  0.9134134134  0.9119638826  0.9949005609  0.9425287356  57.116704805  0.0332886397  3900          1.2855805588 
0.0277274746  1.0642992753  0.9963386728  0.8966942149  0.9834563502  0.8922018349  0.9139139139  0.9119638826  0.9951555329  0.9379310345  61.510297482  0.0309651367  4200          1.2718445794 
0.0257718924  1.0678684175  0.9963386728  0.9008264463  0.9844744210  0.8922018349  0.9124124124  0.9142212190  0.9956654768  0.9356321839  65.903890160  0.0304785638  4500          1.2723922213 
0.0235368283  1.0697230587  0.9963386728  0.9008264463  0.9857470094  0.8876146789  0.9116616617  0.9142212190  0.9959204488  0.9333333333  70.297482837  0.0237907470  4800          1.2717869735 
0.0296157488  1.0666230282  0.9963386728  0.9049586777  0.9857470094  0.8853211009  0.9089089089  0.9119638826  0.9964303927  0.9333333333  73.226544622  0.0259458940  5000          1.2722632051 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kl_loss       step          step_time    
0.9854695201  1.2118296623  0.8361556064  0.8388429752  0.6927971494  0.6857798165  0.8961461461  0.8848758465  0.8982661907  0.9172413793  0.0000000000  1.6045254469  0             2.0648045540 
0.3957419573  0.2374943469  0.9560640732  0.8760330579  0.8872486638  0.8256880734  0.9659659660  0.9480812641  0.9250382458  0.9356321839  4.3935926773  0.4218282260  300           1.2719527451 
0.1847219919  0.6373749002  0.9780320366  0.8925619835  0.9251717994  0.8555045872  0.9787287287  0.9525959368  0.9245283019  0.9425287356  8.7871853547  0.2213917428  600           1.2772282179 
0.1207669310  0.7445121032  0.9876430206  0.8925619835  0.9401883431  0.8623853211  0.9859859860  0.9548532731  0.9237633860  0.9517241379  13.180778032  0.1674084076  900           1.2780780554 
0.0916848458  0.8968617509  0.9899313501  0.8966942149  0.9549503691  0.8738532110  0.9894894895  0.9548532731  0.9252932177  0.9448275862  17.574370709  0.1379990234  1200          1.2738812995 
0.0694645320  0.9647801427  0.9922196796  0.8966942149  0.9623313820  0.8784403670  0.9914914915  0.9548532731  0.9235084141  0.9471264368  21.967963386  0.1113677286  1500          1.2743520713 
0.0588430572  0.9986457473  0.9926773455  0.8966942149  0.9692033596  0.8830275229  0.9934934935  0.9548532731  0.9245283019  0.9425287356  26.361556064  0.0925029483  1800          1.2749103363 
0.0514604152  1.0279492037  0.9931350114  0.9090909091  0.9740391957  0.8899082569  0.9944944945  0.9548532731  0.9232534421  0.9425287356  30.755148741  0.0788855472  2100          1.2727480419 
0.0450979334  1.0357560368  0.9954233410  0.9049586777  0.9760753372  0.8967889908  0.9947447447  0.9616252822  0.9217236104  0.9425287356  35.148741418  0.0680926071  2400          1.2736900242 
0.0429255122  1.0472723669  0.9954233410  0.8966942149  0.9778569611  0.8922018349  0.9952452452  0.9638826185  0.9191738909  0.9402298851  39.542334096  0.0607457726  2700          1.2753918807 
0.0358324141  1.0485033224  0.9954233410  0.9049586777  0.9791295495  0.8899082569  0.9952452452  0.9616252822  0.9189189189  0.9402298851  43.935926773  0.0496281537  3000          1.2779770605 
0.0335112609  1.0539205098  0.9954233410  0.9049586777  0.9804021379  0.8922018349  0.9957457457  0.9638826185  0.9176440592  0.9402298851  48.329519450  0.0452726736  3300          1.2772421630 
0.0338813535  1.0602046575  0.9954233410  0.9049586777  0.9819292441  0.8944954128  0.9959959960  0.9638826185  0.9173890872  0.9379310345  52.723112128  0.0405849944  3600          1.2759230685 
0.0285809578  1.0584811858  0.9958810069  0.9008264463  0.9826927971  0.8899082569  0.9964964965  0.9661399549  0.9178990311  0.9425287356  57.116704805  0.0375363947  3900          1.2753948808 
0.0286435092  1.0636652734  0.9958810069  0.9049586777  0.9832018325  0.8876146789  0.9967467467  0.9661399549  0.9168791433  0.9379310345  61.510297482  0.0367448352  4200          1.2728660067 
0.0269294881  1.0645577806  0.9963386728  0.9049586777  0.9842199033  0.8922018349  0.9969969970  0.9638826185  0.9140744518  0.9402298851  65.903890160  0.0407923972  4500          1.2726628200 
0.0287531871  1.0657352612  0.9963386728  0.9049586777  0.9844744210  0.8922018349  0.9972472472  0.9661399549  0.9120346762  0.9379310345  70.297482837  0.0310636323  4800          1.2729207850 
0.0299711300  1.0601057976  0.9963386728  0.9049586777  0.9852379740  0.8876146789  0.9977477477  0.9661399549  0.9092299847  0.9379310345  73.226544622  0.0381451296  5000          1.2728209090 
