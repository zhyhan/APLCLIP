./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kl_loss       step          step_time    
0.9295658469  1.2033617496  0.8151390319  0.8288659794  0.6975945017  0.6930126002  0.9011824324  0.8850056370  0.8975903614  0.9081515499  0.0000000000  -0.019628424  0             17.347826719 
0.4102384847  0.5748244738  0.8681771370  0.8680412371  0.8966208477  0.8361970218  0.9755067568  0.9492671928  0.9664371773  0.9299655568  4.9433573635  -0.017781129  300           2.3181712198 
0.2099768686  0.8073779446  0.8738414006  0.8721649485  0.9390034364  0.8556701031  0.9839527027  0.9560315671  0.9799196787  0.9322617681  9.8867147271  -0.016268625  600           2.0128011282 
0.1591821004  0.9138055295  0.8795056643  0.8762886598  0.9553264605  0.8648339061  0.9878941441  0.9594137542  0.9868043603  0.9299655568  14.830072090  -0.015966700  900           1.9806310614 
0.1233371291  0.9876728729  0.8784757981  0.8783505155  0.9664948454  0.8705612829  0.9909909910  0.9605411499  0.9896729776  0.9299655568  19.773429454  -0.015806754  1200          2.4300596952 
0.1089434086  1.0235493829  0.8753861998  0.8742268041  0.9736540664  0.8751431844  0.9929617117  0.9627959414  0.9911072863  0.9299655568  24.716786817  -0.015712456  1500          2.0242552090 
0.0883113008  1.0490437591  0.8717816684  0.8742268041  0.9762313860  0.8751431844  0.9938063063  0.9605411499  0.9936890419  0.9288174512  29.660144181  -0.015670767  1800          2.5583179561 
0.0689270772  1.0590473350  0.8733264676  0.8742268041  0.9799541810  0.8774341352  0.9946509009  0.9627959414  0.9942627653  0.9276693456  34.603501544  -0.015525086  2100          2.4332592130 
0.0642076521  1.0664524404  0.8733264676  0.8701030928  0.9819587629  0.8797250859  0.9946509009  0.9616685457  0.9956970740  0.9253731343  39.546858908  -0.015578935  2400          2.0549944027 
0.0555173355  1.0675882991  0.8748712667  0.8721649485  0.9842497136  0.8762886598  0.9952139640  0.9616685457  0.9956970740  0.9253731343  44.490216271  -0.015593752  2700          1.9267644056 
0.0483986348  1.0693607155  0.8759011329  0.8721649485  0.9848224513  0.8774341352  0.9960585586  0.9616685457  0.9962707975  0.9230769231  49.433573635  -0.015500000  3000          1.7364170527 
0.0430257880  1.0755836286  0.8738414006  0.8701030928  0.9853951890  0.8751431844  0.9966216216  0.9616685457  0.9962707975  0.9242250287  54.376930999  -0.015497311  3300          1.8404242571 
0.0407377715  1.0752926219  0.8728115345  0.8659793814  0.9868270332  0.8717067583  0.9966216216  0.9639233371  0.9962707975  0.9219288175  59.320288362  -0.015482036  3600          1.8644391282 
0.0369640488  1.0748172410  0.8743563337  0.8680412371  0.9871134021  0.8705612829  0.9969031532  0.9639233371  0.9962707975  0.9207807118  64.263645726  -0.015515555  3900          1.8896695447 
0.0344972496  1.0782262750  0.8738414006  0.8680412371  0.9873997709  0.8705612829  0.9974662162  0.9639233371  0.9965576592  0.9230769231  69.207003089  -0.015532601  4200          2.5845655473 
0.0358797584  1.0786144300  0.8728115345  0.8680412371  0.9873997709  0.8694158076  0.9974662162  0.9627959414  0.9965576592  0.9219288175  74.150360453  -0.015540594  4500          2.5956386121 
0.0326679407  1.0793177581  0.8707518023  0.8721649485  0.9876861397  0.8694158076  0.9980292793  0.9639233371  0.9968445209  0.9207807118  79.093717816  -0.015513289  4800          1.3180083013 
0.0299879309  1.0771034658  0.8702368692  0.8680412371  0.9879725086  0.8705612829  0.9983108108  0.9616685457  0.9974182444  0.9219288175  82.389289392  -0.015536737  5000          1.2863632154 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kl_loss       step          step_time    
0.9095751643  1.1884964705  0.8223480947  0.8309278351  0.6907216495  0.6827033219  0.8978040541  0.8804960541  0.8964429145  0.9047072331  0.0000000000  -0.018852941  0             5.1412587166 
0.3264466612  0.6402651611  0.9649845520  0.8927835052  0.7425544101  0.7388316151  0.9749436937  0.9503945885  0.9667240390  0.9299655568  4.9433573635  -0.016694912  300           1.2212617620 
0.1579488761  0.7649524721  0.9824922760  0.9010309278  0.7491408935  0.7411225659  0.9847972973  0.9594137542  0.9836488812  0.9311136625  9.8867147271  -0.015533520  600           1.2703901585 
0.1265641728  0.8750631489  0.9917610711  0.9072164948  0.7502863688  0.7468499427  0.9895833333  0.9594137542  0.9882386690  0.9345579793  14.830072090  -0.015467331  900           1.2722581728 
0.1094953873  0.9739973613  0.9933058702  0.9092783505  0.7517182131  0.7445589920  0.9921171171  0.9616685457  0.9899598394  0.9368541906  19.773429454  -0.015419467  1200          1.3227104076 
0.0849200427  1.0200356416  0.9953656025  0.9092783505  0.7528636884  0.7479954181  0.9938063063  0.9627959414  0.9916810098  0.9334098737  24.716786817  -0.015437721  1500          1.3615986037 
0.0679078374  1.0458760740  0.9958805355  0.9072164948  0.7554410080  0.7457044674  0.9943693694  0.9627959414  0.9945496271  0.9311136625  29.660144181  -0.015434502  1800          1.3006080317 
0.0572969027  1.0559870372  0.9963954686  0.9072164948  0.7540091638  0.7457044674  0.9957770270  0.9639233371  0.9951233505  0.9322617681  34.603501544  -0.015375077  2100          1.8484542767 
0.0452354875  1.0616284317  0.9963954686  0.9113402062  0.7540091638  0.7468499427  0.9969031532  0.9639233371  0.9959839357  0.9345579793  39.546858908  -0.015386391  2400          1.8385016592 
0.0408800410  1.0653827449  0.9963954686  0.9092783505  0.7551546392  0.7491408935  0.9974662162  0.9627959414  0.9962707975  0.9357060850  44.490216271  -0.015355857  2700          2.0424514389 
0.0341759014  1.0654399997  0.9963954686  0.9051546392  0.7563001145  0.7502863688  0.9971846847  0.9627959414  0.9968445209  0.9334098737  49.433573635  -0.015414813  3000          2.2671979888 
0.0320689058  1.0700780437  0.9963954686  0.9030927835  0.7571592211  0.7491408935  0.9974662162  0.9650507328  0.9968445209  0.9322617681  54.376930999  -0.015387778  3300          2.4609852521 
0.0298780723  1.0679411894  0.9963954686  0.9072164948  0.7531500573  0.7491408935  0.9974662162  0.9605411499  0.9971313827  0.9322617681  59.320288362  -0.015405462  3600          2.0304905105 
0.0311007164  1.0682282062  0.9963954686  0.9072164948  0.7525773196  0.7445589920  0.9977477477  0.9616685457  0.9971313827  0.9322617681  64.263645726  -0.015436687  3900          2.1586325367 
0.0248502034  1.0721016771  0.9963954686  0.9072164948  0.7525773196  0.7422680412  0.9974662162  0.9616685457  0.9971313827  0.9357060850  69.207003089  -0.015446842  4200          1.9929796044 
0.0277668019  1.0728796681  0.9963954686  0.9051546392  0.7505727377  0.7445589920  0.9980292793  0.9627959414  0.9971313827  0.9357060850  74.150360453  -0.015482163  4500          1.4964115540 
0.0271714772  1.0748553522  0.9963954686  0.9030927835  0.7482817869  0.7422680412  0.9980292793  0.9627959414  0.9974182444  0.9345579793  79.093717816  -0.015446253  4800          1.3135535773 
0.0253956842  1.0729821873  0.9963954686  0.9030927835  0.7459908362  0.7376861397  0.9980292793  0.9673055242  0.9974182444  0.9345579793  82.389289392  -0.015504529  5000          1.3097144294 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kl_loss       step          step_time    
1.1489900351  1.2400398254  0.8249227600  0.8329896907  0.7021764032  0.6975945017  0.8992117117  0.8816234498  0.8958691910  0.9092996556  0.0000000000  -0.023157876  0             2.0876088142 
0.4382872295  0.5179170315  0.9629248198  0.8907216495  0.8946162658  0.8258877434  0.9284909910  0.9267192785  0.9647160069  0.9334098737  4.9433573635  -0.017940128  300           1.2152656913 
0.2109746474  0.7857188841  0.9819773429  0.9010309278  0.9338487973  0.8442153494  0.9239864865  0.9255918828  0.9807802639  0.9345579793  9.8867147271  -0.016072614  600           1.2565263176 
0.1569138290  0.8986744503  0.9902162719  0.9072164948  0.9530355097  0.8591065292  0.9208896396  0.9255918828  0.9879518072  0.9311136625  14.830072090  -0.015805063  900           1.2650682211 
0.1232546678  0.9757796190  0.9933058702  0.9113402062  0.9630584192  0.8602520046  0.9220157658  0.9267192785  0.9905335628  0.9299655568  19.773429454  -0.015652926  1200          1.2531149364 
0.0965529040  1.0307785791  0.9953656025  0.9134020619  0.9696449026  0.8682703322  0.9214527027  0.9255918828  0.9928284567  0.9299655568  24.716786817  -0.015739450  1500          1.2149330004 
0.0849854572  1.0521985509  0.9958805355  0.9092783505  0.9742268041  0.8728522337  0.9189189189  0.9255918828  0.9945496271  0.9276693456  29.660144181  -0.015717889  1800          1.2157476147 
0.0658743332  1.0607504878  0.9963954686  0.9154639175  0.9762313860  0.8762886598  0.9163851351  0.9267192785  0.9945496271  0.9253731343  34.603501544  -0.015612815  2100          1.2438868872 
0.0561194423  1.0705535064  0.9963954686  0.9175257732  0.9788087056  0.8762886598  0.9161036036  0.9255918828  0.9948364888  0.9265212400  39.546858908  -0.015635708  2400          1.2509216928 
0.0502920529  1.0717607983  0.9963954686  0.9113402062  0.9819587629  0.8762886598  0.9155405405  0.9210822999  0.9956970740  0.9265212400  44.490216271  -0.015639658  2700          1.2583663630 
0.0418699516  1.0753852055  0.9963954686  0.9072164948  0.9831042383  0.8762886598  0.9166666667  0.9210822999  0.9959839357  0.9253731343  49.433573635  -0.015627261  3000          1.2150015585 
0.0428699095  1.0747470860  0.9963954686  0.9051546392  0.9839633448  0.8762886598  0.9146959459  0.9199549042  0.9962707975  0.9242250287  54.376930999  -0.015583678  3300          1.2158763003 
0.0347107954  1.0744057439  0.9963954686  0.9051546392  0.9856815578  0.8774341352  0.9135698198  0.9177001127  0.9968445209  0.9242250287  59.320288362  -0.015650292  3600          1.2785816900 
0.0376308476  1.0770387212  0.9963954686  0.9072164948  0.9865406644  0.8774341352  0.9124436937  0.9154453213  0.9968445209  0.9253731343  64.263645726  -0.015637757  3900          1.2607204032 
0.0347277527  1.0762123672  0.9963954686  0.9072164948  0.9871134021  0.8774341352  0.9113175676  0.9143179256  0.9971313827  0.9230769231  69.207003089  -0.015642955  4200          1.2153292513 
0.0337531180  1.0779482094  0.9963954686  0.9134020619  0.9873997709  0.8739977090  0.9099099099  0.9143179256  0.9974182444  0.9230769231  74.150360453  -0.015630751  4500          1.2480472517 
0.0306747324  1.0792353904  0.9963954686  0.9154639175  0.9876861397  0.8739977090  0.9087837838  0.9098083427  0.9974182444  0.9230769231  79.093717816  -0.015656982  4800          1.2164243325 
0.0334396706  1.0776019752  0.9963954686  0.9175257732  0.9879725086  0.8751431844  0.9059684685  0.9075535513  0.9974182444  0.9242250287  82.389289392  -0.015714294  5000          1.2216784739 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kl_loss       step          step_time    
1.1406538486  1.2620800734  0.8300720906  0.8350515464  0.7067583047  0.6953035510  0.9048423423  0.8985343856  0.8975903614  0.9092996556  0.0000000000  -0.022908471  0             2.8433680534 
0.4443824541  0.3719279671  0.9624098867  0.8865979381  0.8934707904  0.8316151203  0.9712837838  0.9503945885  0.9234079174  0.9311136625  4.9433573635  -0.017871848  300           1.2050186507 
slurmstepd-gpu-01: error: *** JOB 173299 ON gpu-01 CANCELLED AT 2023-06-25T04:41:23 DUE TO TIME LIMIT ***
