./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.005
	lr_d: 0.001
	lr_g: 0.001
	mlp_depth: 3
	mlp_dropout: 0.1
	mlp_width: 256
	momentum: 0.1
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "cause the presence of"
Number of context words (tokens): 4
class_loss    disc_loss     dist_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
7.1554374695  1.1048107147  0.8762554526  0.0547703180  0.0530035336  0.2522352941  0.2617702448  0.2928408225  0.2850609756  0.2143650500  0.2074074074  0.0000000000  0             2.0333936214 
4.6904688072  0.6870194473  0.8570210389  0.8489399293  0.7667844523  0.7825882353  0.7740112994  0.8065498858  0.8033536585  0.8093298778  0.8207407407  8.4805653710  300           0.7339102348 
4.3890355539  0.7263740106  0.8297946699  0.9770318021  0.9540636042  0.7976470588  0.7984934087  0.8427265804  0.8277439024  0.8741206960  0.8755555556  16.961130742  600           0.7339998206 
4.3154688708  0.7385319718  0.8245418159  0.9787985866  0.9681978799  0.8061176471  0.8041431262  0.8750952018  0.8506097561  0.8907811922  0.8770370370  25.441696113  900           0.7337767227 
4.2224524005  0.8258966225  0.8141978522  0.9275618375  0.9116607774  0.7995294118  0.8041431262  0.8777608530  0.8307926829  0.9041095890  0.8755555556  33.922261484  1200          0.7338570031 
4.3159874296  0.8776225162  0.8069496111  0.9390459364  0.9257950530  0.8122352941  0.7890772128  0.8830921554  0.8368902439  0.8963346909  0.8740740741  42.402826855  1500          0.7345099394 
4.1597189792  0.9015768627  0.8412594235  0.9293286219  0.9187279152  0.8094117647  0.7777777778  0.8937547601  0.8429878049  0.8915216586  0.8711111111  50.883392226  1800          0.7339364449 
4.2444553312  0.9138208584  0.8556262668  0.8878091873  0.8727915194  0.8268235294  0.7890772128  0.8998476771  0.8399390244  0.9111440207  0.8844444444  59.363957597  2100          0.7344931086 
4.1925704225  0.9217115569  0.8552193113  0.8639575972  0.8339222615  0.8272941176  0.8003766478  0.8987052551  0.8323170732  0.9155868197  0.8874074074  67.844522968  2400          0.7335104044 
4.0746587642  0.9297611835  0.8510713518  0.8922261484  0.8798586572  0.8272941176  0.8041431262  0.9074638233  0.8216463415  0.9152165864  0.8933333333  76.325088339  2700          0.7335900434 
4.0844942713  0.9300007258  0.8490582657  0.8577738516  0.8127208481  0.8178823529  0.7627118644  0.9135567403  0.8384146341  0.9233617179  0.8918518519  84.805653710  3000          0.7328326233 
4.2075453599  0.9496555070  0.8528443905  0.8003533569  0.7561837456  0.8390588235  0.7777777778  0.9146991622  0.8292682927  0.9255831174  0.8814814815  93.286219081  3300          0.7334959706 
4.0933511623  0.9338663552  0.8573923353  0.8312720848  0.7915194346  0.8503529412  0.7984934087  0.9093678599  0.8277439024  0.9359496483  0.8814814815  101.76678445  3600          0.7325715121 
4.1124281089  0.9087270637  0.8657715968  0.8454063604  0.7879858657  0.8423529412  0.7796610169  0.9158415842  0.8368902439  0.9422436135  0.8859259259  110.24734982  3900          0.7335383169 
4.0933122110  0.9403541629  0.8601156044  0.7058303887  0.6431095406  0.8310588235  0.7608286252  0.9196496573  0.8216463415  0.9326175491  0.8770370370  118.72791519  4200          0.7333599377 
4.0441981475  0.9340133009  0.8577717694  0.7747349823  0.7208480565  0.8390588235  0.7758945386  0.9215536938  0.8262195122  0.9485375787  0.8859259259  127.20848056  4500          0.7334912364 
4.1363409932  0.9368473548  0.8490144277  0.7924028269  0.7809187279  0.8630588235  0.7853107345  0.9196496573  0.8231707317  0.9437245465  0.8903703704  135.68904593  4800          0.7341026362 
4.1322623181  0.9498126996  0.8577995086  0.8312720848  0.8303886926  0.8508235294  0.7796610169  0.9318354912  0.8201219512  0.9459459459  0.8859259259  141.34275618  5000          0.7331612611 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.005
	lr_d: 0.001
	lr_g: 0.001
	mlp_depth: 3
	mlp_dropout: 0.1
	mlp_width: 256
	momentum: 0.1
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "cause the presence of"
Number of context words (tokens): 4
class_loss    disc_loss     dist_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
6.9225602150  1.1051403284  0.8688564897  0.2040636042  0.2120141343  0.1924705882  0.1958568738  0.3499619193  0.3399390244  0.3054424287  0.3081481481  0.0000000000  0             1.9597134590 
4.6707097467  0.6367412181  0.8677189755  0.9946996466  0.9964664311  0.6494117647  0.6365348399  0.8412033511  0.8094512195  0.8855979267  0.8696296296  8.4805653710  300           0.7333666062 
4.0686479346  0.4829179866  0.8560215819  0.9982332155  1.0000000000  0.5407058824  0.5348399247  0.8168316832  0.7926829268  0.8396890041  0.8296296296  16.961130742  600           0.7330363409 
3.9308255672  0.4955629932  0.8434357897  1.0000000000  1.0000000000  0.6051764706  0.5913370998  0.8697638995  0.8399390244  0.8955942244  0.8681481481  25.441696113  900           0.7334819730 
3.9865066147  0.5387420381  0.8336772533  1.0000000000  1.0000000000  0.6343529412  0.6045197740  0.9047981721  0.8506097561  0.9255831174  0.8977777778  33.922261484  1200          0.7339929779 
4.0541919549  0.5824904706  0.8341372478  1.0000000000  1.0000000000  0.6569411765  0.6290018832  0.9006092917  0.8384146341  0.9252128841  0.8933333333  42.402826855  1500          0.7337920539 
3.9607739226  0.5342402406  0.8097063386  1.0000000000  1.0000000000  0.6960000000  0.6741996234  0.9185072353  0.8429878049  0.9392817475  0.8888888889  50.883392226  1800          0.7340513992 
3.8493722534  0.4923015252  0.7766155930  1.0000000000  0.9964664311  0.6983529412  0.6779661017  0.8948971820  0.8170731707  0.9078119215  0.8696296296  59.363957597  2100          0.7340303294 
3.8415414619  0.4883340523  0.7855320251  1.0000000000  1.0000000000  0.6658823529  0.6647834275  0.9238385377  0.8368902439  0.9422436135  0.8770370370  67.844522968  2400          0.7337477303 
3.8788266118  0.4832202076  0.7968580890  1.0000000000  1.0000000000  0.6851764706  0.6798493409  0.9306930693  0.8445121951  0.9459459459  0.8948148148  76.325088339  2700          0.7343314966 
3.8010643880  0.5202207838  0.8001190913  1.0000000000  0.9964664311  0.6701176471  0.6384180791  0.9032749429  0.8353658537  0.9163272862  0.8785185185  84.805653710  3000          0.7341556684 
3.8770399237  0.4995764122  0.8014594181  0.9991166078  1.0000000000  0.6564705882  0.6403013183  0.9424980960  0.8399390244  0.9574231766  0.8918518519  93.286219081  3300          0.7339604362 
3.7348526780  0.5273341719  0.8106573959  1.0000000000  1.0000000000  0.6738823529  0.6459510358  0.8945163747  0.8109756098  0.9018881896  0.8577777778  101.76678445  3600          0.7339362780 
4.0654227813  0.5632791707  0.8103013210  1.0000000000  1.0000000000  0.6640000000  0.6346516008  0.9421172887  0.8338414634  0.9533506109  0.8785185185  110.24734982  3900          0.7337491123 
3.8477604294  0.5554552710  0.8188153338  1.0000000000  1.0000000000  0.6602352941  0.6327683616  0.9577303884  0.8353658537  0.9633469086  0.8859259259  118.72791519  4200          0.7340099875 
3.8685931365  0.5522715235  0.8087405189  1.0000000000  1.0000000000  0.6512941176  0.6327683616  0.9615384615  0.8262195122  0.9733432062  0.8859259259  127.20848056  4500          0.7336163855 
3.8385180600  0.5540459483  0.8132038466  1.0000000000  0.9964664311  0.6258823529  0.6007532957  0.9619192688  0.8323170732  0.9648278415  0.8918518519  135.68904593  4800          0.7344517414 
3.7506986785  0.5790986936  0.8109614646  0.9991166078  0.9929328622  0.6432941176  0.6252354049  0.9672505712  0.8323170732  0.9677897075  0.8948148148  141.34275618  5000          0.7343798947 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.005
	lr_d: 0.001
	lr_g: 0.001
	mlp_depth: 3
	mlp_dropout: 0.1
	mlp_width: 256
	momentum: 0.1
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "cause the presence of"
Number of context words (tokens): 4
class_loss    disc_loss     dist_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
7.0013527870  1.1039038897  0.8724482656  0.2676678445  0.3144876325  0.3284705882  0.3333333333  0.3530083778  0.3673780488  0.3546834506  0.3407407407  0.0000000000  0             1.8366286755 
4.6961657874  0.6312590389  0.8867870931  0.9955830389  1.0000000000  0.7651764706  0.7664783427  0.7041127190  0.6844512195  0.8656053314  0.8548148148  8.4805653710  300           0.7338046678 
4.2082987833  0.5389679114  0.8799164844  0.9938162544  0.9929328622  0.7642352941  0.7363465160  0.6915460777  0.6676829268  0.8230285080  0.8192592593  16.961130742  600           0.7335727954 
4.1229617627  0.5097193469  0.8758546233  1.0000000000  1.0000000000  0.8004705882  0.7890772128  0.7052551409  0.6753048780  0.8644946316  0.8488888889  25.441696113  900           0.7328265317 
4.1303625218  0.5243359351  0.8701840699  0.9991166078  0.9964664311  0.8221176471  0.7890772128  0.7555217060  0.7225609756  0.9141058867  0.8829629630  33.922261484  1200          0.7334817457 
3.9734335820  0.5060739049  0.8550618851  0.9982332155  1.0000000000  0.7529411765  0.7476459510  0.7151561310  0.6920731707  0.8974453906  0.8622222222  42.402826855  1500          0.7330456082 
4.1011641343  0.5290383640  0.8677493942  1.0000000000  1.0000000000  0.7543529412  0.7325800377  0.7105864433  0.6875000000  0.9122547205  0.8666666667  50.883392226  1800          0.7343640645 
3.9899287748  0.5630237607  0.8628709940  1.0000000000  1.0000000000  0.8418823529  0.8079096045  0.7361005331  0.7088414634  0.9241021844  0.8681481481  59.363957597  2100          0.7346674164 
3.9442928648  0.5433670515  0.8566594736  1.0000000000  1.0000000000  0.8202352941  0.7645951036  0.7147753237  0.7149390244  0.9163272862  0.8592592593  67.844522968  2400          0.7340371585 
3.9303247166  0.5324303916  0.8526926911  0.9973498233  1.0000000000  0.8338823529  0.7890772128  0.7368621478  0.7073170732  0.9352091818  0.8681481481  76.325088339  2700          0.7341295711 
3.9696539068  0.5726755025  0.8528944341  1.0000000000  1.0000000000  0.8348235294  0.7796610169  0.7288651942  0.7164634146  0.9274342836  0.8711111111  84.805653710  3000          0.7334893576 
3.9070200920  0.6035272322  0.8620275891  1.0000000000  1.0000000000  0.7957647059  0.7495291902  0.7178217822  0.7088414634  0.9255831174  0.8711111111  93.286219081  3300          0.7332679431 
4.0012452952  0.6763686725  0.8560224501  0.9982332155  1.0000000000  0.8644705882  0.7721280603  0.7364813404  0.7073170732  0.9381710478  0.8696296296  101.76678445  3600          0.7343070658 
3.9826258802  0.8047180428  0.8596420904  0.9946996466  1.0000000000  0.8757647059  0.7890772128  0.7364813404  0.7240853659  0.9411329137  0.8681481481  110.24734982  3900          0.7344450283 
3.9877933884  0.8662473065  0.8698219693  0.9982332155  1.0000000000  0.8818823529  0.7909604520  0.7368621478  0.7164634146  0.9418733802  0.8785185185  118.72791519  4200          0.7336882257 
3.9773285564  0.8678652434  0.8640900167  0.9991166078  1.0000000000  0.9007058824  0.7890772128  0.7402894136  0.7301829268  0.9485375787  0.8696296296  127.20848056  4500          0.7340546759 
3.8886281554  0.8660524160  0.8645121578  0.9955830389  0.9964664311  0.9054117647  0.7966101695  0.7463823305  0.7469512195  0.9596445761  0.8755555556  135.68904593  4800          0.7344330462 
3.9081316972  0.8896157464  0.8609416628  0.9911660777  0.9964664311  0.8969411765  0.8003766478  0.7608530084  0.7469512195  0.9603850426  0.8800000000  141.34275618  5000          0.7353034782 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.005
	lr_d: 0.001
	lr_g: 0.001
	mlp_depth: 3
	mlp_dropout: 0.1
	mlp_width: 256
	momentum: 0.1
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "cause the presence of"
Number of context words (tokens): 4
class_loss    disc_loss     dist_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
7.0729947090  1.1023695469  0.8653194904  0.2031802120  0.2155477032  0.2616470588  0.2768361582  0.3610053313  0.3582317073  0.2972972973  0.3051851852  0.0000000000  0             1.9001445770 
4.6191337872  0.5586950969  0.8767455705  0.9982332155  0.9964664311  0.7520000000  0.7514124294  0.8046458492  0.7972560976  0.7911884487  0.8118518519  8.4805653710  300           0.7343004974 
4.0346595097  0.4464768893  0.8578305074  1.0000000000  1.0000000000  0.7360000000  0.6986817326  0.8164508759  0.7957317073  0.7752684191  0.7777777778  16.961130742  600           0.7341569908 
4.0380061595  0.4439976626  0.8484443649  1.0000000000  0.9964664311  0.7835294118  0.7721280603  0.8575780655  0.8155487805  0.8267308404  0.8370370370  25.441696113  900           0.7343527166 
4.0193060064  0.4469255201  0.8340018678  0.9982332155  0.9964664311  0.8155294118  0.8003766478  0.8933739528  0.8384146341  0.8226582747  0.8325925926  33.922261484  1200          0.7344066477 
3.8820501216  0.4476791688  0.8412728592  1.0000000000  0.9964664311  0.8192941176  0.7966101695  0.9002284844  0.8490853659  0.7967419474  0.8148148148  42.402826855  1500          0.7335678633 
4.0452918927  0.4647395494  0.8591412870  1.0000000000  0.9964664311  0.6865882353  0.6836158192  0.8156892612  0.7743902439  0.7619400222  0.7851851852  50.883392226  1800          0.7346567019 
3.9373672311  0.4539271291  0.8514710927  1.0000000000  0.9964664311  0.8296470588  0.7740112994  0.9040365575  0.8338414634  0.7848944835  0.7911111111  59.363957597  2100          0.7349438961 
3.9725580057  0.5139604899  0.8434661341  1.0000000000  0.9964664311  0.8235294118  0.7853107345  0.9070830160  0.8277439024  0.7941503147  0.8088888889  67.844522968  2400          0.7352765743 
3.9816980251  0.5118686179  0.8301413333  1.0000000000  0.9964664311  0.8418823529  0.7721280603  0.9169840061  0.8368902439  0.7871158830  0.8014814815  76.325088339  2700          0.7345745428 
3.8900309436  0.4881735732  0.8247015166  1.0000000000  1.0000000000  0.8541176471  0.7928436911  0.9116527037  0.8277439024  0.7915586820  0.7881481481  84.805653710  3000          0.7354096460 
3.8945744562  0.5019773715  0.8364394836  1.0000000000  1.0000000000  0.8644705882  0.8135593220  0.9246001523  0.8307926829  0.8071084783  0.8162962963  93.286219081  3300          0.7350347098 
3.9417721748  0.4863211594  0.8132291563  1.0000000000  0.9964664311  0.8691764706  0.7966101695  0.9352627570  0.8201219512  0.7993335802  0.8148148148  101.76678445  3600          0.7352738706 
3.8355737543  0.4889264856  0.7985305953  1.0000000000  1.0000000000  0.8583529412  0.7721280603  0.9325971059  0.8140243902  0.8082191781  0.8103703704  110.24734982  3900          0.7346237032 
3.8762329356  0.4946415874  0.7966924493  0.9929328622  0.9893992933  0.8395294118  0.7551789077  0.9162223915  0.8018292683  0.7841540170  0.7748148148  118.72791519  4200          0.7350444667 
3.8976811298  0.5049567566  0.8284086140  0.9991166078  1.0000000000  0.8512941176  0.7419962335  0.9367859863  0.8079268293  0.7952610144  0.7866666667  127.20848056  4500          0.7355384262 
3.8435287269  0.4987513595  0.8388470809  0.9946996466  0.9964664311  0.8663529412  0.7476459510  0.9345011424  0.8018292683  0.7867456498  0.7688888889  135.68904593  4800          0.7343793790 
3.9187376642  0.4748154576  0.8375283122  1.0000000000  1.0000000000  0.8734117647  0.7325800377  0.9474485910  0.8048780488  0.7930396150  0.7911111111  141.34275618  5000          0.7331913245 
