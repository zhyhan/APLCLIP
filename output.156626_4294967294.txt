./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CoCoOpCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	data_augmentation: True
	lr: 0.001
	mlp_depth: 3
	mlp_dropout: 0.1
	mlp_width: 512
	momentum: 0.1
	nonlinear_classifier: False
	num_domain_tokens: 16
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "a photo of a"
Number of context words (tokens): 4
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          step          step_time    
0.5945229682  0.5936395760  0.4814117647  0.4595103578  0.5144706778  0.5274390244  0.5568308034  0.5674074074  0.0000000000  1.3760386705  0             1.4529082775 
0.9408127208  0.9399293286  0.6767058824  0.7024482109  0.7235338919  0.7073170732  0.7567567568  0.7718518519  8.4805653710  0.5740692938  300           0.4529184484 
0.9593639576  0.9469964664  0.6781176471  0.6873822976  0.7345773039  0.7240853659  0.7745279526  0.7911111111  16.961130742  0.5380056226  600           0.4504019260 
0.9743816254  0.9717314488  0.6847058824  0.6911487759  0.7528560548  0.7271341463  0.7915586820  0.8088888889  25.441696113  0.5111018291  900           0.4543985518 
0.9770318021  0.9752650177  0.6870588235  0.6967984934  0.7520944402  0.7393292683  0.7941503147  0.8177777778  33.922261484  0.4949727360  1200          0.4562332106 
0.9770318021  0.9681978799  0.6894117647  0.6949152542  0.7589489718  0.7408536585  0.7982228804  0.8148148148  42.402826855  0.4904989140  1500          0.3967677800 
0.9823321555  0.9823321555  0.6889411765  0.6911487759  0.7539984768  0.7378048780  0.8000740466  0.8207407407  50.883392226  0.4779609464  1800          0.3985597229 
0.9814487633  0.9858657244  0.6912941176  0.6986817326  0.7578065499  0.7439024390  0.8082191781  0.8266666667  59.363957597  0.4746817180  2100          0.4476964021 
0.9832155477  0.9929328622  0.6964705882  0.6967984934  0.7608530084  0.7378048780  0.8111810441  0.8266666667  67.844522968  0.4649076784  2400          0.4457710950 
0.9805653710  0.9823321555  0.7021176471  0.7043314501  0.7578065499  0.7301829268  0.8156238430  0.8148148148  76.325088339  0.4495931040  2700          0.4511759130 
0.9867491166  0.9858657244  0.7035294118  0.7024482109  0.7578065499  0.7286585366  0.8141429100  0.8281481481  84.805653710  0.4516507374  3000          0.4444776678 
0.9893992933  0.9893992933  0.7040000000  0.7062146893  0.7631378522  0.7256097561  0.8119215106  0.8355555556  93.286219081  0.4456204010  3300          0.4514728443 
0.9858657244  0.9893992933  0.7087058824  0.7212806026  0.7638994669  0.7256097561  0.8174750093  0.8266666667  101.76678445  0.4446282206  3600          0.4501188986 
0.9911660777  0.9964664311  0.7105882353  0.7212806026  0.7619954303  0.7271341463  0.8182154757  0.8340740741  110.24734982  0.4454416043  3900          0.4375015720 
0.9867491166  0.9929328622  0.7162352941  0.7175141243  0.7623762376  0.7378048780  0.8167345428  0.8281481481  118.72791519  0.4413326246  4200          0.3969980717 
0.9911660777  0.9964664311  0.7124705882  0.7231638418  0.7623762376  0.7362804878  0.8126619770  0.8251851852  127.20848056  0.4401236254  4500          0.3972209867 
0.9876325088  0.9929328622  0.7101176471  0.7175141243  0.7677075400  0.7408536585  0.8152536098  0.8296296296  135.68904593  0.4363256457  4800          0.4413378127 
0.9885159011  0.9964664311  0.7218823529  0.7193973635  0.7677075400  0.7545731707  0.8208071085  0.8266666667  141.34275618  0.4229005443  5000          0.4483230639 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CoCoOpCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone": "clip"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	class_balanced: False
	clip_transform: True
	data_augmentation: True
	lr: 0.001
	mlp_depth: 3
	mlp_dropout: 0.1
	mlp_width: 512
	momentum: 0.1
	nonlinear_classifier: False
	num_domain_tokens: 16
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 240, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 213, in __init__
    print('Using clip_transform', hparams['clip_backbone'])
                                  ~~~~~~~^^^^^^^^^^^^^^^^^
KeyError: 'clip_backbone'
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CoCoOpCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone": "clip"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	class_balanced: False
	clip_transform: True
	data_augmentation: True
	lr: 0.001
	mlp_depth: 3
	mlp_dropout: 0.1
	mlp_width: 512
	momentum: 0.1
	nonlinear_classifier: False
	num_domain_tokens: 16
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 240, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 213, in __init__
    print('Using clip_transform', hparams['clip_backbone'])
                                  ~~~~~~~^^^^^^^^^^^^^^^^^
KeyError: 'clip_backbone'
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CoCoOpCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"backbone": "clip"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	class_balanced: False
	clip_transform: True
	data_augmentation: True
	lr: 0.001
	mlp_depth: 3
	mlp_dropout: 0.1
	mlp_width: 512
	momentum: 0.1
	nonlinear_classifier: False
	num_domain_tokens: 16
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 240, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 213, in __init__
    print('Using clip_transform', hparams['clip_backbone'])
                                  ~~~~~~~^^^^^^^^^^^^^^^^^
KeyError: 'clip_backbone'
