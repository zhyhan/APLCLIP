./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kl_loss       step          step_time    
0.3703275621  1.1159186363  0.9984301413  1.0000000000  0.7151235861  0.6905660377  0.7786052810  0.7682926829  0.8838433695  0.9139465875  0.0000000000  0.7156496048  0             5.2922086716 
0.1593381026  0.2872487142  1.0000000000  1.0000000000  0.6816087139  0.6415094340  0.9377115775  0.8902439024  0.9605133268  0.9554896142  7.5353218210  0.1580373051  300           1.3263759136 
0.0746920769  0.7419894784  1.0000000000  1.0000000000  0.6862170088  0.6339622642  0.9631008802  0.9085365854  0.9769661073  0.9584569733  15.070643642  0.1190440481  600           1.3271896672 
0.0454801708  0.9615566903  1.0000000000  1.0000000000  0.6816087139  0.6339622642  0.9763033175  0.9146341463  0.9878249424  0.9554896142  22.605965463  0.0957329425  900           1.3269380554 
0.0320766393  1.0049694252  1.0000000000  1.0000000000  0.6807708421  0.6339622642  0.9830737982  0.9146341463  0.9901283317  0.9584569733  30.141287284  0.0780173409  1200          1.3275186602 
0.0278798644  1.0087568190  1.0000000000  1.0000000000  0.6820276498  0.6301886792  0.9861205146  0.9146341463  0.9924317210  0.9584569733  37.676609105  0.0657321522  1500          1.3280184444 
0.0232159244  1.0159543141  1.0000000000  1.0000000000  0.6811897780  0.6301886792  0.9908598510  0.9176829268  0.9944060546  0.9584569733  45.211930926  0.0581294902  1800          1.3278236246 
0.0185236955  1.0154540696  1.0000000000  1.0000000000  0.6816087139  0.6301886792  0.9928909953  0.9207317073  0.9960513327  0.9584569733  52.747252747  0.0452972323  2100          1.3279355550 
0.0132920774  1.0115448763  1.0000000000  1.0000000000  0.6803519062  0.6377358491  0.9939065674  0.9115853659  0.9970384995  0.9584569733  60.282574568  0.0362806665  2400          1.3284853681 
0.0105596059  1.0088621054  1.0000000000  1.0000000000  0.6803519062  0.6377358491  0.9959377116  0.9054878049  0.9973675551  0.9584569733  67.817896389  0.0334467403  2700          1.3284492159 
0.0063480353  1.0085263018  1.0000000000  1.0000000000  0.6790950984  0.6301886792  0.9972918077  0.9085365854  0.9976966107  0.9584569733  75.353218210  0.0271783625  3000          1.3278654130 
0.0079110643  1.0067720155  1.0000000000  1.0000000000  0.6795140344  0.6264150943  0.9976303318  0.9054878049  0.9976966107  0.9584569733  82.888540031  0.0253980024  3300          1.3280970343 
0.0059213126  1.0101957164  1.0000000000  1.0000000000  0.6790950984  0.6301886792  0.9986459039  0.9085365854  0.9976966107  0.9584569733  90.423861852  0.0244183117  3600          1.3289486631 
0.0060387528  1.0027667340  1.0000000000  1.0000000000  0.6778382907  0.6339622642  0.9986459039  0.9085365854  0.9986837776  0.9584569733  97.959183673  0.0216691734  3900          1.3285715874 
0.0050549094  1.0041906476  1.0000000000  1.0000000000  0.6778382907  0.6339622642  0.9993229519  0.9115853659  0.9990128332  0.9584569733  105.49450549  0.0215093856  4200          1.3285090995 
0.0044642983  1.0069340026  1.0000000000  1.0000000000  0.6790950984  0.6339622642  0.9996614760  0.9115853659  0.9993418888  0.9554896142  113.02982731  0.0184555773  4500          1.3293749722 
0.0051573140  1.0102891956  1.0000000000  1.0000000000  0.6786761625  0.6339622642  0.9996614760  0.9115853659  0.9996709444  0.9525222552  120.56514913  0.0229552572  4800          1.3290202252 
0.0053820613  1.0064936396  1.0000000000  1.0000000000  0.6778382907  0.6339622642  0.9996614760  0.9115853659  1.0000000000  0.9525222552  125.58869701  0.0240953933  5000          1.3289789903 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kl_loss       step          step_time    
0.5189587474  1.1158493757  0.9976452119  1.0000000000  0.7419354839  0.7283018868  0.7149627624  0.6829268293  0.8752879237  0.9020771513  0.0000000000  0.6299413443  0             7.1591565609 
0.2256594881  0.2433491660  1.0000000000  1.0000000000  0.8743192292  0.8000000000  0.8016249154  0.7743902439  0.9453767687  0.9347181009  7.5353218210  0.1864786346  300           1.3319385997 
0.1114637426  0.7213811161  1.0000000000  1.0000000000  0.9141181399  0.8226415094  0.8056872038  0.7743902439  0.9641329385  0.9525222552  15.070643642  0.1455363156  600           1.3294558724 
0.0681221798  0.9799678369  1.0000000000  1.0000000000  0.9426057813  0.8339622642  0.8097494922  0.7774390244  0.9812438302  0.9525222552  22.605965463  0.1240495072  900           1.3323485390 
0.0467230725  1.0091055419  1.0000000000  1.0000000000  0.9581064097  0.8377358491  0.8100880162  0.7774390244  0.9874958868  0.9525222552  30.141287284  0.0966229818  1200          1.3298168747 
0.0306720780  1.0144415321  1.0000000000  1.0000000000  0.9698366150  0.8339622642  0.8077183480  0.7713414634  0.9901283317  0.9495548961  37.676609105  0.0756728078  1500          1.3306260602 
0.0220955851  1.0213848082  1.0000000000  1.0000000000  0.9777963972  0.8301886792  0.8077183480  0.7743902439  0.9921026654  0.9465875371  45.211930926  0.0575932740  1800          1.3291585294 
0.0188793953  1.0110189088  1.0000000000  1.0000000000  0.9836614998  0.8301886792  0.8073798240  0.7621951220  0.9944060546  0.9525222552  52.747252747  0.0555526480  2100          1.3369128585 
0.0161010348  1.0167457589  1.0000000000  1.0000000000  0.9874319229  0.8339622642  0.8053486798  0.7652439024  0.9957222771  0.9525222552  60.282574568  0.0438307075  2400          1.3533232117 
0.0120687399  1.0173776488  1.0000000000  1.0000000000  0.9912023460  0.8264150943  0.8060257278  0.7682926829  0.9967094439  0.9495548961  67.817896389  0.0351578209  2700          1.3582746482 
0.0093522793  1.0173654087  1.0000000000  1.0000000000  0.9932970256  0.8226415094  0.8046716317  0.7743902439  0.9973675551  0.9495548961  75.353218210  0.0317444934  3000          1.3463826458 
0.0074124570  1.0166319038  1.0000000000  1.0000000000  0.9949727692  0.8226415094  0.8067027759  0.7743902439  0.9980256663  0.9465875371  82.888540031  0.0273932768  3300          1.3508747244 
0.0075287112  1.0118672238  1.0000000000  1.0000000000  0.9958106410  0.8226415094  0.8067027759  0.7743902439  0.9986837776  0.9465875371  90.423861852  0.0240895721  3600          1.3303997723 
0.0063075366  1.0157002048  1.0000000000  1.0000000000  0.9966485128  0.8226415094  0.8070412999  0.7774390244  0.9990128332  0.9436201780  97.959183673  0.0225217499  3900          1.3423444422 
0.0066242111  1.0149210519  1.0000000000  1.0000000000  0.9979053205  0.8226415094  0.8053486798  0.7804878049  0.9993418888  0.9436201780  105.49450549  0.0233433931  4200          1.3561086718 
0.0064582431  1.0171522776  1.0000000000  1.0000000000  0.9987431923  0.8301886792  0.8046716317  0.7804878049  0.9993418888  0.9436201780  113.02982731  0.0228522094  4500          1.4152146244 
0.0067084041  1.0209849183  1.0000000000  1.0000000000  0.9991621282  0.8301886792  0.8053486798  0.7743902439  0.9996709444  0.9436201780  120.56514913  0.0260451536  4800          1.3793173401 
0.0050102116  1.0126051775  1.0000000000  1.0000000000  0.9995810641  0.8264150943  0.8036560596  0.7682926829  1.0000000000  0.9436201780  125.58869701  0.0187373665  5000          1.3782323492 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kl_loss       step          step_time    
0.6582040191  1.1072759628  0.9984301413  1.0000000000  0.7398408044  0.7358490566  0.7241029113  0.7042682927  0.8700230339  0.8931750742  0.0000000000  0.6776674986  0             2.9132616520 
0.2559516808  0.1762808999  1.0000000000  1.0000000000  0.8810222036  0.8188679245  0.9326337170  0.8841463415  0.8897663705  0.9228486647  7.5353218210  0.1948202389  300           1.3837073572 
0.1191490812  0.5698508978  1.0000000000  1.0000000000  0.9204021785  0.8264150943  0.9587000677  0.8993902439  0.8825271471  0.9050445104  15.070643642  0.1470313096  600           1.3497483047 
0.0745926627  0.9452229761  1.0000000000  1.0000000000  0.9467951403  0.8339622642  0.9763033175  0.8993902439  0.8772622573  0.9080118694  22.605965463  0.1346517929  900           1.4321555225 
0.0509291460  0.9636706219  1.0000000000  1.0000000000  0.9627147047  0.8377358491  0.9844278944  0.9024390244  0.8739717012  0.9020771513  30.141287284  0.1088589778  1200          1.3541280961 
0.0360402900  0.9776645716  1.0000000000  1.0000000000  0.9748638458  0.8377358491  0.9878131347  0.9054878049  0.8690358671  0.8991097923  37.676609105  0.0861344194  1500          1.3461698540 
0.0283306178  0.9688215963  1.0000000000  1.0000000000  0.9794721408  0.8339622642  0.9908598510  0.9054878049  0.8677196446  0.8991097923  45.211930926  0.0721700793  1800          1.3452361401 
0.0213966485  0.9737916956  1.0000000000  1.0000000000  0.9853372434  0.8301886792  0.9935680433  0.9085365854  0.8621256992  0.8991097923  52.747252747  0.0594697503  2100          1.3573908456 
0.0166125127  0.9733779883  1.0000000000  1.0000000000  0.9870129870  0.8339622642  0.9949221395  0.9085365854  0.8598223100  0.8902077151  60.282574568  0.0529318032  2400          1.3545191646 
0.0127420026  0.9784350550  1.0000000000  1.0000000000  0.9907834101  0.8339622642  0.9966147596  0.9024390244  0.8565317539  0.8842729970  67.817896389  0.0418714786  2700          1.3369695250 
0.0083019346  0.9693764420  1.0000000000  1.0000000000  0.9920402178  0.8264150943  0.9976303318  0.9024390244  0.8552155314  0.8842729970  75.353218210  0.0375195233  3000          1.3504440538 
0.0088483522  0.9687311874  1.0000000000  1.0000000000  0.9928780897  0.8188679245  0.9979688558  0.9054878049  0.8552155314  0.8872403561  82.888540031  0.0316049778  3300          1.3454800797 
0.0067152022  0.9705095828  1.0000000000  1.0000000000  0.9962295769  0.8188679245  0.9979688558  0.8993902439  0.8512668641  0.8872403561  90.423861852  0.0280139254  3600          1.3486137533 
0.0069239908  0.9704778556  1.0000000000  1.0000000000  0.9979053205  0.8188679245  0.9986459039  0.8993902439  0.8499506417  0.8813056380  97.959183673  0.0286677100  3900          1.3348021452 
0.0068347602  0.9651856971  1.0000000000  1.0000000000  0.9983242564  0.8150943396  0.9989844279  0.8993902439  0.8489634748  0.8783382789  105.49450549  0.0273234601  4200          1.3438925687 
0.0058326300  0.9716688333  1.0000000000  1.0000000000  0.9987431923  0.8150943396  0.9989844279  0.9024390244  0.8509378085  0.8753709199  113.02982731  0.0241913320  4500          1.3353540031 
0.0068313931  0.9708792794  1.0000000000  1.0000000000  0.9987431923  0.8188679245  0.9993229519  0.8932926829  0.8509378085  0.8783382789  120.56514913  0.0253909868  4800          1.3299492176 
0.0038790601  0.9657403100  1.0000000000  1.0000000000  1.0000000000  0.8188679245  0.9996614760  0.8902439024  0.8476472524  0.8724035608  125.58869701  0.0250689719  5000          1.3315034044 
