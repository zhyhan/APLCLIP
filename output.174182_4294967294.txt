./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kl_loss       step          step_time    
0.7974040508  1.0789757967  0.8297482838  0.8512396694  0.6983965386  0.6926605505  0.8973973974  0.8848758465  0.8980112188  0.9080459770  0.0000000000  1.5367023945  0             18.879538536 
0.3664390877  0.5856317743  0.8768878719  0.8347107438  0.8867396284  0.8142201835  0.9672172172  0.9480812641  0.9576746558  0.9402298851  4.3935926773  0.4618605744  300           1.3380712167 
0.1757740224  0.7657100912  0.8759725400  0.8388429752  0.9241537287  0.8486238532  0.9794794795  0.9571106095  0.9722080571  0.9471264368  8.7871853547  0.2340535858  600           1.3966586208 
0.1103109220  0.8811940014  0.8791762014  0.8553719008  0.9452786969  0.8555045872  0.9857357357  0.9616252822  0.9808771035  0.9425287356  13.180778032  0.1791306454  900           1.3290689858 
0.0929457144  0.9617113191  0.8778032037  0.8595041322  0.9574955459  0.8692660550  0.9884884885  0.9638826185  0.9885262621  0.9448275862  17.574370709  0.1501268193  1200          1.3311423532 
0.0721114568  1.0114914882  0.8823798627  0.8595041322  0.9636039705  0.8738532110  0.9904904905  0.9661399549  0.9910759816  0.9402298851  21.967963386  0.1230604789  1500          1.3317129874 
0.0596458006  1.0399596834  0.8828375286  0.8595041322  0.9692033596  0.8807339450  0.9927427427  0.9729119639  0.9926058134  0.9402298851  26.361556064  0.0996754434  1800          1.3330853295 
0.0470806942  1.0580397111  0.8805491991  0.8636363636  0.9730211250  0.8807339450  0.9937437437  0.9751693002  0.9928607853  0.9402298851  30.755148741  0.0762387921  2100          1.3324141320 
0.0426370393  1.0648083232  0.8810068650  0.8595041322  0.9765843726  0.8807339450  0.9947447447  0.9729119639  0.9943906170  0.9402298851  35.148741418  0.0651429306  2400          1.3322640459 
0.0340365445  1.0710481254  0.8800915332  0.8636363636  0.9793840672  0.8830275229  0.9947447447  0.9729119639  0.9943906170  0.9425287356  39.542334096  0.0527303535  2700          1.3327574547 
0.0341939652  1.0725409053  0.8791762014  0.8636363636  0.9801476203  0.8830275229  0.9949949950  0.9729119639  0.9946455890  0.9379310345  43.935926773  0.0503818403  3000          1.3390474423 
0.0320360373  1.0731539456  0.8791762014  0.8677685950  0.9806566556  0.8830275229  0.9957457457  0.9729119639  0.9946455890  0.9356321839  48.329519450  0.0418217365  3300          1.3338657649 
0.0323403309  1.0760147385  0.8787185355  0.8677685950  0.9821837618  0.8830275229  0.9964964965  0.9751693002  0.9949005609  0.9379310345  52.723112128  0.0426733965  3600          1.3318691516 
0.0302899863  1.0786799765  0.8773455378  0.8677685950  0.9824382795  0.8830275229  0.9969969970  0.9774266366  0.9951555329  0.9379310345  57.116704805  0.0375132841  3900          1.3326017475 
0.0269063868  1.0770091176  0.8736842105  0.8677685950  0.9832018325  0.8830275229  0.9969969970  0.9774266366  0.9954105048  0.9379310345  61.510297482  0.0312858906  4200          1.3333077923 
0.0252700800  1.0728845688  0.8741418764  0.8677685950  0.9839653856  0.8784403670  0.9974974975  0.9751693002  0.9956654768  0.9356321839  65.903890160  0.0345963016  4500          1.3314230092 
0.0256629984  1.0760084629  0.8732265446  0.8677685950  0.9847289387  0.8784403670  0.9974974975  0.9751693002  0.9961754207  0.9379310345  70.297482837  0.0365560306  4800          1.3309784444 
0.0216243447  1.0775523654  0.8732265446  0.8677685950  0.9847289387  0.8830275229  0.9977477477  0.9774266366  0.9964303927  0.9310344828  73.226544622  0.0233813593  5000          1.3311965024 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kl_loss       step          step_time    
0.5174990296  1.1409356594  0.8356979405  0.8512396694  0.6884703487  0.6857798165  0.8963963964  0.8848758465  0.8944416114  0.9103448276  0.0000000000  1.4137797356  0             2.1807157993 
0.2557645037  0.6648363621  0.9624713959  0.8719008264  0.7493000764  0.7224770642  0.9689689690  0.9571106095  0.9574196838  0.9425287356  4.3935926773  0.3854813540  300           1.3300034269 
0.1188757624  0.7591752517  0.9803203661  0.9132231405  0.7510817002  0.7408256881  0.9814814815  0.9616252822  0.9760326364  0.9494252874  8.7871853547  0.1981607956  600           1.3294609586 
0.0771239937  0.8429020649  0.9885583524  0.9090909091  0.7495545940  0.7431192661  0.9869869870  0.9661399549  0.9847016828  0.9471264368  13.180778032  0.1524256006  900           1.3305083179 
0.0577712335  0.9548411014  0.9917620137  0.9090909091  0.7472639348  0.7431192661  0.9912412412  0.9661399549  0.9890362060  0.9494252874  17.574370709  0.1226002104  1200          1.3314508088 
0.0467128789  1.0011380517  0.9922196796  0.9049586777  0.7470094172  0.7385321101  0.9922422422  0.9661399549  0.9910759816  0.9494252874  21.967963386  0.1078252986  1500          1.3320810533 
0.0375604736  1.0329040062  0.9940503432  0.9090909091  0.7498091117  0.7408256881  0.9937437437  0.9683972912  0.9926058134  0.9448275862  26.361556064  0.0823186766  1800          1.3319853107 
0.0318403620  1.0495892098  0.9954233410  0.9090909091  0.7498091117  0.7431192661  0.9944944945  0.9683972912  0.9933707292  0.9448275862  30.755148741  0.0674102735  2100          1.3320773101 
0.0320954290  1.0553954562  0.9954233410  0.9090909091  0.7475184525  0.7408256881  0.9949949950  0.9683972912  0.9943906170  0.9448275862  35.148741418  0.0537857979  2400          1.3324215953 
0.0226212701  1.0604149997  0.9954233410  0.9132231405  0.7477729702  0.7385321101  0.9954954955  0.9706546275  0.9946455890  0.9471264368  39.542334096  0.0480847140  2700          1.3321056922 
0.0201930263  1.0646986632  0.9958810069  0.9132231405  0.7457368287  0.7362385321  0.9959959960  0.9706546275  0.9949005609  0.9448275862  43.935926773  0.0390867885  3000          1.3325056481 
0.0240664029  1.0658038149  0.9958810069  0.9090909091  0.7467548995  0.7339449541  0.9964964965  0.9706546275  0.9954105048  0.9471264368  48.329519450  0.0374509494  3300          1.3316785995 
0.0185203752  1.0663768093  0.9958810069  0.9132231405  0.7462458641  0.7339449541  0.9964964965  0.9706546275  0.9954105048  0.9471264368  52.723112128  0.0319434876  3600          1.3315355571 
0.0138394817  1.0660733249  0.9958810069  0.9173553719  0.7472639348  0.7385321101  0.9972472472  0.9706546275  0.9954105048  0.9471264368  57.116704805  0.0273024187  3900          1.3312108151 
0.0175073204  1.0711593006  0.9958810069  0.9173553719  0.7475184525  0.7385321101  0.9972472472  0.9706546275  0.9959204488  0.9471264368  61.510297482  0.0313820107  4200          1.3322305624 
0.0154003560  1.0665254897  0.9963386728  0.9173553719  0.7462458641  0.7339449541  0.9974974975  0.9706546275  0.9959204488  0.9448275862  65.903890160  0.0312727890  4500          1.3317324424 
0.0137622866  1.0720287287  0.9963386728  0.9173553719  0.7470094172  0.7339449541  0.9979979980  0.9729119639  0.9964303927  0.9448275862  70.297482837  0.0246973486  4800          1.3321300896 
0.0144327903  1.0711687052  0.9963386728  0.9132231405  0.7457368287  0.7316513761  0.9979979980  0.9751693002  0.9964303927  0.9448275862  73.226544622  0.0283889580  5000          1.3317212069 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kl_loss       step          step_time    
0.8603046536  1.1822209358  0.8324942792  0.8512396694  0.6950878086  0.6834862385  0.8981481481  0.8893905192  0.8980112188  0.9080459770  0.0000000000  1.8752620220  0             2.7892117500 
0.3791367050  0.5371950534  0.9537757437  0.8719008264  0.8824128277  0.8256880734  0.9226726727  0.9209932280  0.9579296277  0.9264367816  4.3935926773  0.4925378275  300           1.3302166788 
0.1640704616  0.7812442215  0.9794050343  0.8842975207  0.9254263171  0.8532110092  0.9254254254  0.9209932280  0.9747577766  0.9287356322  8.7871853547  0.2349859406  600           1.3311404808 
0.1072830590  0.8733635789  0.9858123570  0.8842975207  0.9406973785  0.8715596330  0.9259259259  0.9232505643  0.9816420194  0.9356321839  13.180778032  0.1748310916  900           1.3304255446 
0.0767832806  0.9547257278  0.9903890160  0.8884297521  0.9544413337  0.8715596330  0.9231731732  0.9232505643  0.9864864865  0.9379310345  17.574370709  0.1427488530  1200          1.3295126454 
0.0618285659  1.0027508529  0.9922196796  0.8966942149  0.9618223467  0.8807339450  0.9241741742  0.9232505643  0.9900560938  0.9379310345  21.967963386  0.1143181431  1500          1.3307912509 
0.0521219279  1.0309419153  0.9926773455  0.9008264463  0.9692033596  0.8853211009  0.9239239239  0.9187358916  0.9915859255  0.9333333333  26.361556064  0.0892508708  1800          1.3299757735 
0.0410900217  1.0462318772  0.9935926773  0.9008264463  0.9725120896  0.8922018349  0.9229229229  0.9209932280  0.9926058134  0.9333333333  30.755148741  0.0687825471  2100          1.3294078437 
0.0411951783  1.0561681310  0.9949656751  0.9090909091  0.9745482311  0.8967889908  0.9226726727  0.9187358916  0.9931157573  0.9333333333  35.148741418  0.0629200844  2400          1.3301514912 
0.0360378523  1.0588694030  0.9949656751  0.9090909091  0.9763298549  0.8990825688  0.9221721722  0.9187358916  0.9936257012  0.9333333333  39.542334096  0.0572902880  2700          1.3303510396 
0.0338807811  1.0627460384  0.9949656751  0.9090909091  0.9781114787  0.8990825688  0.9214214214  0.9187358916  0.9938806731  0.9310344828  43.935926773  0.0496473905  3000          1.3300013916 
0.0316441307  1.0678870608  0.9954233410  0.9090909091  0.9793840672  0.8990825688  0.9189189189  0.9187358916  0.9941356451  0.9310344828  48.329519450  0.0397796946  3300          1.3295992406 
0.0320695491  1.0669855366  0.9954233410  0.9090909091  0.9809111733  0.8967889908  0.9186686687  0.9164785553  0.9941356451  0.9333333333  52.723112128  0.0387152214  3600          1.3303998590 
0.0266127585  1.0712257665  0.9958810069  0.9008264463  0.9819292441  0.8922018349  0.9191691692  0.9119638826  0.9946455890  0.9356321839  57.116704805  0.0318857928  3900          1.3301901491 
0.0296741016  1.0728859254  0.9963386728  0.9008264463  0.9834563502  0.8899082569  0.9181681682  0.9097065463  0.9951555329  0.9402298851  61.510297482  0.0360074189  4200          1.3301488558 
0.0250013450  1.0723820664  0.9963386728  0.9049586777  0.9839653856  0.8899082569  0.9174174174  0.9097065463  0.9954105048  0.9402298851  65.903890160  0.0351386329  4500          1.3303755554 
0.0235289149  1.0731294147  0.9963386728  0.9090909091  0.9849834564  0.8899082569  0.9169169169  0.9119638826  0.9959204488  0.9379310345  70.297482837  0.0287823602  4800          1.3299736412 
0.0260430691  1.0743823826  0.9963386728  0.9132231405  0.9849834564  0.8899082569  0.9156656657  0.9051918736  0.9964303927  0.9379310345  73.226544622  0.0325464612  5000          1.3299391294 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kl_loss       step          step_time    
0.9854694009  1.2435494661  0.8334096110  0.8471074380  0.6953423263  0.6972477064  0.8966466466  0.8916478555  0.8980112188  0.9172413793  0.0000000000  2.0008440018  0             2.1015369892 
0.3814656630  0.4059849670  0.9574370709  0.8801652893  0.8859760753  0.8165137615  0.9642142142  0.9503386005  0.9237633860  0.9333333333  4.3935926773  0.5040099994  300           1.3298193320 
0.1741378031  0.6782919999  0.9771167048  0.8884297521  0.9228811402  0.8555045872  0.9779779780  0.9571106095  0.9255481897  0.9356321839  8.7871853547  0.2470711702  600           1.3282346718 
0.1110210171  0.7984633893  0.9881006865  0.8884297521  0.9406973785  0.8761467890  0.9864864865  0.9571106095  0.9268230495  0.9333333333  13.180778032  0.1869164780  900           1.3297203851 
0.0820517938  0.9238133150  0.9903890160  0.8842975207  0.9529142275  0.8761467890  0.9899899900  0.9616252822  0.9275879653  0.9264367816  17.574370709  0.1497431897  1200          1.3277381206 
0.0659703016  0.9906618277  0.9922196796  0.8842975207  0.9608042759  0.8807339450  0.9909909910  0.9661399549  0.9263131056  0.9241379310  21.967963386  0.1224685912  1500          1.3292296195 
0.0540922116  1.0168176103  0.9935926773  0.8842975207  0.9674217358  0.8853211009  0.9924924925  0.9661399549  0.9245283019  0.9264367816  26.361556064  0.0984206319  1800          1.3279104384 
0.0464400689  1.0403945774  0.9935926773  0.8884297521  0.9717485365  0.8876146789  0.9932432432  0.9638826185  0.9250382458  0.9310344828  30.755148741  0.0818310899  2100          1.3293405636 
0.0392175883  1.0482381948  0.9940503432  0.8884297521  0.9748027488  0.8944954128  0.9939939940  0.9638826185  0.9235084141  0.9310344828  35.148741418  0.0709915172  2400          1.3296054824 
0.0380254684  1.0513296862  0.9940503432  0.8884297521  0.9760753372  0.8922018349  0.9942442442  0.9661399549  0.9222335543  0.9264367816  39.542334096  0.0614419849  2700          1.3291418568 
0.0319033996  1.0574841468  0.9949656751  0.8884297521  0.9776024434  0.8876146789  0.9944944945  0.9706546275  0.9209586945  0.9241379310  43.935926773  0.0485612378  3000          1.3289986523 
0.0311799596  1.0600347581  0.9954233410  0.8925619835  0.9786205141  0.8876146789  0.9954954955  0.9706546275  0.9212136665  0.9241379310  48.329519450  0.0474424630  3300          1.3291819747 
0.0301839399  1.0591864047  0.9954233410  0.8966942149  0.9798931026  0.8853211009  0.9959959960  0.9729119639  0.9217236104  0.9264367816  52.723112128  0.0442972842  3600          1.3294122521 
0.0256847515  1.0626758256  0.9958810069  0.8925619835  0.9809111733  0.8830275229  0.9962462462  0.9729119639  0.9222335543  0.9264367816  57.116704805  0.0385570510  3900          1.3282354188 
0.0252659276  1.0678573022  0.9958810069  0.8884297521  0.9824382795  0.8807339450  0.9964964965  0.9706546275  0.9229984702  0.9241379310  61.510297482  0.0397160362  4200          1.3292895627 
0.0262874039  1.0657452293  0.9963386728  0.8842975207  0.9834563502  0.8761467890  0.9964964965  0.9706546275  0.9219785824  0.9218390805  65.903890160  0.0390839417  4500          1.3303656832 
0.0243285293  1.0686397241  0.9963386728  0.8801652893  0.9839653856  0.8761467890  0.9969969970  0.9729119639  0.9217236104  0.9218390805  70.297482837  0.0382444048  4800          1.3294922916 
0.0282769470  1.0648672292  0.9963386728  0.8801652893  0.9852379740  0.8761467890  0.9977477477  0.9706546275  0.9194288628  0.9195402299  73.226544622  0.0401425090  5000          1.3300950885 
