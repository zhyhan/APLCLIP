./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	data_augmentation: True
	lr: 0.005
	mlp_depth: 3
	mlp_dropout: 0.1
	mlp_width: 512
	momentum: 0.1
	nonlinear_classifier: False
	num_domain_tokens: 16
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "cause the presence of"
Number of context words (tokens): 4
class_loss    disc_loss     dist_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
7.1554374695  1.0721360445  0.8762554526  0.1139575972  0.1060070671  0.4663529412  0.4538606403  0.3610053313  0.3399390244  0.2458348760  0.2474074074  0.0000000000  0             1.9544768333 
4.5167003123  0.6200413255  0.8669382308  0.9699646643  0.9540636042  0.7510588235  0.7495291902  0.7932216299  0.7545731707  0.8437615698  0.8325925926  8.4805653710  300           0.7320642185 
4.2499060448  0.4278472841  0.8411594677  0.9752650177  0.9575971731  0.7750588235  0.7796610169  0.8194973343  0.8094512195  0.8663457979  0.8429629630  16.961130742  600           0.7316924723 
4.0952277827  0.3977121422  0.8378845861  0.9443462898  0.9257950530  0.7741176471  0.7758945386  0.8084539223  0.7911585366  0.8607922991  0.8518518519  25.441696113  900           0.7315840634 
4.0644024579  0.3796705570  0.8346932497  0.9054770318  0.9010600707  0.7858823529  0.7984934087  0.8065498858  0.7743902439  0.8559792669  0.8400000000  33.922261484  1200          0.7316638144 
3.9992906944  0.3878579657  0.8288840954  0.9646643110  0.9717314488  0.7967058824  0.8060263653  0.8648134044  0.8216463415  0.8926323584  0.8622222222  42.402826855  1500          0.7323120824 
3.9886325415  0.4004104183  0.8319622662  0.9602473498  0.9717314488  0.8056470588  0.8060263653  0.8579588728  0.8231707317  0.8959644576  0.8637037037  50.883392226  1800          0.7320349932 
3.9808400679  0.4233966863  0.8251053492  0.9328621908  0.9505300353  0.8112941176  0.7947269303  0.8499619193  0.7942073171  0.8741206960  0.8414814815  59.363957597  2100          0.7327911607 
3.9334190981  0.4077826591  0.8238190130  0.7632508834  0.7632508834  0.7882352941  0.7777777778  0.8465346535  0.7728658537  0.8426508700  0.8355555556  67.844522968  2400          0.7326066875 
3.9334368388  0.4016650578  0.8298662718  0.9814487633  0.9752650177  0.8169411765  0.7966101695  0.8880426504  0.8262195122  0.9181784524  0.8755555556  76.325088339  2700          0.7325659593 
3.9332547855  0.3993547286  0.8287184701  0.9496466431  0.9681978799  0.8188235294  0.8041431262  0.8499619193  0.7865853659  0.8778230285  0.8518518519  84.805653710  3000          0.7329929105 
3.8633835657  0.4004937502  0.8263209655  0.9443462898  0.9434628975  0.8216470588  0.8060263653  0.8747143945  0.8094512195  0.9015179563  0.8607407407  93.286219081  3300          0.7326263897 
3.9151715358  0.4043431700  0.8203752663  0.9010600707  0.9293286219  0.8174117647  0.8003766478  0.8453922315  0.7835365854  0.8815253610  0.8459259259  101.76678445  3600          0.7316996145 
3.8757714295  0.3984551647  0.8205668809  0.9337455830  0.9505300353  0.8291764706  0.8116760829  0.8610053313  0.7865853659  0.8915216586  0.8577777778  110.24734982  3900          0.7329887613 
3.8408672818  0.3990962599  0.8126374612  0.8736749117  0.8621908127  0.8282352941  0.8041431262  0.8690022848  0.7896341463  0.8752313958  0.8400000000  118.72791519  4200          0.7324962385 
3.8814052669  0.4065751491  0.8131423020  0.9037102473  0.9257950530  0.8254117647  0.8060263653  0.8450114242  0.7789634146  0.8715290633  0.8429629630  127.20848056  4500          0.7324948351 
3.8408350396  0.3975458219  0.8112843722  0.8772084806  0.8727915194  0.8305882353  0.8097928437  0.8442498096  0.7621951220  0.8648648649  0.8429629630  135.68904593  4800          0.7315257144 
3.9076930463  0.4068912937  0.8139941132  0.8975265018  0.8869257951  0.8235294118  0.7871939736  0.8663366337  0.7774390244  0.8741206960  0.8355555556  141.34275618  5000          0.7319022024 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	data_augmentation: True
	lr: 0.005
	mlp_depth: 3
	mlp_dropout: 0.1
	mlp_width: 512
	momentum: 0.1
	nonlinear_classifier: False
	num_domain_tokens: 16
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "cause the presence of"
Number of context words (tokens): 4
class_loss    disc_loss     dist_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
6.9225578308  1.1187466383  0.8688564897  0.4531802120  0.4416961131  0.3750588235  0.3785310734  0.3617669459  0.3643292683  0.3557941503  0.3555555556  0.0000000000  0             2.0432980061 
4.5018921447  0.5853787998  0.8574679343  0.9823321555  0.9681978799  0.5750588235  0.5574387947  0.7810357959  0.7713414634  0.8337652721  0.8281481481  8.4805653710  300           0.7310638126 
3.8983279761  0.4079405350  0.8041700240  0.9991166078  1.0000000000  0.5058823529  0.4839924670  0.6386138614  0.6280487805  0.6042206590  0.5822222222  16.961130742  600           0.7314796082 
3.8059070460  0.3911209836  0.6787958942  0.9982332155  0.9964664311  0.6089411765  0.5969868173  0.8198781417  0.7926829268  0.8322843391  0.8222222222  25.441696113  900           0.7313515528 
3.7541513546  0.3978718119  0.6969505574  0.9991166078  1.0000000000  0.5680000000  0.5273069680  0.7665651181  0.7286585366  0.7600888560  0.7570370370  33.922261484  1200          0.7320265905 
3.6890694690  0.3946846478  0.6119883770  0.9991166078  1.0000000000  0.5181176471  0.4821092279  0.6580350343  0.6326219512  0.6305072195  0.6133333333  42.402826855  1500          0.7316796740 
3.7590766772  0.4130933304  0.6456337881  0.9991166078  1.0000000000  0.5774117647  0.5442561205  0.7787509520  0.7408536585  0.7760088856  0.7629629630  50.883392226  1800          0.7326105571 
3.6717185394  0.4060823740  0.6536458540  1.0000000000  1.0000000000  0.6202352941  0.5838041431  0.7638994669  0.7179878049  0.7678637542  0.7629629630  59.363957597  2100          0.7313381974 
3.6553483494  0.3912250032  0.6721000995  1.0000000000  0.9964664311  0.5971764706  0.5725047081  0.8248286367  0.7774390244  0.8348759719  0.8162962963  67.844522968  2400          0.7327288858 
3.6693094214  0.3891488350  0.6964048268  1.0000000000  1.0000000000  0.6141176471  0.5762711864  0.7852246763  0.7286585366  0.7882265827  0.7718518519  76.325088339  2700          0.7329097597 
3.6265793173  0.3806138760  0.6951202722  0.9991166078  1.0000000000  0.6512941176  0.6365348399  0.8084539223  0.7667682927  0.8356164384  0.8192592593  84.805653710  3000          0.7329626687 
3.6268325798  0.3823669555  0.6740956064  1.0000000000  1.0000000000  0.6352941176  0.6139359699  0.8229246002  0.7728658537  0.8441318030  0.8311111111  93.286219081  3300          0.7327938763 
3.6250289520  0.3779559825  0.6863252352  1.0000000000  0.9964664311  0.6428235294  0.6233521657  0.8747143945  0.8155487805  0.8822658275  0.8562962963  101.76678445  3600          0.7330878003 
3.5888354111  0.3768568979  0.6761584506  1.0000000000  1.0000000000  0.6381176471  0.6101694915  0.8663366337  0.8079268293  0.8881895594  0.8666666667  110.24734982  3900          0.7332050006 
3.6086457213  0.3860680826  0.6864981653  1.0000000000  0.9964664311  0.6338823529  0.6139359699  0.8697638995  0.7957317073  0.8867086264  0.8592592593  118.72791519  4200          0.7336698922 
3.5866970571  0.3909307495  0.6915147690  1.0000000000  1.0000000000  0.6272941176  0.5875706215  0.9028941356  0.8170731707  0.9044798223  0.8725925926  127.20848056  4500          0.7331907558 
3.5814250326  0.3870676918  0.6608733835  1.0000000000  1.0000000000  0.6480000000  0.6120527307  0.8815689261  0.8003048780  0.8893002592  0.8548148148  135.68904593  4800          0.7332543119 
3.5718575001  0.3769608043  0.6760816133  1.0000000000  1.0000000000  0.6465882353  0.6177024482  0.9013709063  0.8048780488  0.9163272862  0.8740740741  141.34275618  5000          0.7334085977 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	data_augmentation: True
	lr: 0.005
	mlp_depth: 3
	mlp_dropout: 0.1
	mlp_width: 512
	momentum: 0.1
	nonlinear_classifier: False
	num_domain_tokens: 16
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "cause the presence of"
Number of context words (tokens): 4
class_loss    disc_loss     dist_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
7.0013508797  1.1176103354  0.8724482656  0.5812720848  0.6077738516  0.4837647059  0.5084745763  0.3766184311  0.3871951220  0.4427989633  0.4429629630  0.0000000000  0             2.0494785309 
4.5514565309  0.4429950856  0.9005893838  0.9858657244  0.9787985866  0.7482352941  0.7514124294  0.7136329018  0.6753048780  0.8230285080  0.8074074074  8.4805653710  300           0.7316067076 
3.9590260355  0.2676735257  0.8310598983  0.9991166078  1.0000000000  0.5990588235  0.5706214689  0.6675552171  0.6280487805  0.7945205479  0.7688888889  16.961130742  600           0.7317698129 
3.8216183639  0.2591273132  0.8011737059  0.9991166078  1.0000000000  0.6400000000  0.6440677966  0.6919268850  0.6463414634  0.7978526472  0.7807407407  25.441696113  900           0.7315520716 
3.8054363728  0.3394329714  0.7929358896  0.9991166078  1.0000000000  0.7449411765  0.7250470810  0.7166793602  0.6768292683  0.8352462051  0.8148148148  33.922261484  1200          0.7318138194 
3.7566342346  0.3192040195  0.7688313013  0.9991166078  1.0000000000  0.7355294118  0.7024482109  0.7102056359  0.6753048780  0.8296927064  0.8014814815  42.402826855  1500          0.7318594686 
3.7776293914  0.3075717141  0.7804467797  0.9991166078  1.0000000000  0.7223529412  0.6873822976  0.7452399086  0.7164634146  0.8689374306  0.8370370370  50.883392226  1800          0.7317534200 
3.7169460193  0.3024673599  0.7911718825  1.0000000000  1.0000000000  0.6974117647  0.6723163842  0.7151561310  0.6692073171  0.8359866716  0.7822222222  59.363957597  2100          0.7323697631 
3.7229765852  0.3255997906  0.8011736387  1.0000000000  1.0000000000  0.6480000000  0.6064030132  0.7220106626  0.6920731707  0.7978526472  0.7644444444  67.844522968  2400          0.7314880339 
3.7172369591  0.3098490299  0.7784819790  1.0000000000  1.0000000000  0.6945882353  0.6403013183  0.7060167555  0.6814024390  0.8089596446  0.7570370370  76.325088339  2700          0.7324502357 
3.7169932620  0.3182979236  0.7734883664  1.0000000000  1.0000000000  0.7538823529  0.7024482109  0.7688499619  0.7332317073  0.8800444280  0.8281481481  84.805653710  3000          0.7329687858 
3.6753657977  0.3184970416  0.7455907253  1.0000000000  1.0000000000  0.6795294118  0.6195856874  0.7124904798  0.6768292683  0.7900777490  0.7659259259  93.286219081  3300          0.7312938062 
3.6765486733  0.3185033343  0.7251495816  1.0000000000  1.0000000000  0.6672941176  0.6158192090  0.7201066260  0.7027439024  0.7863754165  0.7377777778  101.76678445  3600          0.7325437125 
3.9071524398  0.3319686431  0.8265133423  0.9991166078  1.0000000000  0.8258823529  0.7890772128  0.7307692308  0.6859756098  0.9192891522  0.8696296296  110.24734982  3900          0.7320350099 
3.7070815794  0.3318110650  0.8378181537  0.9991166078  1.0000000000  0.7943529412  0.7645951036  0.7471439452  0.6981707317  0.9141058867  0.8607407407  118.72791519  4200          0.7312512136 
3.6703836346  0.3117698880  0.8160350649  0.9991166078  1.0000000000  0.8188235294  0.7664783427  0.7197258187  0.6829268293  0.9070714550  0.8429629630  127.20848056  4500          0.7327925420 
3.6630996052  0.3328825421  0.8069503585  0.9991166078  1.0000000000  0.6969411765  0.6553672316  0.7048743336  0.6722560976  0.8167345428  0.7525925926  135.68904593  4800          0.7322328671 
3.6508197963  0.3272654892  0.8023412940  0.9991166078  1.0000000000  0.8112941176  0.7419962335  0.6976389947  0.6661585366  0.8833765272  0.8148148148  141.34275618  5000          0.7316118646 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.005
	lr_d: 0.001
	lr_g: 0.001
	mlp_depth: 3
	mlp_dropout: 0.1
	mlp_width: 256
	momentum: 0.1
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "cause the presence of"
Number of context words (tokens): 4
class_loss    disc_loss     dist_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
7.0729961395  1.1023695469  0.8653194904  0.2031802120  0.2155477032  0.2616470588  0.2768361582  0.3610053313  0.3582317073  0.2972972973  0.3051851852  0.0000000000  0             2.1650159359 
4.6254110702  0.5598186466  0.8770799637  0.9973498233  0.9964664311  0.7520000000  0.7344632768  0.8122619954  0.7865853659  0.7863754165  0.8133333333  8.4805653710  300           0.7316679422 
4.0990963380  0.4528501551  0.8561606995  1.0000000000  1.0000000000  0.7548235294  0.7062146893  0.8309215537  0.8003048780  0.7549055905  0.7807407407  16.961130742  600           0.7327245728 
4.0250271718  0.4666086330  0.8487248413  1.0000000000  0.9964664311  0.8018823529  0.7834274953  0.8670982483  0.8338414634  0.8252499074  0.8177777778  25.441696113  900           0.7319587493 
4.0435535415  0.4321665976  0.8733056712  0.9982332155  1.0000000000  0.8178823529  0.7984934087  0.8632901752  0.8140243902  0.8382080711  0.8340740741  33.922261484  1200          0.7333631849 
3.9546713750  0.4379690220  0.8679898457  0.9991166078  1.0000000000  0.8174117647  0.8022598870  0.8785224676  0.8216463415  0.8193261755  0.8133333333  42.402826855  1500          0.7330933030 
3.9723894294  0.4517837253  0.8706971268  1.0000000000  1.0000000000  0.7661176471  0.7589453861  0.8552932216  0.7820121951  0.8067382451  0.8266666667  50.883392226  1800          0.7323524094 
4.0045917924  0.4442764507  0.8709234615  1.0000000000  1.0000000000  0.8320000000  0.7984934087  0.8941355674  0.8216463415  0.7878563495  0.8192592593  59.363957597  2100          0.7330606469 
3.8738741016  0.4460896389  0.8612609629  1.0000000000  1.0000000000  0.8465882353  0.8003766478  0.9055597867  0.8338414634  0.7889670492  0.7985185185  67.844522968  2400          0.7332826026 
3.9375750033  0.4390982915  0.8629854349  0.9991166078  1.0000000000  0.8315294118  0.7683615819  0.9002284844  0.8170731707  0.7782302851  0.7911111111  76.325088339  2700          0.7326602117 
3.7911615880  0.4458533341  0.8317179545  1.0000000000  1.0000000000  0.8409411765  0.7683615819  0.9143183549  0.8201219512  0.7656423547  0.7674074074  84.805653710  3000          0.7329520178 
3.8930330102  0.4727451954  0.8411923595  1.0000000000  1.0000000000  0.8512941176  0.7966101695  0.9272658035  0.8307926829  0.7697149204  0.7896296296  93.286219081  3300          0.7331964636 
4.0025618951  0.4769841424  0.8420516642  1.0000000000  1.0000000000  0.8687058824  0.7871939736  0.9265041889  0.8094512195  0.7652721214  0.7822222222  101.76678445  3600          0.7333472308 
3.8830722952  0.4861528913  0.8265877895  0.9991166078  1.0000000000  0.8588235294  0.7853107345  0.9306930693  0.8216463415  0.7663828212  0.7925925926  110.24734982  3900          0.7329797371 
3.8486741638  0.4569465943  0.8124782753  1.0000000000  1.0000000000  0.8644705882  0.7740112994  0.9143183549  0.8079268293  0.7467604591  0.7466666667  118.72791519  4200          0.7334806744 
3.7627547852  0.4510375356  0.7942232800  1.0000000000  1.0000000000  0.8560000000  0.7551789077  0.9116527037  0.8140243902  0.7134394669  0.7333333333  127.20848056  4500          0.7333068577 
3.8266188669  0.4688819627  0.7993376871  1.0000000000  1.0000000000  0.8343529412  0.7382297552  0.9295506474  0.8155487805  0.7341725287  0.7407407407  135.68904593  4800          0.7329324333 
3.8320582438  0.4679292126  0.8205653763  1.0000000000  1.0000000000  0.8607058824  0.7476459510  0.9352627570  0.8185975610  0.7375046279  0.7422222222  141.34275618  5000          0.7332572436 
