./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: MetricSoftmaxAlignPatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.2968514860  1.1664400101  0.9982332155  1.0000000000  0.6720075401  0.6622641509  0.7966488957  0.7667682927  0.8718992966  0.8874074074  0.0000000000  0             1.9878592491 
0.0671588910  0.3551088979  1.0000000000  1.0000000000  0.6804901037  0.6905660377  0.9645849200  0.8780487805  0.9788967049  0.9377777778  8.4805653710  300           0.7293476812 
0.0373692141  0.8599704427  1.0000000000  1.0000000000  0.6781338360  0.6867924528  0.9840060929  0.8887195122  0.9877823029  0.9377777778  16.961130742  600           0.7296480465 
0.0261284683  0.9963906223  1.0000000000  1.0000000000  0.6786050895  0.6867924528  0.9851485149  0.8841463415  0.9911144021  0.9377777778  25.441696113  900           0.7294402893 
0.0162574370  1.0130347290  1.0000000000  1.0000000000  0.6795475966  0.6849056604  0.9878141660  0.8826219512  0.9929655683  0.9362962963  33.922261484  1200          0.7308965953 
0.0125546683  1.0225170803  1.0000000000  0.9964664311  0.6823751178  0.6773584906  0.9897182026  0.8887195122  0.9944465013  0.9407407407  42.402826855  1500          0.7310100230 
0.0112497892  1.0211007979  0.9982332155  0.9964664311  0.6837888784  0.6735849057  0.9916222391  0.8841463415  0.9944465013  0.9422222222  50.883392226  1800          0.7304189881 
0.0115628577  1.0152025455  0.9982332155  0.9929328622  0.6828463713  0.6735849057  0.9927646611  0.8841463415  0.9929655683  0.9422222222  59.363957597  2100          0.7298023248 
0.0088592965  1.0129258484  0.9982332155  0.9929328622  0.6819038643  0.6716981132  0.9939070830  0.8841463415  0.9937060348  0.9392592593  67.844522968  2400          0.7297042314 
0.0096869757  1.0088167091  0.9982332155  0.9929328622  0.6809613572  0.6698113208  0.9946686976  0.8841463415  0.9944465013  0.9377777778  76.325088339  2700          0.7305807137 
0.0083690725  1.0100642864  0.9991166078  0.9929328622  0.6814326107  0.6698113208  0.9954303123  0.8841463415  0.9948167345  0.9318518519  84.805653710  3000          0.7301935426 
0.0110304249  1.0163702794  1.0000000000  0.9929328622  0.6819038643  0.6698113208  0.9965727342  0.8826219512  0.9959274343  0.9318518519  93.286219081  3300          0.7301215537 
0.0108564650  1.0217115664  1.0000000000  0.9929328622  0.6823751178  0.6716981132  0.9969535415  0.8810975610  0.9970381340  0.9333333333  101.76678445  3600          0.7302440206 
0.0121818786  1.0194793399  1.0000000000  0.9929328622  0.6837888784  0.6716981132  0.9969535415  0.8810975610  0.9962976675  0.9348148148  110.24734982  3900          0.7310332108 
0.0125292938  1.0220954289  1.0000000000  0.9893992933  0.6837888784  0.6754716981  0.9969535415  0.8780487805  0.9959274343  0.9333333333  118.72791519  4200          0.7304191462 
0.0121053737  1.0241323809  1.0000000000  0.9893992933  0.6823751178  0.6792452830  0.9973343488  0.8780487805  0.9959274343  0.9362962963  127.20848056  4500          0.7295413049 
0.0157784624  1.0315353775  1.0000000000  0.9929328622  0.6819038643  0.6773584906  0.9977151561  0.8750000000  0.9959274343  0.9318518519  135.68904593  4800          0.7295203336 
0.0190293807  1.0461155525  1.0000000000  0.9929328622  0.6809613572  0.6773584906  0.9977151561  0.8689024390  0.9970381340  0.9348148148  141.34275618  5000          0.7298451519 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: MetricSoftmaxAlignPatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4641954899  1.1196166277  0.9973498233  1.0000000000  0.6201696513  0.6056603774  0.5833968012  0.5655487805  0.8485746020  0.8607407407  0.0000000000  0             1.5402767658 
0.0876252842  0.3161443650  1.0000000000  1.0000000000  0.8953817154  0.7622641509  0.7696115765  0.7454268293  0.9603850426  0.9170370370  8.4805653710  300           0.8296558094 
0.0363492957  0.9084924837  0.9973498233  0.9964664311  0.9151743638  0.7584905660  0.7669459254  0.7439024390  0.9707515735  0.9125925926  16.961130742  600           0.7313654637 
0.0359987057  0.9722673678  0.9938162544  0.9929328622  0.9297832234  0.7641509434  0.7715156131  0.7439024390  0.9763050722  0.9111111111  25.441696113  900           0.7304032334 
0.0154440504  1.0291666464  0.9938162544  0.9929328622  0.9392082941  0.7584905660  0.7791317593  0.7439024390  0.9777860052  0.9037037037  33.922261484  1200          0.7298954566 
0.0158029972  1.0229299776  0.9911660777  0.9929328622  0.9467483506  0.7566037736  0.7787509520  0.7454268293  0.9803776379  0.8977777778  42.402826855  1500          0.7300244999 
0.0126409906  1.0316481437  0.9902826855  0.9929328622  0.9453345900  0.7452830189  0.7798933740  0.7484756098  0.9825990374  0.8962962963  50.883392226  1800          0.7345200594 
0.0125549134  1.0285049262  0.9893992933  0.9893992933  0.9458058435  0.7471698113  0.7871287129  0.7576219512  0.9829692706  0.8992592593  59.363957597  2100          0.7328181609 
0.0121062206  1.0374020090  0.9911660777  0.9929328622  0.9467483506  0.7433962264  0.7878903275  0.7576219512  0.9840799704  0.9022222222  67.844522968  2400          0.7323329123 
0.0120698525  1.0316738278  0.9902826855  0.9929328622  0.9472196041  0.7471698113  0.7936024372  0.7621951220  0.9833395039  0.9007407407  76.325088339  2700          0.7330016422 
0.0152938265  1.0537369895  0.9902826855  0.9929328622  0.9425070688  0.7528301887  0.7939832445  0.7682926829  0.9844502036  0.9051851852  84.805653710  3000          0.7319611049 
0.0161476218  1.0646756625  0.9902826855  0.9893992933  0.9420358153  0.7433962264  0.7993145468  0.7774390244  0.9833395039  0.9037037037  93.286219081  3300          0.7330022701 
0.0126814895  1.0770512499  0.9902826855  0.9893992933  0.9349670123  0.7471698113  0.7996953542  0.7820121951  0.9848204369  0.9022222222  101.76678445  3600          0.7308526421 
0.0124112422  1.0823923836  0.9885159011  0.9893992933  0.9326107446  0.7509433962  0.8019801980  0.7835365854  0.9848204369  0.9037037037  110.24734982  3900          0.7263006679 
0.0138975927  1.0863663252  0.9849823322  0.9893992933  0.9260131951  0.7566037736  0.8069306931  0.7865853659  0.9829692706  0.9022222222  118.72791519  4200          0.7242737683 
0.0088700182  1.0939841000  0.9840989399  0.9823321555  0.9236569274  0.7528301887  0.8076923077  0.7804878049  0.9822288041  0.9007407407  127.20848056  4500          0.7246279701 
0.0126244524  1.0941065425  0.9849823322  0.9823321555  0.9175306315  0.7584905660  0.8099771516  0.7820121951  0.9818585709  0.8962962963  135.68904593  4800          0.7251325949 
0.0075751233  1.1028957194  0.9867491166  0.9858657244  0.9099905749  0.7584905660  0.8103579589  0.7820121951  0.9818585709  0.8874074074  141.34275618  5000          0.7251549172 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: MetricSoftmaxAlignPatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4900920689  1.1172126532  0.9982332155  1.0000000000  0.6427898209  0.6245283019  0.6599390708  0.6463414634  0.8633839319  0.8725925926  0.0000000000  0             1.4833161831 
0.1003640031  0.1761105554  1.0000000000  1.0000000000  0.9066918002  0.7811320755  0.9531607007  0.8582317073  0.8759718623  0.9007407407  8.4805653710  300           0.7254782740 
0.0450140125  0.9203141130  1.0000000000  1.0000000000  0.9264844486  0.7660377358  0.9672505712  0.8597560976  0.8722695298  0.9051851852  16.961130742  600           0.7258739964 
0.0272025353  0.9768439098  1.0000000000  1.0000000000  0.9434495759  0.7735849057  0.9722010663  0.8521341463  0.8674564976  0.8918518519  25.441696113  900           0.7259183478 
0.0196943464  0.9808216683  0.9991166078  1.0000000000  0.9552309142  0.7716981132  0.9782939832  0.8460365854  0.8630136986  0.8829629630  33.922261484  1200          0.7254429221 
0.0183139856  0.9816607900  0.9991166078  1.0000000000  0.9528746466  0.7679245283  0.9817212490  0.8506097561  0.8600518327  0.8740740741  42.402826855  1500          0.7247480218 
0.0141072349  0.9854128536  0.9991166078  1.0000000000  0.9585296890  0.7716981132  0.9824828637  0.8490853659  0.8574601999  0.8829629630  50.883392226  1800          0.7251974281 
0.0144523698  0.9840346479  0.9991166078  1.0000000000  0.9547596607  0.7773584906  0.9866717441  0.8490853659  0.8563495002  0.8903703704  59.363957597  2100          0.7249041923 
0.0138910481  0.9799533433  0.9991166078  1.0000000000  0.9542884072  0.7698113208  0.9870525514  0.8597560976  0.8548685672  0.8874074074  67.844522968  2400          0.7251185258 
0.0131989088  0.9731590698  1.0000000000  1.0000000000  0.9547596607  0.7660377358  0.9870525514  0.8582317073  0.8552388004  0.8829629630  76.325088339  2700          0.7250141263 
0.0146972092  0.9846239163  1.0000000000  1.0000000000  0.9552309142  0.7622641509  0.9878141660  0.8567073171  0.8544983340  0.8770370370  84.805653710  3000          0.7254420241 
0.0176010662  1.0003260134  1.0000000000  1.0000000000  0.9566446748  0.7584905660  0.9881949733  0.8567073171  0.8522769345  0.8814814815  93.286219081  3300          0.7263920999 
0.0197279408  1.0080964245  1.0000000000  1.0000000000  0.9561734213  0.7622641509  0.9885757807  0.8567073171  0.8511662347  0.8814814815  101.76678445  3600          0.7259905370 
0.0173052747  1.0105634574  1.0000000000  1.0000000000  0.9561734213  0.7622641509  0.9897182026  0.8567073171  0.8482043688  0.8785185185  110.24734982  3900          0.7257905157 
0.0169529978  1.0314025327  1.0000000000  0.9964664311  0.9561734213  0.7584905660  0.9889565880  0.8567073171  0.8467234358  0.8785185185  118.72791519  4200          0.7259682584 
0.0225920548  1.0424318737  0.9991166078  0.9964664311  0.9533459001  0.7528301887  0.9885757807  0.8551829268  0.8430211033  0.8681481481  127.20848056  4500          0.7256968435 
0.0204024235  1.0526115664  0.9973498233  0.9964664311  0.9505183789  0.7566037736  0.9900990099  0.8460365854  0.8430211033  0.8696296296  135.68904593  4800          0.7251106985 
0.0210586388  1.0631426793  0.9973498233  0.9964664311  0.9311969840  0.7377358491  0.9900990099  0.8429878049  0.8445020363  0.8637037037  141.34275618  5000          0.7257278001 
