./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: MetricSoftmaxAlignPatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.5009089112  1.2359608412  0.8228630278  0.8268041237  0.6944444444  0.6941580756  0.8992117117  0.8906426156  0.8941480207  0.9081515499  0.0000000000  0             6.6729784012 
0.2177501446  0.5603256024  0.8640576725  0.8577319588  0.9272623139  0.8339060710  0.9831081081  0.9447576099  0.9790590935  0.9265212400  4.9433573635  300           0.7467130065 
0.0746049120  0.8767341705  0.8671472709  0.8701030928  0.9596219931  0.8522336770  0.9918355856  0.9515219842  0.9890992542  0.9242250287  9.8867147271  600           0.7277274028 
0.0600848124  0.9954785732  0.8743563337  0.8742268041  0.9725085911  0.8648339061  0.9943693694  0.9571589628  0.9931153184  0.9219288175  14.830072090  900           0.7275633605 
0.0407496287  1.0440488462  0.8702368692  0.8783505155  0.9785223368  0.8728522337  0.9957770270  0.9594137542  0.9948364888  0.9219288175  19.773429454  1200          0.7280464745 
0.0338407064  1.0668877482  0.8707518023  0.8742268041  0.9819587629  0.8751431844  0.9966216216  0.9582863585  0.9956970740  0.9242250287  24.716786817  1500          0.7280064917 
0.0282941956  1.0752467531  0.8728115345  0.8742268041  0.9833906071  0.8717067583  0.9966216216  0.9594137542  0.9962707975  0.9242250287  29.660144181  1800          0.7285140038 
0.0229259370  1.0787184364  0.8733264676  0.8680412371  0.9851088202  0.8705612829  0.9977477477  0.9616685457  0.9962707975  0.9253731343  34.603501544  2100          0.7286740510 
0.0170615871  1.0805112584  0.8748712667  0.8639175258  0.9856815578  0.8682703322  0.9977477477  0.9616685457  0.9965576592  0.9253731343  39.546858908  2400          0.7327579451 
0.0160294463  1.0808488778  0.8717816684  0.8639175258  0.9871134021  0.8671248568  0.9980292793  0.9627959414  0.9965576592  0.9288174512  44.490216271  2700          0.7277832317 
0.0154094884  1.0836165078  0.8686920700  0.8639175258  0.9876861397  0.8636884307  0.9980292793  0.9627959414  0.9968445209  0.9276693456  49.433573635  3000          0.7283274245 
0.0119098065  1.0833013757  0.8697219361  0.8639175258  0.9879725086  0.8671248568  0.9980292793  0.9639233371  0.9968445209  0.9288174512  54.376930999  3300          0.7285347756 
0.0121412838  1.0865075918  0.8666323378  0.8659793814  0.9885452463  0.8682703322  0.9980292793  0.9639233371  0.9968445209  0.9276693456  59.320288362  3600          0.7292156482 
0.0124224667  1.0845854620  0.8686920700  0.8701030928  0.9885452463  0.8625429553  0.9980292793  0.9650507328  0.9971313827  0.9265212400  64.263645726  3900          0.7289646975 
0.0118852751  1.0881441339  0.8686920700  0.8762886598  0.9885452463  0.8636884307  0.9977477477  0.9639233371  0.9971313827  0.9265212400  69.207003089  4200          0.7287960426 
0.0104894198  1.0865398022  0.8676622039  0.8742268041  0.9885452463  0.8625429553  0.9977477477  0.9650507328  0.9971313827  0.9253731343  74.150360453  4500          0.7650126115 
0.0117311651  1.0861693001  0.8686920700  0.8721649485  0.9888316151  0.8613974800  0.9977477477  0.9650507328  0.9971313827  0.9242250287  79.093717816  4800          0.7286156408 
0.0094259044  1.0869920576  0.8661174047  0.8680412371  0.9888316151  0.8625429553  0.9977477477  0.9616685457  0.9971313827  0.9207807118  82.389289392  5000          0.7292229795 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: MetricSoftmaxAlignPatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4533676803  1.2207415104  0.8285272915  0.8350515464  0.6784077892  0.6689576174  0.8932995495  0.8816234498  0.8950086059  0.9001148106  0.0000000000  0             1.7625889778 
0.1320880796  0.6628873962  0.9876416066  0.8948453608  0.7348224513  0.7308132875  0.9864864865  0.9537767756  0.9813539874  0.9311136625  4.9433573635  300           0.7288514678 
0.0460325568  0.8625213152  0.9953656025  0.9010309278  0.7436998855  0.7331042383  0.9935247748  0.9571589628  0.9899598394  0.9345579793  9.8867147271  600           0.7286175434 
0.0363998304  1.0093207109  0.9963954686  0.9092783505  0.7471363116  0.7353951890  0.9949324324  0.9605411499  0.9936890419  0.9276693456  14.830072090  900           0.7284126504 
0.0281133466  1.0497369272  0.9963954686  0.9072164948  0.7471363116  0.7468499427  0.9952139640  0.9616685457  0.9948364888  0.9288174512  19.773429454  1200          0.7286641971 
0.0215927780  1.0705790269  0.9963954686  0.9092783505  0.7494272623  0.7422680412  0.9963400901  0.9639233371  0.9956970740  0.9265212400  24.716786817  1500          0.7288529118 
0.0169886811  1.0735285189  0.9963954686  0.9092783505  0.7488545246  0.7353951890  0.9966216216  0.9650507328  0.9962707975  0.9242250287  29.660144181  1800          0.7291520905 
0.0171265486  1.0788927060  0.9963954686  0.9072164948  0.7517182131  0.7342497136  0.9969031532  0.9639233371  0.9968445209  0.9242250287  34.603501544  2100          0.7281090450 
0.0136258613  1.0762622837  0.9963954686  0.9051546392  0.7540091638  0.7342497136  0.9971846847  0.9661781285  0.9971313827  0.9242250287  39.546858908  2400          0.7283225989 
0.0108941967  1.0782472360  0.9963954686  0.9030927835  0.7528636884  0.7376861397  0.9974662162  0.9661781285  0.9971313827  0.9253731343  44.490216271  2700          0.7280677843 
0.0119852440  1.0790457280  0.9963954686  0.9030927835  0.7534364261  0.7376861397  0.9977477477  0.9650507328  0.9974182444  0.9265212400  49.433573635  3000          0.7282854072 
0.0098867835  1.0781278513  0.9963954686  0.9030927835  0.7548682703  0.7342497136  0.9977477477  0.9650507328  0.9974182444  0.9265212400  54.376930999  3300          0.7279952288 
0.0097309118  1.0807754008  0.9963954686  0.9010309278  0.7540091638  0.7342497136  0.9977477477  0.9661781285  0.9974182444  0.9276693456  59.320288362  3600          0.7280179874 
0.0084758631  1.0808291384  0.9963954686  0.8989690722  0.7525773196  0.7353951890  0.9977477477  0.9650507328  0.9974182444  0.9288174512  64.263645726  3900          0.7282071257 
0.0084443842  1.0837327051  0.9963954686  0.8969072165  0.7525773196  0.7331042383  0.9977477477  0.9650507328  0.9974182444  0.9276693456  69.207003089  4200          0.7529815833 
0.0082097158  1.0820049767  0.9963954686  0.8948453608  0.7528636884  0.7331042383  0.9977477477  0.9661781285  0.9974182444  0.9311136625  74.150360453  4500          0.7283086332 
0.0088132048  1.0823101922  0.9963954686  0.8948453608  0.7514318442  0.7331042383  0.9977477477  0.9673055242  0.9974182444  0.9288174512  79.093717816  4800          0.7278409044 
0.0098032462  1.0844151825  0.9963954686  0.8907216495  0.7491408935  0.7331042383  0.9980292793  0.9650507328  0.9974182444  0.9299655568  82.389289392  5000          0.7278274584 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: MetricSoftmaxAlignPatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.8386952877  1.2378232479  0.8259526262  0.8391752577  0.7030355097  0.6998854525  0.8975225225  0.8816234498  0.8973034997  0.8978185993  0.0000000000  0             2.3335814476 
0.2266198268  0.5316361593  0.9835221421  0.8845360825  0.9252577320  0.8281786942  0.9228603604  0.9210822999  0.9781985083  0.9265212400  4.9433573635  300           0.7283147669 
0.0757915408  0.8804675041  0.9927909372  0.8907216495  0.9587628866  0.8487972509  0.9245495495  0.9255918828  0.9896729776  0.9299655568  9.8867147271  600           0.7284923800 
0.0537792530  0.9907716999  0.9953656025  0.8948453608  0.9727949599  0.8533791523  0.9242680180  0.9233370913  0.9916810098  0.9276693456  14.830072090  900           0.7281163184 
0.0403350142  1.0396362964  0.9958805355  0.8927835052  0.9776632302  0.8648339061  0.9222972973  0.9210822999  0.9939759036  0.9299655568  19.773429454  1200          0.7286786572 
0.0322302929  1.0661469972  0.9958805355  0.8948453608  0.9802405498  0.8659793814  0.9208896396  0.9210822999  0.9948364888  0.9253731343  24.716786817  1500          0.7283177288 
0.0262509975  1.0716810552  0.9958805355  0.8989690722  0.9813860252  0.8659793814  0.9192004505  0.9188275085  0.9954102123  0.9242250287  29.660144181  1800          0.7281141432 
0.0220604922  1.0727473933  0.9963954686  0.8969072165  0.9828178694  0.8659793814  0.9186373874  0.9199549042  0.9962707975  0.9253731343  34.603501544  2100          0.7280382172 
0.0232567568  1.0770337383  0.9963954686  0.8927835052  0.9836769759  0.8636884307  0.9177927928  0.9222096956  0.9965576592  0.9230769231  39.546858908  2400          0.7290430880 
0.0179818960  1.0776935500  0.9963954686  0.8948453608  0.9848224513  0.8648339061  0.9146959459  0.9210822999  0.9968445209  0.9242250287  44.490216271  2700          0.7297967498 
0.0154378799  1.0790845247  0.9963954686  0.8969072165  0.9848224513  0.8694158076  0.9141328829  0.9199549042  0.9971313827  0.9230769231  49.433573635  3000          0.7285110323 
0.0156052564  1.0797244471  0.9963954686  0.8969072165  0.9851088202  0.8671248568  0.9121621622  0.9188275085  0.9971313827  0.9207807118  54.376930999  3300          0.7283708270 
0.0133155830  1.0810435573  0.9963954686  0.8969072165  0.9853951890  0.8682703322  0.9115990991  0.9199549042  0.9971313827  0.9184845006  59.320288362  3600          0.7277664383 
0.0134048284  1.0832035196  0.9963954686  0.8969072165  0.9865406644  0.8648339061  0.9115990991  0.9199549042  0.9971313827  0.9173363949  64.263645726  3900          0.7270778060 
0.0130868285  1.0834238052  0.9963954686  0.8969072165  0.9871134021  0.8636884307  0.9115990991  0.9199549042  0.9971313827  0.9184845006  69.207003089  4200          0.7292620476 
0.0119085359  1.0837363811  0.9963954686  0.8927835052  0.9873997709  0.8625429553  0.9113175676  0.9154453213  0.9971313827  0.9161882893  74.150360453  4500          0.7309432236 
0.0132463078  1.0827099371  0.9963954686  0.8948453608  0.9876861397  0.8591065292  0.9104729730  0.9154453213  0.9971313827  0.9138920781  79.093717816  4800          0.7335149844 
0.0121932071  1.0878401220  0.9963954686  0.8948453608  0.9882588774  0.8568155785  0.9085022523  0.9098083427  0.9974182444  0.9138920781  82.389289392  5000          0.7337667167 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: MetricSoftmaxAlignPatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.9578172565  1.2477504015  0.8228630278  0.8371134021  0.6998854525  0.6964490263  0.9054054054  0.8906426156  0.8961560528  0.9035591274  0.0000000000  0             2.0863134861 
0.2251329998  0.4258309322  0.9799176107  0.8804123711  0.9238258877  0.8316151203  0.9831081081  0.9481397971  0.9265633964  0.9288174512  4.9433573635  300           0.7342027752 
0.0774537636  0.8079562396  0.9917610711  0.8886597938  0.9544673540  0.8522336770  0.9901463964  0.9571589628  0.9254159495  0.9276693456  9.8867147271  600           0.7329041481 
0.0519835937  0.9720357881  0.9948506694  0.8969072165  0.9684994273  0.8602520046  0.9940878378  0.9571589628  0.9225473322  0.9299655568  14.830072090  900           0.7332826455 
0.0463741932  1.0271946126  0.9953656025  0.8969072165  0.9759450172  0.8671248568  0.9954954955  0.9616685457  0.9202524383  0.9299655568  19.773429454  1200          0.7325895174 
0.0382784762  1.0528605396  0.9958805355  0.8927835052  0.9785223368  0.8694158076  0.9963400901  0.9594137542  0.9191049914  0.9253731343  24.716786817  1500          0.7338559977 
0.0273439217  1.0682585371  0.9958805355  0.8989690722  0.9813860252  0.8728522337  0.9963400901  0.9582863585  0.9182444062  0.9242250287  29.660144181  1800          0.7338765748 
0.0238387224  1.0697242139  0.9958805355  0.9010309278  0.9828178694  0.8717067583  0.9966216216  0.9571589628  0.9165232358  0.9219288175  34.603501544  2100          0.7335779230 
0.0213921264  1.0705307662  0.9958805355  0.9010309278  0.9842497136  0.8728522337  0.9971846847  0.9560315671  0.9179575445  0.9207807118  39.546858908  2400          0.7333730976 
0.0201708988  1.0742594536  0.9958805355  0.9010309278  0.9851088202  0.8705612829  0.9974662162  0.9560315671  0.9168100975  0.9184845006  44.490216271  2700          0.7325662025 
0.0160324854  1.0738667808  0.9958805355  0.9030927835  0.9868270332  0.8682703322  0.9977477477  0.9594137542  0.9165232358  0.9161882893  49.433573635  3000          0.7341126506 
0.0164644258  1.0724464717  0.9958805355  0.9010309278  0.9873997709  0.8694158076  0.9977477477  0.9594137542  0.9165232358  0.9150401837  54.376930999  3300          0.7332271830 
0.0159795730  1.0763509729  0.9963954686  0.9010309278  0.9873997709  0.8705612829  0.9980292793  0.9582863585  0.9145152037  0.9138920781  59.320288362  3600          0.7332343165 
0.0141169184  1.0771644457  0.9963954686  0.9051546392  0.9876861397  0.8705612829  0.9977477477  0.9594137542  0.9142283419  0.9138920781  64.263645726  3900          0.7340252391 
0.0145149044  1.0771848961  0.9963954686  0.9051546392  0.9879725086  0.8705612829  0.9977477477  0.9549041714  0.9130808950  0.9115958668  69.207003089  4200          0.7330778297 
0.0141881793  1.0784146682  0.9963954686  0.9051546392  0.9882588774  0.8682703322  0.9977477477  0.9549041714  0.9122203098  0.9104477612  74.150360453  4500          0.7322153250 
0.0153592277  1.0821401608  0.9963954686  0.9030927835  0.9885452463  0.8659793814  0.9977477477  0.9560315671  0.9110728629  0.9138920781  79.093717816  4800          0.7332756782 
0.0133268997  1.0791631544  0.9963954686  0.9010309278  0.9885452463  0.8613974800  0.9980292793  0.9560315671  0.9104991394  0.9138920781  82.389289392  5000          0.7325826180 
