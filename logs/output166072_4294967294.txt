./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: MetricSoftmaxAlign
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.001
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.2
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
class_loss    env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         lr            step          step_time    
0.7782537937  0.9991166078  1.0000000000  0.7096470588  0.7212806026  0.7433358720  0.7347560976  0.8570899667  0.8770370370  0.0000000000  0.0000100000  0             5.4528946877 
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         lr            step          step_time    
0.4112211126  0.2469905180  0.9964664311  1.0000000000  0.8202352941  0.8173258004  0.8815689261  0.8612804878  0.9104035542  0.9096296296  8.4805653710  0.0000099924  300           1.6420376174 
0.3240699691  0.2171612228  0.9973498233  1.0000000000  0.8409411765  0.8173258004  0.9105102818  0.8673780488  0.9392817475  0.9288888889  16.961130742  0.0000099477  600           1.6438403964 
0.2771195177  0.3115442503  0.9964664311  0.9964664311  0.8494117647  0.8248587571  0.9181264280  0.8612804878  0.9566827101  0.9244444444  25.441696113  0.0000098591  900           1.6425601808 
0.2599238918  0.3946292103  0.9946996466  0.9929328622  0.8432941176  0.8229755179  0.9120335110  0.8643292683  0.9577934098  0.9288888889  33.922261484  0.0000097274  1200          1.6426117484 
0.3148023865  0.7468525159  0.9920494700  0.9893992933  0.8320000000  0.8342749529  0.8846153846  0.8246951220  0.9189189189  0.9081481481  42.402826855  0.0000095538  1500          1.5948652959 
0.3183994154  0.9554615589  0.9938162544  0.9823321555  0.8367058824  0.8399246704  0.9135567403  0.8582317073  0.9422436135  0.9333333333  50.883392226  0.0000093399  1800          1.6419550252 
0.2690906308  0.9907446086  0.9929328622  0.9823321555  0.8550588235  0.8418079096  0.9272658035  0.8704268293  0.9503887449  0.9303703704  59.363957597  0.0000090874  2100          1.6426218271 
0.2417627956  0.9955362447  0.9946996466  0.9858657244  0.8658823529  0.8418079096  0.9345011424  0.8582317073  0.9607552758  0.9362962963  67.844522968  0.0000087988  2400          1.6429569546 
0.2188097325  0.9983184393  0.9920494700  0.9858657244  0.8776470588  0.8229755179  0.9451637471  0.8734756098  0.9700111070  0.9274074074  76.325088339  0.0000084764  2700          1.6234660482 
0.2002506169  0.9808766723  0.9946996466  0.9858657244  0.8790588235  0.8173258004  0.9539223153  0.8795731707  0.9737134395  0.9303703704  84.805653710  0.0000081233  3000          1.6426389257 
0.1813727618  0.9719361246  0.9938162544  0.9858657244  0.9025882353  0.8229755179  0.9626808835  0.8734756098  0.9796371714  0.9200000000  93.286219081  0.0000077424  3300          1.6420262305 
0.1515419145  0.9722795089  0.9920494700  0.9858657244  0.9049411765  0.8060263653  0.9687738005  0.8765243902  0.9818585709  0.9244444444  101.76678445  0.0000073373  3600          1.6276910369 
0.1386314323  0.9741457542  0.9938162544  0.9858657244  0.9223529412  0.8116760829  0.9638233054  0.8689024390  0.9892632358  0.9318518519  110.24734982  0.0000069114  3900          1.6437793581 
0.1287840844  0.9708186162  0.9929328622  0.9858657244  0.9232941176  0.8041431262  0.9809596344  0.8689024390  0.9911144021  0.9303703704  118.72791519  0.0000064687  4200          1.6973692226 
0.1098000857  0.9697206394  0.9938162544  0.9858657244  0.9341176471  0.8041431262  0.9817212490  0.8673780488  0.9937060348  0.9200000000  127.20848056  0.0000060129  4500          1.6334632810 
0.0999389343  0.9598934305  0.9920494700  0.9858657244  0.9491764706  0.8210922787  0.9851485149  0.8795731707  0.9937060348  0.9259259259  135.68904593  0.0000055482  4800          1.6414440846 
0.1033011873  0.9563516659  0.9920494700  0.9858657244  0.9501176471  0.8116760829  0.9885757807  0.8704268293  0.9940762680  0.9214814815  141.34275618  0.0000051572  5000          1.6421521437 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: MetricSoftmax
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.2
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
Using clip_transform ViT-B/16
ViT-B/16
Using ViT-B/16...
class_loss    env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         lr            step          step_time    
0.4587939680  0.8280123584  0.8371134021  0.6849942726  0.6746849943  0.8952702703  0.8872604284  0.8964429145  0.9092996556  0.0000000000  0.0000100000  0             5.0427246094 
0.2416845344  0.9562306900  0.8865979381  0.7070446735  0.7056128293  0.9648085586  0.9379932356  0.9566838784  0.9322617681  4.9433573635  0.0000099700  300           1.6408944726 
0.0889072008  0.9917610711  0.8865979381  0.7087628866  0.7205040092  0.9864864865  0.9503945885  0.9836488812  0.9391504018  9.8867147271  0.0000097933  600           1.6733227905 
0.0355029371  0.9958805355  0.8907216495  0.7139175258  0.7193585338  0.9940878378  0.9526493799  0.9936890419  0.9380022962  14.830072090  0.0000094469  900           1.6447641389 
0.0204602636  0.9963954686  0.8948453608  0.7142038946  0.7170675830  0.9966216216  0.9549041714  0.9956970740  0.9357060850  19.773429454  0.0000089431  1200          1.6441572547 
0.0139985628  0.9963954686  0.9010309278  0.7133447881  0.7205040092  0.9977477477  0.9571589628  0.9968445209  0.9345579793  24.716786817  0.0000082998  1500          1.5317328223 
0.0102351187  0.9963954686  0.8989690722  0.7153493700  0.7227949599  0.9980292793  0.9571589628  0.9974182444  0.9368541906  29.660144181  0.0000075399  1800          1.6421446149 
0.0102061238  0.9963954686  0.9051546392  0.7222222222  0.7216494845  0.9980292793  0.9549041714  0.9974182444  0.9402985075  34.603501544  0.0000066901  2100          1.6434133879 
0.0071859254  0.9963954686  0.8969072165  0.7164948454  0.7182130584  0.9980292793  0.9582863585  0.9974182444  0.9357060850  39.546858908  0.0000057806  2400          1.5570515331 
0.0073719835  0.9963954686  0.8927835052  0.7173539519  0.7205040092  0.9977477477  0.9582863585  0.9974182444  0.9334098737  44.490216271  0.0000048436  2700          0.7266164764 
0.0066771931  0.9963954686  0.9051546392  0.7187857961  0.7227949599  0.9983108108  0.9582863585  0.9971313827  0.9311136625  49.433573635  0.0000039124  3000          0.7323071305 
0.0072912145  0.9963954686  0.8948453608  0.7170675830  0.7227949599  0.9980292793  0.9582863585  0.9974182444  0.9368541906  54.376930999  0.0000030199  3300          0.7013506023 
0.0056254309  0.9963954686  0.9030927835  0.7184994273  0.7216494845  0.9983108108  0.9582863585  0.9974182444  0.9380022962  59.320288362  0.0000021977  3600          0.7014413722 
0.0061076249  0.9963954686  0.8989690722  0.7167812142  0.7216494845  0.9980292793  0.9605411499  0.9974182444  0.9334098737  64.263645726  0.0000014749  3900          0.7010008176 
0.0057649162  0.9963954686  0.9010309278  0.7187857961  0.7193585338  0.9983108108  0.9605411499  0.9974182444  0.9345579793  69.207003089  0.0000008772  4200          0.7009721343 
0.0055436305  0.9963954686  0.9010309278  0.7187857961  0.7182130584  0.9983108108  0.9582863585  0.9974182444  0.9334098737  74.150360453  0.0000004257  4500          0.7011932627 
0.0051037392  0.9963954686  0.9010309278  0.7176403207  0.7193585338  0.9983108108  0.9605411499  0.9974182444  0.9345579793  79.093717816  0.0000001365  4800          0.7009683895 
0.0054884429  0.9963954686  0.9010309278  0.7176403207  0.7193585338  0.9983108108  0.9605411499  0.9974182444  0.9345579793  82.389289392  0.0000000228  5000          0.7014690340 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: MetricSoftmaxAlign
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.001
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.2
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         lr            step          step_time    
0.8387029767  1.1017845869  0.8295571576  0.8371134021  0.6878579611  0.6746849943  0.8955518018  0.8861330327  0.8967297762  0.9070034443  0.0000000000  0.0000100000  0             1.5410513878 
0.5578542980  0.3313341676  0.9037075180  0.8597938144  0.7797823597  0.7560137457  0.9265202703  0.9233370913  0.9311531842  0.9253731343  4.9433573635  0.0000099924  300           0.7038972998 
0.3893846450  0.2501046268  0.9330587024  0.8762886598  0.8141466208  0.7823596793  0.9265202703  0.9199549042  0.9475043029  0.9334098737  9.8867147271  0.0000099477  600           0.7032523108 
0.3187314796  0.3506497928  0.9521112255  0.8804123711  0.8436426117  0.7835051546  0.9234234234  0.9165727170  0.9563970166  0.9242250287  14.830072090  0.0000098591  900           0.7043668842 
0.2970277339  0.4685239270  0.9675592173  0.8783505155  0.8645475372  0.7995418099  0.9208896396  0.9210822999  0.9584050488  0.9345579793  19.773429454  0.0000097274  1200          0.7033573405 
0.2677213099  0.4558090350  0.9701338826  0.8824742268  0.8765750286  0.7983963345  0.9208896396  0.9177001127  0.9627079748  0.9276693456  24.716786817  0.0000095538  1500          0.7042666960 
0.2555498229  0.4968828489  0.9706488157  0.8762886598  0.8857388316  0.8052691867  0.9200450450  0.9154453213  0.9635685600  0.9230769231  29.660144181  0.0000093399  1800          0.7035326052 
0.2580702406  0.5393535092  0.9711637487  0.8804123711  0.8911798396  0.8167239404  0.9175112613  0.9143179256  0.9615605278  0.9265212400  34.603501544  0.0000090874  2100          0.7037643107 
0.2520116033  0.5873063358  0.9757981462  0.8783505155  0.8969072165  0.8041237113  0.9158220721  0.9120631342  0.9647160069  0.9311136625  39.546858908  0.0000087988  2400          0.7039460405 
0.2600844833  0.6505960268  0.9752832132  0.8762886598  0.9012027491  0.8098510882  0.9189189189  0.9131905299  0.9661503155  0.9288174512  44.490216271  0.0000084764  2700          0.7048786076 
0.2457625579  0.7496913702  0.9757981462  0.8742268041  0.8994845361  0.8029782360  0.9135698198  0.9131905299  0.9667240390  0.9253731343  49.433573635  0.0000081233  3000          0.7041115737 
0.2485785773  0.8390843034  0.9773429454  0.8783505155  0.9097938144  0.8144329897  0.9177927928  0.9154453213  0.9687320711  0.9253731343  54.376930999  0.0000077424  3300          0.7036959068 
0.2092943535  0.7967980862  0.9799176107  0.8721649485  0.9163802978  0.8098510882  0.9163851351  0.9143179256  0.9721744119  0.9242250287  59.320288362  0.0000073373  3600          0.7038390827 
0.1847464153  0.7567699774  0.9824922760  0.8865979381  0.9269759450  0.8235967927  0.9146959459  0.9109357384  0.9736087206  0.9276693456  64.263645726  0.0000069114  3900          0.7036235023 
0.1607230859  0.7026222825  0.9855818744  0.8659793814  0.9295532646  0.8224513173  0.9107545045  0.9098083427  0.9753298910  0.9219288175  69.207003089  0.0000064687  4200          0.7035750087 
0.1602818824  0.6640037615  0.9871266735  0.8721649485  0.9372852234  0.8258877434  0.9149774775  0.9086809470  0.9773379231  0.9242250287  74.150360453  0.0000060129  4500          0.7038448580 
0.1469371965  0.6508431705  0.9855818744  0.8721649485  0.9367124857  0.8190148912  0.9186373874  0.9064261556  0.9784853701  0.9253731343  79.093717816  0.0000055482  4800          0.7046809324 
0.1457637880  0.6477344051  0.9860968074  0.8721649485  0.9375715922  0.8121420389  0.9146959459  0.9075535513  0.9776247849  0.9276693456  82.389289392  0.0000051572  5000          0.7036067605 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: MetricSoftmaxAlign
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.001
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.2
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         lr            step          step_time    
0.9624992013  1.0936866999  0.8290422245  0.8371134021  0.6881443299  0.6758304696  0.8966779279  0.8872604284  0.8961560528  0.9081515499  0.0000000000  0.0000100000  0             1.4528176785 
0.5489419048  0.1490407401  0.9042224511  0.8515463918  0.7906643757  0.7514318442  0.9490427928  0.9289740699  0.9162363741  0.9184845006  4.9433573635  0.0000099924  300           0.7032345573 
0.3821847984  0.1395240183  0.9376930999  0.8721649485  0.8258877434  0.7823596793  0.9546734234  0.9346110485  0.9176706827  0.9219288175  9.8867147271  0.0000099477  600           0.7027235317 
0.3260866322  0.3162462031  0.9582904222  0.8742268041  0.8462199313  0.7949599084  0.9625563063  0.9425028185  0.9176706827  0.9276693456  14.830072090  0.0000098591  900           0.7038637066 
0.2732463281  0.3343990032  0.9680741504  0.8804123711  0.8659793814  0.8041237113  0.9636824324  0.9425028185  0.9148020654  0.9276693456  19.773429454  0.0000097274  1200          0.7039807677 
0.2580992693  0.3233798117  0.9716786818  0.8783505155  0.8771477663  0.8041237113  0.9693130631  0.9391206313  0.9139414802  0.9322617681  24.716786817  0.0000095538  1500          0.7041601038 
0.2562891563  0.3628390606  0.9778578785  0.8804123711  0.8843069874  0.7995418099  0.9704391892  0.9368658399  0.9096385542  0.9219288175  29.660144181  0.0000093399  1800          0.7042836579 
0.2495522564  0.4398808743  0.9747682801  0.8618556701  0.8900343643  0.8098510882  0.9684684685  0.9436302142  0.9090648308  0.9253731343  34.603501544  0.0000090874  2100          0.7038580346 
0.2482771959  0.4835051073  0.9768280124  0.8680412371  0.8969072165  0.8052691867  0.9687500000  0.9368658399  0.9067699369  0.9265212400  39.546858908  0.0000087988  2400          0.7042225130 
0.2589329617  0.6676157828  0.9706488157  0.8577319588  0.8951890034  0.8075601375  0.9701576577  0.9357384442  0.9018932874  0.9184845006  44.490216271  0.0000084764  2700          0.7047141631 
0.2655447260  0.7968026408  0.9737384140  0.8824742268  0.8969072165  0.8087056128  0.9695945946  0.9391206313  0.9056224900  0.9207807118  49.433573635  0.0000081233  3000          0.7044908937 
0.2286366014  0.6875543170  0.9783728115  0.8783505155  0.9040664376  0.8155784651  0.9721283784  0.9391206313  0.9113597246  0.9253731343  54.376930999  0.0000077424  3300          0.7036764892 
0.2166263333  0.6435008509  0.9814624099  0.8804123711  0.9112256586  0.8155784651  0.9721283784  0.9413754228  0.9084911073  0.9276693456  59.320288362  0.0000073373  3600          0.7050083979 
0.1873388713  0.6635942603  0.9819773429  0.8742268041  0.9152348225  0.8224513173  0.9749436937  0.9391206313  0.9053356282  0.9207807118  64.263645726  0.0000069114  3900          0.7036574117 
0.1823615496  0.6550347822  0.9860968074  0.8804123711  0.9218213058  0.8178694158  0.9780405405  0.9391206313  0.9041881813  0.9219288175  69.207003089  0.0000064687  4200          0.7044878157 
0.1851075675  0.6369973396  0.9850669413  0.8804123711  0.9261168385  0.8247422680  0.9774774775  0.9391206313  0.9039013196  0.9207807118  74.150360453  0.0000060129  4500          0.7040147916 
0.1847286714  0.6199432524  0.9866117405  0.8783505155  0.9243986254  0.8201603666  0.9771959459  0.9357384442  0.9033275961  0.9253731343  79.093717816  0.0000055482  4800          0.7051924372 
0.1835974876  0.5954519899  0.9881565396  0.8762886598  0.9258304696  0.8155784651  0.9760698198  0.9368658399  0.9007458405  0.9230769231  82.389289392  0.0000051572  5000          0.7050292528 
