Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.9044365883  0.6863026619  1.0946360826  0.8306636156  0.8553719008  0.6927971494  0.6995412844  0.9019019019  0.8871331828  0.9005609383  0.9218390805  0.0000000000  0             4.4436271191 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.7060184479  0.8685792089  1.1158350706  0.9968602826  1.0000000000  0.7494763301  0.7396226415  0.7376438727  0.7225609756  0.8736426456  0.9139465875  0.0000000000  0             17.617830991 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
1.0114219189  0.6940098405  1.1316448450  0.8260869565  0.8429752066  0.7019597862  0.7110091743  0.8996496496  0.8961625282  0.9013258542  0.9195402299  0.0000000000  0             2.1565990448 
0.2740796270  0.3203325789  0.4756528683  0.8681922197  0.8553719008  0.9053194197  0.8371559633  0.9757257257  0.9480812641  0.9671086181  0.9379310345  4.3935926773  300           1.3723836088 
0.1561688469  0.2552210104  0.3010174031  0.9937205651  1.0000000000  0.9032258065  0.8037735849  0.9438050102  0.8993902439  0.9657782165  0.9465875371  7.5353218210  300           1.3468964513 
0.1589870847  0.2545480262  0.3252872428  0.9905808477  0.9929078014  0.8990364474  0.8113207547  0.9438050102  0.8993902439  0.9723593287  0.9495548961  7.5353218210  300           1.3484057307 
0.2722819908  0.3226581013  0.4716235960  0.8727688787  0.8553719008  0.9043013489  0.8119266055  0.9732232232  0.9480812641  0.9683834778  0.9264367816  4.3935926773  300           1.3490955194 
0.1336189591  0.1444628633  0.7491915427  0.8759725400  0.8553719008  0.9412064138  0.8486238532  0.9847347347  0.9571106095  0.9816420194  0.9333333333  8.7871853547  600           1.3510885461 
0.0806721223  0.0787144925  0.5071500417  0.9866562009  0.9929078014  0.9505655635  0.8150943396  0.9698713609  0.8993902439  0.9845343863  0.9495548961  15.070643642  600           1.3496486910 
0.1354133639  0.1426646273  0.7347552363  0.8773455378  0.8471074380  0.9404428608  0.8463302752  0.9832332332  0.9548532731  0.9834268230  0.9310344828  8.7871853547  600           1.3493049463 
0.0626341630  0.0422190243  0.8087281834  0.9850863422  0.9787234043  0.9715123586  0.8150943396  0.9800270819  0.8993902439  0.9914445541  0.9465875371  22.605965463  900           1.3486874930 
0.0959249055  0.0966546751  0.8585888992  0.8778032037  0.8553719008  0.9567319929  0.8600917431  0.9887387387  0.9661399549  0.9857215706  0.9379310345  13.180778032  900           1.3515961607 
0.0723195499  0.0479801988  0.8241279552  0.9843014129  0.9858156028  0.9706744868  0.8113207547  0.9813811781  0.8993902439  0.9924317210  0.9495548961  22.605965463  900           1.3499852554 
0.0945914936  0.0856525530  0.8562089926  0.8796338673  0.8471074380  0.9541868160  0.8577981651  0.9897397397  0.9616252822  0.9880163182  0.9287356322  13.180778032  900           1.3510627548 
0.0510334753  0.0255282914  0.9231505901  0.9835164835  0.9787234043  0.9849183075  0.8188679245  0.9878131347  0.8841463415  0.9957222771  0.9436201780  30.141287284  1200          1.3501707530 
0.0807080248  0.0740322758  0.9628333362  0.8800915332  0.8512396694  0.9648765589  0.8623853211  0.9917417417  0.9661399549  0.9887812341  0.9333333333  17.574370709  1200          1.3527893106 
0.0512633916  0.0268236463  0.9237210741  0.9819466248  0.9787234043  0.9832425639  0.8075471698  0.9888287068  0.8902439024  0.9950641658  0.9436201780  30.141287284  1200          1.3499571689 
0.0753840137  0.0701297729  0.9694811074  0.8810068650  0.8471074380  0.9643675235  0.8692660550  0.9912412412  0.9661399549  0.9928607853  0.9333333333  17.574370709  1200          1.3512684925 
0.0428586677  0.0197251028  0.9588180325  0.9795918367  0.9787234043  0.9903644742  0.8264150943  0.9922139472  0.8750000000  0.9973675551  0.9495548961  37.676609105  1500          1.3517785390 
0.0405822776  0.0224821849  0.9670324335  0.9811616954  0.9716312057  0.9907834101  0.7962264151  0.9939065674  0.8841463415  0.9970384995  0.9436201780  37.676609105  1500          1.3549866144 
0.0652990704  0.0578937552  1.0139053553  0.8814645309  0.8553719008  0.9702214304  0.8715596330  0.9937437437  0.9683972912  0.9910759816  0.9333333333  21.967963386  1500          1.3527152967 
0.0584455152  0.0612065029  1.0137717597  0.8819221968  0.8512396694  0.9697123950  0.8807339450  0.9932432432  0.9593679458  0.9941356451  0.9310344828  21.967963386  1500          1.3520708021 
0.0343128259  0.0150106960  0.9802260405  0.9772370487  0.9787234043  0.9937159615  0.8264150943  0.9952606635  0.8719512195  0.9983547219  0.9436201780  45.211930926  1800          1.3505087757 
0.0326527130  0.0149012219  0.9727029838  0.9795918367  0.9716312057  0.9945538333  0.7849056604  0.9976303318  0.8871951220  0.9980256663  0.9465875371  45.211930926  1800          1.3510448758 
0.0504036200  0.0510163670  1.0448174514  0.8823798627  0.8553719008  0.9737846780  0.8784403670  0.9944944945  0.9616252822  0.9943906170  0.9264367816  26.361556064  1800          1.3517299040 
0.0509670221  0.0499050864  1.0458800912  0.8791762014  0.8553719008  0.9742937134  0.8830275229  0.9952452452  0.9683972912  0.9933707292  0.9333333333  26.361556064  1800          1.3524247940 
0.0318455010  0.0117430314  0.9917499059  0.9764521193  0.9716312057  0.9966485128  0.8113207547  0.9969532837  0.8689024390  0.9993418888  0.9436201780  52.747252747  2100          1.3497423402 
0.0281211560  0.0115905467  0.9803773046  0.9764521193  0.9716312057  0.9966485128  0.7849056604  0.9979688558  0.8871951220  0.9983547219  0.9465875371  52.747252747  2100          1.3512808800 
0.0432646690  0.0391392277  1.0630193077  0.8842105263  0.8595041322  0.9765843726  0.8830275229  0.9952452452  0.9638826185  0.9954105048  0.9264367816  30.755148741  2100          1.3510774239 
0.0437083918  0.0440741814  1.0567582911  0.8810068650  0.8553719008  0.9765843726  0.8830275229  0.9957457457  0.9616252822  0.9943906170  0.9333333333  30.755148741  2100          1.3520359842 
0.0228236896  0.0120798970  0.9897964509  0.9748822606  0.9574468085  0.9987431923  0.8113207547  0.9986459039  0.8750000000  0.9993418888  0.9465875371  60.282574568  2400          1.3484928830 
0.0226195479  0.0096036537  0.9925606066  0.9748822606  0.9716312057  0.9979053205  0.7886792453  0.9986459039  0.8871951220  0.9983547219  0.9436201780  60.282574568  2400          1.3531976199 
0.0383852907  0.0377621403  1.0661630408  0.8832951945  0.8595041322  0.9793840672  0.8830275229  0.9964964965  0.9638826185  0.9954105048  0.9264367816  35.148741418  2400          1.3528359675 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
                                                                                                                                                                                                                                                                                                                                                                                                        bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.7211979628  0.5738081932  1.0862641335  0.9968602826  1.0000000000  0.7352325094  0.7207547170  0.7647257955  0.7530487805  0.8808818690  0.9050445104  0.0000000000  0             26.243830442 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.8999824524  0.6726636887  1.0936815739  0.8347826087  0.8471074380  0.6991600916  0.6949541284  0.8991491491  0.8893905192  0.8982661907  0.9172413793  0.0000000000  0             30.421331644 
0.0177417373  0.0089730864  0.9924395901  0.9740973312  0.9716312057  0.9979053205  0.7849056604  0.9989844279  0.8871951220  0.9990128332  0.9406528190  67.817896389  2700          1.3520422308 
0.0292793630  0.0335150160  1.0723223493  0.8846681922  0.8636363636  0.9811656910  0.8830275229  0.9967467467  0.9638826185  0.9959204488  0.9264367816  39.542334096  2700          1.3522772225 
0.0172613599  0.0056216629  0.9996061995  0.9740973312  0.9574468085  1.0000000000  0.8075471698  0.9993229519  0.8780487805  0.9996709444  0.9436201780  75.353218210  3000          1.3512344678 
0.0319934631  0.0326358810  1.0680757658  0.8759725400  0.8595041322  0.9811656910  0.8761467890  0.9967467467  0.9638826185  0.9951555329  0.9333333333  39.542334096  2700          1.3540147312 
/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.5048906803  0.5428468585  1.1411796808  0.9976452119  1.0000000000  0.7021365731  0.6528301887  0.8002708192  0.7804878049  0.8828562027  0.8961424332  0.0000000000  0             2.0921854973 
0.1582515920  0.2552903008  0.2946352192  0.9952904239  1.0000000000  0.9036447424  0.8075471698  0.9410968179  0.8841463415  0.9621586048  0.9554896142  7.5353218210  300           1.3578153316 
0.0165717103  0.0065350534  0.9965811308  0.9717425432  0.9645390071  0.9983242564  0.7735849057  0.9993229519  0.8871951220  0.9990128332  0.9436201780  75.353218210  3000          1.3535584847 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0239632741  0.0344762642  1.0752093891  0.8819221968  0.8719008264  0.9826927971  0.8830275229  0.9967467467  0.9638826185  0.9961754207  0.9287356322  43.935926773  3000          1.3531200767 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4781680703  0.4337741435  1.1462684870  0.9976452119  1.0000000000  0.6887306242  0.6264150943  0.7877454299  0.7530487805  0.8690358671  0.8813056380  0.0000000000  0             6.2764899731 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.7868086696  0.5462315679  1.1950656176  0.8274599542  0.8264462810  0.6854161364  0.6972477064  0.8928928929  0.8826185102  0.8977562468  0.9172413793  0.0000000000  0             2.1770455837 
0.0261992972  0.0275332943  1.0720095627  0.8755148741  0.8595041322  0.9824382795  0.8738532110  0.9967467467  0.9638826185  0.9954105048  0.9333333333  43.935926773  3000          1.3536028934 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0819607089  0.1196493291  0.2473072649  1.0000000000  1.0000000000  0.6841223293  0.6452830189  0.9580230196  0.9115853659  0.9730174399  0.9554896142  7.5353218210  300           1.3481565738 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.7110723257  0.3772952259  1.0848067999  0.8338672769  0.8471074380  0.6897429371  0.7018348624  0.8986486486  0.8871331828  0.9026007139  0.9172413793  0.0000000000  0             2.1698417664 
0.1330408705  0.1428035709  0.7384706203  0.8759725400  0.8471074380  0.9366250954  0.8509174312  0.9857357357  0.9571106095  0.9826619072  0.9333333333  8.7871853547  600           1.3557560968 
0.0790870015  0.1171414138  0.2659972431  1.0000000000  1.0000000000  0.6987850859  0.6679245283  0.9566689235  0.9176829268  0.9726883843  0.9495548961  7.5353218210  300           1.3461812735 
0.2153213574  0.2086320232  0.5788722381  0.9675057208  0.8925619835  0.7538813948  0.7293577982  0.9717217217  0.9525959368  0.9704232534  0.9402298851  4.3935926773  300           1.3514696709 
0.0442744325  0.0426746057  0.5004257808  1.0000000000  1.0000000000  0.6807708421  0.6490566038  0.9807041300  0.9176829268  0.9881539980  0.9554896142  15.070643642  600           1.3471001863 
0.2156705959  0.2072994081  0.5745545802  0.9693363844  0.8719008264  0.7472639348  0.7477064220  0.9729729730  0.9525959368  0.9650688424  0.9402298851  4.3935926773  300           1.3471837902 
0.0997383807  0.0809043532  0.7360705096  0.9881006865  0.9008264463  0.7556630186  0.7431192661  0.9839839840  0.9616252822  0.9824069352  0.9425287356  8.7871853547  600           1.3513780355 
0.0397944549  0.0296631353  0.9215306401  1.0000000000  1.0000000000  0.6807708421  0.6490566038  0.9888287068  0.9115853659  0.9950641658  0.9554896142  22.605965463  900           1.3482134795 
0.1047079011  0.0813733043  0.7417201862  0.9876430206  0.8842975207  0.7447187580  0.7408256881  0.9864864865  0.9571106095  0.9793472718  0.9448275862  8.7871853547  600           1.3498575815 
0.0766213976  0.0528013693  0.8610116243  0.9922196796  0.8966942149  0.7559175363  0.7362385321  0.9889889890  0.9661399549  0.9877613463  0.9448275862  13.180778032  900           1.3512453334 
0.0322970183  0.0233833237  0.9751906836  1.0000000000  1.0000000000  0.6811897780  0.6452830189  0.9952606635  0.9054878049  0.9967094439  0.9495548961  30.141287284  1200          1.3487640611 
0.0766305136  0.0710070508  0.9583767738  0.8832951945  0.8471074380  0.9641130059  0.8669724771  0.9917417417  0.9616252822  0.9910759816  0.9379310345  17.574370709  1200          1.3567563057 
0.0693254684  0.0560011391  0.8795848072  0.9917620137  0.8842975207  0.7442097226  0.7362385321  0.9912412412  0.9638826185  0.9864864865  0.9425287356  13.180778032  900           1.3483882856 
0.0392443819  0.0176733095  0.9922031822  0.9788069074  0.9787234043  0.9928780897  0.8150943396  0.9918754232  0.8780487805  0.9967094439  0.9465875371  37.676609105  1500          1.3559147120 
0.0563298872  0.0406380454  0.9766306961  0.9945080092  0.0000000000  0.6904063678  0.6490566038  0.9935680433  0.9146341463  0.9963803883  0.9465875371  30.141287284  1200          1.3486957661 
                                                                                                                                                                                                                                                                                                                                                                                                        0.0633120430  0.0438349277  0.9625630214  0.9926773455  0.8884297521  0.7467548995  0.7316513761  0.9939939940  0.9683972912  0.9892911780  0.9425287356  17.574370709  1200          1.3507221095 
0.0323557794  0.0146923656  0.9824197497  0.9795918367  0.9716312057  0.9945538333  0.8037735849  0.9942450914  0.8841463415  0.9980256663  0.9525222552  45.211930926  1800          1.3561877537 
0.0233206079  0.0156486084  0.9906149308  1.0000000000  1.0000000000  0.6904063678  0.6528301887  0.9955991875  0.9115853659  0.9983547219  0.9406528190  37.676609105  1500          1.3492594020 
0.0461142226  0.0333889348  1.0237983904  0.9958810069  0.8925619835  0.7579536778  0.7568807339  0.9949949950  0.9683972912  0.9931157573  0.9379310345  21.967963386  1500          1.3522374598 
0.0555810390  0.0372431211  1.0059281166  0.9949656751  0.8884297521  0.7487910410  0.7293577982  0.9949949950  0.9683972912  0.9920958695  0.9448275862  21.967963386  1500          1.3512289635 
0.0538202503  0.0469164898  1.0362265833  0.8842105263  0.8471074380  0.9753117842  0.8784403670  0.9949949950  0.9661399549  0.9936257012  0.9379310345  26.361556064  1800          1.3573116231 
0.0219132253  0.0117876888  1.0081530348  1.0000000000  1.0000000000  0.6887306242  0.6528301887  0.9966147596  0.9115853659  0.9983547219  0.9376854599  45.211930926  1800          1.3500112732 
0.0272954751  0.0128286764  0.9984699808  0.9764521193  0.9716312057  0.9966485128  0.7962264151  0.9976303318  0.8750000000  0.9990128332  0.9554896142  52.747252747  2100          1.3564694452 
0.0430185186  0.0313557870  1.0412478767  0.9963386728  0.8966942149  0.7589717485  0.7522935780  0.9957457457  0.9706546275  0.9943906170  0.9356321839  26.361556064  1800          1.3527214471 
0.0385421608  0.0269788983  1.0420438347  0.9949656751  0.8842975207  0.7500636294  0.7316513761  0.9959959960  0.9729119639  0.9938806731  0.9448275862  26.361556064  1800          1.3523659547 
0.0132450002  0.0080345624  1.0059032526  1.0000000000  0.9929078014  0.6841223293  0.6415094340  0.9989844279  0.8993902439  0.9993418888  0.9584569733  60.282574568  2400          1.3486766879 
0.0193826505  0.0091283572  1.0040290248  1.0000000000  0.9929078014  0.6878927524  0.6528301887  0.9986459039  0.9115853659  0.9986837776  0.9317507418  52.747252747  2100          1.3500194724 
0.0287789708  0.0222102733  1.0581901413  0.9963386728  0.8966942149  0.7607533724  0.7477064220  0.9967467467  0.9706546275  0.9951555329  0.9310344828  30.755148741  2100          1.3511699716 
0.0127975551  0.0072047906  1.0092649601  1.0000000000  0.9858156028  0.6824465857  0.6377358491  0.9989844279  0.8993902439  0.9996709444  0.9584569733  67.817896389  2700          1.3463833300 
0.0285203438  0.0220587748  1.0541629694  0.9954233410  0.8801652893  0.7523542886  0.7339449541  0.9959959960  0.9706546275  0.9951555329  0.9448275862  30.755148741  2100          1.3540965199 
0.0148761922  0.0064361365  0.9995590037  1.0000000000  0.9929078014  0.6883116883  0.6490566038  0.9996614760  0.9176829268  0.9993418888  0.9406528190  60.282574568  2400          1.3504014436 
0.0174147524  0.0044675426  0.9884936559  0.9725274725  0.9645390071  0.9995810641  0.8000000000  0.9989844279  0.8810975610  0.9996709444  0.9614243323  67.817896389  2700          1.3584425418 
0.0360346353  0.0388160475  1.0678639567  0.8796338673  0.8553719008  0.9796385849  0.8853211009  0.9962462462  0.9638826185  0.9949005609  0.9356321839  35.148741418  2400          1.3589919440 
0.0114660715  0.0040848305  1.0096520573  1.0000000000  0.9858156028  0.6816087139  0.6377358491  0.9993229519  0.9024390244  0.9993418888  0.9584569733  75.353218210  3000          1.3467205405 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4348478317  0.5399425626  1.1546360254  0.9982332155  1.0000000000  0.7398680490  0.7169811321  0.6675552171  0.6463414634  0.8615327656  0.8740740741  0.0000000000  0             2.0651409626 
                                                                                                                                                                                                    0.0214841448  0.0161217698  1.0693135512  0.9963386728  0.8966942149  0.7602443370  0.7408256881  0.9972472472  0.9683972912  0.9954105048  0.9310344828  39.542334096  2700          1.3527990254 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                0.1060941850  0.1702713128  0.1432684353  1.0000000000  1.0000000000  0.9142318567  0.8339622642  0.8145468393  0.7820121951  0.9626064421  0.9348148148  8.4805653710  300           1.3472840039 
                                                                                                                                                                                                    0.0131490177  0.0148766891  1.0689226379  0.9963386728  0.9008264463  0.7610078900  0.7431192661  0.9972472472  0.9729119639  0.9959204488  0.9333333333  43.935926773  3000          1.3534420975 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0114337828  0.0048524800  0.9999497286  1.0000000000  0.9858156028  0.6874738165  0.6415094340  1.0000000000  0.9115853659  1.0000000000  0.9406528190  75.353218210  3000          1.3504801957 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
1.2367700338  0.9436728954  1.1779502630  0.8424304840  0.8412371134  0.7073310424  0.7021764032  0.9023085586  0.8917700113  0.8984509466  0.9150401837  0.0000000000  0             3.2142715454 
e: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0541869893  0.0412298122  0.4724776727  1.0000000000  1.0000000000  0.9604147031  0.8320754717  0.8172124905  0.7881097561  0.9855609034  0.9407407407  16.961130742  600           1.3464444089 
0.4130013883  0.6769765019  1.1703894138  0.9982332155  1.0000000000  0.7214891612  0.6830188679  0.6230007616  0.6204268293  0.8541281007  0.8637037037  0.0000000000  0             8.7668859959 
0.0748055715  0.1056042448  0.2368497117  0.9991166078  1.0000000000  0.6852026390  0.6962264151  0.9623000762  0.8902439024  0.9755646057  0.9451851852  8.4805653710  300           1.3537961475 
0.0250550903  0.0331886382  1.0747517016  0.8796338673  0.8595041322  0.9839653856  0.8807339450  0.9974974975  0.9616252822  0.9956654768  0.9333333333  43.935926773  3000          1.3580927666 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0157499999  0.0158798289  1.0675179805  0.9963386728  0.8801652893  0.7551539832  0.7293577982  0.9969969970  0.9774266366  0.9961754207  0.9379310345  43.935926773  3000          1.3600467221 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.2484148618  0.3069558570  0.4268559000  0.9763130793  0.8886597938  0.9072164948  0.8373424971  0.9301801802  0.9278466742  0.9704532415  0.9299655568  4.9433573635  300           1.3525056044 
0.7419699430  0.4673905373  1.1697407961  0.8295571576  0.8329896907  0.6852806415  0.6827033219  0.8969594595  0.8838782413  0.8958691910  0.9024110218  0.0000000000  0             9.5605146885 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.8768722415  0.6238525510  1.2031840086  0.8300720906  0.8288659794  0.6984536082  0.6987399771  0.8932995495  0.8827508455  0.8961560528  0.9047072331  0.0000000000  0             3.1213960648 
0.1049524200  0.1621672227  0.1465202451  1.0000000000  1.0000000000  0.9222431668  0.8245283019  0.8168316832  0.7835365854  0.9629766753  0.9244444444  8.4805653710  300           1.3492944709 
0.0472606627  0.0374988671  0.5651934691  0.9991166078  1.0000000000  0.6809613572  0.6943396226  0.9859101295  0.8856707317  0.9914846353  0.9481481481  16.961130742  600           1.3542488392 
0.1163622886  0.1141556407  0.7280697124  0.9886714727  0.9072164948  0.9455899198  0.8499427262  0.9287725225  0.9267192785  0.9850831899  0.9276693456  9.8867147271  600           1.3494148072 
0.2016011812  0.1912319199  0.6019243931  0.9763130793  0.8948453608  0.7408361970  0.7388316151  0.9783220721  0.9515219842  0.9704532415  0.9276693456  4.9433573635  300           1.3555482697 
0.0403756420  0.0203023834  1.0008923660  1.0000000000  1.0000000000  0.9924599434  0.8283018868  0.8145468393  0.7926829268  0.9948167345  0.9362962963  33.922261484  1200          1.3464233239 
0.0571633000  0.0492401124  0.4537424263  1.0000000000  1.0000000000  0.9655984920  0.8245283019  0.8175932978  0.7881097561  0.9874120696  0.9333333333  16.961130742  600           1.3480288911 
0.0418532100  0.0305208657  0.9174693859  0.9982332155  1.0000000000  0.6786050895  0.6943396226  0.9935262757  0.8917682927  0.9970381340  0.9481481481  25.441696113  900           1.3566360593 
0.0788301929  0.0796666239  0.8614861403  0.9927909372  0.9092783505  0.9579037801  0.8545246277  0.9270833333  0.9222096956  0.9905335628  0.9311136625  14.830072090  900           1.3503868477 
0.0273927201  0.0168852076  1.0115903252  1.0000000000  1.0000000000  0.9957587182  0.8320754717  0.8141660320  0.7957317073  0.9985190670  0.9392592593  42.402826855  1500          1.3466975069 
0.0963567891  0.0762260065  0.7282984423  0.9876416066  0.9010309278  0.7500000000  0.7468499427  0.9895833333  0.9571589628  0.9859437751  0.9311136625  9.8867147271  600           1.3576561666 
0.1067522937  0.1091795573  0.7288862047  0.9902162719  0.8969072165  0.9470217640  0.8510882016  0.9237049550  0.9233370913  0.9873780838  0.9322617681  9.8867147271  600           1.3514716864 
0.0485591340  0.0364510114  0.9416716268  1.0000000000  1.0000000000  0.9806786051  0.8207547170  0.8191165270  0.7911585366  0.9944465013  0.9303703704  25.441696113  900           1.3489293702 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                0.0784132971  0.0777741205  0.8785826365  0.9927909372  0.9072164948  0.9599083620  0.8591065292  0.9217342342  0.9222096956  0.9902467011  0.9299655568  14.830072090  900           1.3542944455 
0.0360543216  0.0230059443  1.0033609535  1.0000000000  1.0000000000  0.9919886899  0.8150943396  0.8172124905  0.7926829268  0.9966679008  0.9288888889  33.922261484  1200          1.3509721883 
0.0243285906  0.0127179937  1.0030383235  1.0000000000  1.0000000000  0.6757775683  0.6867924528  0.9980959634  0.8917682927  0.9988893003  0.9466666667  42.402826855  1500          1.3589069883 
0.0526337715  0.0476011455  1.0125484125  0.9953656025  0.9134020619  0.9779495991  0.8671248568  0.9242680180  0.9143179256  0.9939759036  0.9253731343  24.716786817  1500          1.3500661230 
0.0168843235  0.0068323377  1.0232683295  1.0000000000  1.0000000000  0.9981149859  0.8320754717  0.8137852247  0.8033536585  0.9988893003  0.9407407407  59.363957597  2100          1.3437363203 
0.0547080214  0.0369279045  0.9770227551  0.9953656025  0.9072164948  0.7580183276  0.7388316151  0.9938063063  0.9673055242  0.9928284567  0.9322617681  19.773429454  1200          1.3629271928 
0.0649803056  0.0512975977  0.9747803650  0.9943357364  0.9134020619  0.9707903780  0.8728522337  0.9192004505  0.9188275085  0.9931153184  0.9322617681  19.773429454  1200          1.3520033662 
0.0279752745  0.0160635105  1.0205552357  1.0000000000  1.0000000000  0.9957587182  0.8188679245  0.8191165270  0.7987804878  0.9977786005  0.9288888889  42.402826855  1500          1.3516398350 
0.0360468351  0.0414890304  1.0460118339  0.9958805355  0.9134020619  0.9813860252  0.8671248568  0.9225788288  0.9131905299  0.9942627653  0.9265212400  29.660144181  1800          1.3505452959 
0.0164485212  0.0064349099  1.0185523111  1.0000000000  1.0000000000  0.9990574929  0.8188679245  0.8118811881  0.8079268293  1.0000000000  0.9377777778  67.844522968  2400          1.3431390071 
0.0517573001  0.0363719835  1.0259794931  0.9963954686  0.9010309278  0.7585910653  0.7399770905  0.9952139640  0.9673055242  0.9951233505  0.9345579793  24.716786817  1500          1.3582047145 
0.0490460021  0.0515790620  1.0167686399  0.9953656025  0.9154639175  0.9793814433  0.8739977090  0.9177927928  0.9222096956  0.9936890419  0.9288174512  24.716786817  1500          1.3530067770 
0.0355640270  0.0332509626  1.0506630148  0.9963954686  0.9092783505  0.9836769759  0.8636884307  0.9222972973  0.9165727170  0.9945496271  0.9242250287  34.603501544  2100          1.3493828686 
0.0130256669  0.0049247187  1.0207318205  1.0000000000  1.0000000000  0.9995287465  0.8169811321  0.8099771516  0.8048780488  1.0000000000  0.9377777778  76.325088339  2700          1.3408753204 
0.0129734900  0.0066536223  1.0138876524  1.0000000000  1.0000000000  0.6781338360  0.6849056604  1.0000000000  0.8917682927  0.9996297668  0.9496296296  59.363957597  2100          1.3540130814 
0.0307139862  0.0219109300  1.0511558447  0.9963954686  0.9030927835  0.7591638030  0.7434135166  0.9954954955  0.9673055242  0.9954102123  0.9334098737  29.660144181  1800          1.3599597661 
0.0099286052  0.0031402888  1.0199446493  1.0000000000  1.0000000000  1.0000000000  0.8226415094  0.8103579589  0.8064024390  1.0000000000  0.9392592593  84.805653710  3000          1.3418730768 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0289720214  0.0325037779  1.0627448809  0.9963954686  0.9113402062  0.9851088202  0.8625429553  0.9214527027  0.9154453213  0.9956970740  0.9253731343  39.546858908  2400          1.3501230613 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4974076748  0.5268250108  1.1354995966  0.9973498233  1.0000000000  0.7450518379  0.7226415094  0.6728865194  0.6493902439  0.8585708997  0.8711111111  0.0000000000  0             2.1906116009 
                                                                                                                                                                                                    0.0207585709  0.0259305221  1.0665705055  0.9963954686  0.9113402062  0.9853951890  0.8671248568  0.9186373874  0.9154453213  0.9962707975  0.9242250287  44.490216271  2700          1.3503529708 
0.1171362060  0.1800928096  0.1253323640  1.0000000000  1.0000000000  0.9175306315  0.8320754717  0.9543031226  0.8734756098  0.8807848945  0.9066666667  8.4805653710  300           1.3453054714 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            0.0583062672  0.0465066285  0.4255897752  1.0000000000  1.0000000000  0.9613572102  0.8301886792  0.9821020564  0.8658536585  0.8767123288  0.9007407407  16.961130742  600           1.3456796328 
0.0242295945  0.0222938493  1.0642607504  0.9963954686  0.9134020619  0.9853951890  0.8671248568  0.9172297297  0.9143179256  0.9965576592  0.9253731343  49.433573635  3000          1.3503601098 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0282998720  0.0200222875  1.0634958498  0.9963954686  0.9051546392  0.7594501718  0.7399770905  0.9963400901  0.9706877114  0.9962707975  0.9368541906  39.546858908  2400          1.3585865458 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
1.2928061485  0.9999722838  1.2068601847  0.8383110196  0.8371134021  0.7084765178  0.7033218786  0.9045608108  0.8906426156  0.8975903614  0.9150401837  0.0000000000  0             2.6952216625 
0.0120015928  0.0048018225  1.0210276067  1.0000000000  1.0000000000  0.9995287465  0.8037735849  0.8153084539  0.8003048780  1.0000000000  0.9274074074  76.325088339  2700          1.3548108753 
0.0290650946  0.0316569336  1.0586017128  0.9958805355  0.9195876289  0.9842497136  0.8659793814  0.9183558559  0.9120631342  0.9956970740  0.9265212400  39.546858908  2400          1.3560275022 
0.0114896325  0.0040577011  1.0080368382  1.0000000000  0.9964664311  0.6771913289  0.6830188679  1.0000000000  0.8978658537  1.0000000000  0.9466666667  84.805653710  3000          1.3562994345 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0534089107  0.0354797418  0.8816466305  1.0000000000  1.0000000000  0.9816211122  0.8415094340  0.9904798172  0.8689024390  0.8722695298  0.8903703704  25.441696113  900           1.3427299047 
0.4782474935  0.5293558240  1.0899044275  0.9973498233  1.0000000000  0.7398680490  0.7207547170  0.6458492003  0.6280487805  0.8570899667  0.8651851852  0.0000000000  0             9.7068252563 
0.2744125199  0.3134098959  0.3174342259  0.9685890834  0.8865979381  0.9072164948  0.8224513173  0.9732545045  0.9379932356  0.9245553643  0.9253731343  4.9433573635  300           1.3479695129 
0.0190669253  0.0196233169  1.0681806119  0.9963954686  0.9051546392  0.7600229095  0.7353951890  0.9969031532  0.9684329200  0.9968445209  0.9334098737  44.490216271  2700          1.4013513382 
0.0129583430  0.0050478155  1.0151427919  1.0000000000  1.0000000000  1.0000000000  0.8056603774  0.8141660320  0.7987804878  1.0000000000  0.9274074074  84.805653710  3000          1.3544348200 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0415477483  0.0224709097  0.9420558127  1.0000000000  1.0000000000  0.9910461828  0.8415094340  0.9965727342  0.8704268293  0.8678267308  0.8844444444  33.922261484  1200          1.3437911328 
0.1021579381  0.1598871635  0.1418572760  1.0000000000  1.0000000000  0.9118755891  0.8188679245  0.8084539223  0.7728658537  0.9666790078  0.9348148148  8.4805653710  300           1.3557230822 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.5383257270  0.6413691044  1.2016643286  0.9973498233  1.0000000000  0.7422243167  0.7358490566  0.6839299315  0.6524390244  0.8604220659  0.8740740741  0.0000000000  0             10.173121690 
0.1155181293  0.1149748267  0.6450893326  0.9897013388  0.8927835052  0.9461626575  0.8430698740  0.9825450450  0.9526493799  0.9271371199  0.9265212400  9.8867147271  600           1.3489430539 
0.0209595130  0.0147480105  1.0658019416  0.9963954686  0.9030927835  0.7600229095  0.7342497136  0.9971846847  0.9706877114  0.9971313827  0.9334098737  49.433573635  3000          1.3604676851 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0313310460  0.0159088325  0.9674085174  1.0000000000  1.0000000000  0.9962299717  0.8396226415  0.9977151561  0.8704268293  0.8637541651  0.8814814815  42.402826855  1500          1.3433484467 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
1.1167805195  0.8228257298  1.1788178682  0.8321318229  0.8371134021  0.7004581901  0.6975945017  0.9009009009  0.8827508455  0.8990246701  0.9058553387  0.0000000000  0             8.3103849888 
0.0566984720  0.0477378339  0.4972718853  1.0000000000  1.0000000000  0.9618284637  0.8358490566  0.8164508759  0.7759146341  0.9866716031  0.9318518519  16.961130742  600           1.3634321856 
el_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.1147710982  0.1810918761  0.0993301096  1.0000000000  1.0000000000  0.9222431668  0.8320754717  0.9634424981  0.8673780488  0.8815253610  0.9037037037  8.4805653710  300           1.3625879526 
0.0949044116  0.0773774582  0.8036981065  0.9927909372  0.8989690722  0.9627720504  0.8533791523  0.9884572072  0.9571589628  0.9254159495  0.9276693456  14.830072090  900           1.3487010892 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.8762828708  0.5790016055  1.2673335075  0.8331616890  0.8371134021  0.6984536082  0.6884306987  0.8972409910  0.8827508455  0.8958691910  0.9081515499  0.0000000000  0             6.1047699451 
0.0243058238  0.0128353769  0.9771238975  1.0000000000  1.0000000000  0.9976437323  0.8396226415  0.9984767708  0.8658536585  0.8615327656  0.8814814815  50.883392226  1800          1.3435214980 
0.0517388597  0.0341448702  0.9261711850  1.0000000000  1.0000000000  0.9816211122  0.8301886792  0.8115003808  0.7789634146  0.9933358016  0.9318518519  25.441696113  900           1.3538719471 
0.2548021481  0.3084523697  0.4133720470  0.9768280124  0.8907216495  0.9123711340  0.8281786942  0.9284909910  0.9278466742  0.9687320711  0.9288174512  4.9433573635  300           1.3528946861 
0.0700401603  0.0590308937  0.9419413906  0.9938208033  0.9030927835  0.9702176403  0.8625429553  0.9929617117  0.9571589628  0.9236947791  0.9276693456  19.773429454  1200          1.3501220552 
0.2761420640  0.3186986847  0.3299242844  0.9732234809  0.8948453608  0.9049255441  0.8247422680  0.9738175676  0.9425028185  0.9239816408  0.9242250287  4.9433573635  300           1.3511942220 
0.0635648333  0.0532106591  0.9983331297  0.9938208033  0.9051546392  0.9768041237  0.8625429553  0.9940878378  0.9582863585  0.9205393001  0.9311136625  24.716786817  1500          1.3513222313 
0.0545709902  0.0374979715  0.8654712090  1.0000000000  1.0000000000  0.9830348728  0.8283018868  0.9897182026  0.8673780488  0.8718992966  0.9007407407  25.441696113  900           1.3501884874 
0.1100969114  0.1127250428  0.7413782972  0.9881565396  0.8907216495  0.9464490263  0.8476517755  0.9265202703  0.9244644870  0.9856569134  0.9322617681  9.8867147271  600           1.3536261018 
0.0173308439  0.0069128048  0.9786664943  1.0000000000  1.0000000000  0.9995287465  0.8377358491  0.9996191927  0.8658536585  0.8537578675  0.8829629630  67.844522968  2400          1.3427841806 
0.1233879158  0.1227050611  0.6804406971  0.9881565396  0.9010309278  0.9398625430  0.8476517755  0.9850788288  0.9526493799  0.9259896730  0.9276693456  9.8867147271  600           1.3518696872 
0.0497803073  0.0440682241  1.0292196266  0.9953656025  0.9092783505  0.9790950745  0.8682703322  0.9954954955  0.9594137542  0.9199655766  0.9265212400  29.660144181  1800          1.3501787035 
0.0313673394  0.0155331336  1.0143803972  1.0000000000  1.0000000000  0.9971724788  0.8301886792  0.8164508759  0.7774390244  0.9981488338  0.9333333333  42.402826855  1500          1.3563833968 
0.0165666470  0.0057309517  0.9728943191  1.0000000000  1.0000000000  0.9995287465  0.8377358491  0.9996191927  0.8612804878  0.8519067012  0.8814814815  76.325088339  2700          1.3446383874 
0.0836433181  0.0743347915  0.8516130312  0.9933058702  0.8948453608  0.9619129439  0.8556701031  0.9245495495  0.9323562570  0.9905335628  0.9288174512  14.830072090  900           1.3536118158 
0.0924255867  0.0796132528  0.8204838906  0.9912461380  0.9113402062  0.9607674685  0.8556701031  0.9898648649  0.9560315671  0.9228341939  0.9322617681  14.830072090  900           1.3535219614 
0.0465458549  0.0401039517  1.0407141475  0.9963954686  0.9113402062  0.9816723940  0.8671248568  0.9957770270  0.9605411499  0.9202524383  0.9253731343  34.603501544  2100          1.3504454048 
0.0127770196  0.0045740486  0.9676088780  1.0000000000  1.0000000000  1.0000000000  0.8320754717  1.0000000000  0.8628048780  0.8537578675  0.8829629630  84.805653710  3000          1.3404046877 
0.0303412664  0.0158612848  0.9706027418  1.0000000000  1.0000000000  0.9957587182  0.8283018868  0.9954303123  0.8673780488  0.8656053314  0.8948148148  42.402826855  1500          1.3557656026 
0.0637187780  0.0609327631  0.9688993907  0.9948506694  0.9051546392  0.9702176403  0.8591065292  0.9211711712  0.9255918828  0.9928284567  0.9311136625  19.773429454  1200          1.3555247275 
0.0743892413  0.0617633552  0.9541308073  0.9953656025  0.9051546392  0.9696449026  0.8625429553  0.9929617117  0.9582863585  0.9213998853  0.9253731343  19.773429454  1200          1.3543140054 
0.0370886714  0.0318444862  1.0514784046  0.9963954686  0.9092783505  0.9836769759  0.8694158076  0.9957770270  0.9605411499  0.9176706827  0.9219288175  39.546858908  2400          1.3485083103 
0.0275844747  0.0131327621  0.9715708297  1.0000000000  1.0000000000  0.9976437323  0.8188679245  0.9969535415  0.8689024390  0.8644946316  0.8888888889  50.883392226  1800          1.3629914236 
0.0482454101  0.0517789725  1.0202684738  0.9953656025  0.9092783505  0.9750859107  0.8659793814  0.9214527027  0.9278466742  0.9945496271  0.9288174512  24.716786817  1500          1.3564146630 
0.0604549305  0.0495308455  1.0076165819  0.9958805355  0.9010309278  0.9750859107  0.8682703322  0.9943693694  0.9616685457  0.9193918531  0.9276693456  24.716786817  1500          1.3537019467 
0.0234896951  0.0133188384  0.9754004488  1.0000000000  1.0000000000  0.9985862394  0.8169811321  0.9973343488  0.8689024390  0.8626434654  0.8844444444  59.363957597  2100          1.3517354941 
0.0428148855  0.0435879332  1.0403500175  0.9953656025  0.9092783505  0.9796678121  0.8705612829  0.9200450450  0.9312288613  0.9951233505  0.9265212400  29.660144181  1800          1.3596478057 
0.0280623181  0.0268783234  1.0590938479  0.9963954686  0.9092783505  0.9862542955  0.8659793814  0.9969031532  0.9605411499  0.9162363741  0.9173363949  49.433573635  3000          1.3482267729 
0.0403129639  0.0388825799  1.0350002571  0.9958805355  0.9051546392  0.9788087056  0.8671248568  0.9952139640  0.9616685457  0.9196787149  0.9276693456  29.660144181  1800          1.3537065403 
0.0193062819  0.0078797346  0.9757118311  1.0000000000  1.0000000000  1.0000000000  0.8169811321  0.9980959634  0.8719512195  0.8600518327  0.8829629630  67.844522968  2400          1.3539498687 
0.0348828056  0.0337518563  1.0517472742  0.9958805355  0.9113402062  0.9822451317  0.8659793814  0.9183558559  0.9301014656  0.9954102123  0.9276693456  34.603501544  2100          1.3561269029 
0.0375482727  0.0335138257  1.0454965172  0.9958805355  0.9051546392  0.9822451317  0.8682703322  0.9960585586  0.9627959414  0.9182444062  0.9276693456  34.603501544  2100          1.3533157651 
0.0178654528  0.0052327489  0.9762969569  1.0000000000  1.0000000000  1.0000000000  0.8188679245  0.9980959634  0.8704268293  0.8593113662  0.8755555556  76.325088339  2700          1.3524996273 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.5277967453  0.5493335128  1.1069604158  0.9964664311  1.0000000000  0.7516493874  0.7433962264  0.7364813404  0.7149390244  0.8704183636  0.8785185185  0.0000000000  0             8.4237499237 
0.0289137916  0.0314236190  1.0568898888  0.9963954686  0.9092783505  0.9833906071  0.8636884307  0.9172297297  0.9289740699  0.9965576592  0.9253731343  39.546858908  2400          1.4006244485 
0.0302477354  0.0309529600  1.0494196765  0.9958805355  0.9030927835  0.9842497136  0.8625429553  0.9963400901  0.9616685457  0.9173838210  0.9276693456  39.546858908  2400          1.3543894156 
0.0140906015  0.0056386455  0.9730811661  1.0000000000  1.0000000000  1.0000000000  0.8169811321  0.9988575781  0.8719512195  0.8556090337  0.8651851852  84.805653710  3000          1.3547975222 
0.1109447022  0.1826130002  0.1228304429  1.0000000000  1.0000000000  0.9203581527  0.8283018868  0.9584920030  0.8628048780  0.8822658275  0.9155555556  8.4805653710  300           1.3586680118 
0.0225660084  0.0282996645  1.0673113386  0.9963954686  0.9092783505  0.9848224513  0.8648339061  0.9177927928  0.9278466742  0.9965576592  0.9242250287  44.490216271  2700          1.3607807628 
0.0300166017  0.0302414623  1.0561953882  0.9958805355  0.9051546392  0.9856815578  0.8602520046  0.9966216216  0.9594137542  0.9165232358  0.9265212400  44.490216271  2700          1.3520612439 
0.0653584145  0.0485809601  0.4281987699  1.0000000000  1.0000000000  0.9641847314  0.8207547170  0.9798172125  0.8567073171  0.8807848945  0.9007407407  16.961130742  600           1.3559328151 
0.0261245351  0.0261094214  1.0535282904  0.9963954686  0.9030927835  0.9862542955  0.8625429553  0.9969031532  0.9594137542  0.9139414802  0.9288174512  49.433573635  3000          1.3513012600 
0.0195374577  0.0228941310  1.0676726836  0.9963954686  0.9092783505  0.9859679267  0.8602520046  0.9161036036  0.9244644870  0.9965576592  0.9230769231  49.433573635  3000          1.3605050421 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
1.0701770782  0.9256696701  1.2486757040  0.8357363543  0.8412371134  0.7018900344  0.6930126002  0.9045608108  0.8872604284  0.8987378084  0.9070034443  0.0000000000  0             4.9604802132 
0.0597772377  0.0377717253  0.8884370924  1.0000000000  1.0000000000  0.9811498586  0.8245283019  0.9874333587  0.8475609756  0.8733802295  0.8933333333  25.441696113  900           1.3863710260 
0.2717795015  0.3185425130  0.3044254411  0.9691040165  0.8886597938  0.9077892325  0.8419243986  0.9749436937  0.9447576099  0.9248422260  0.9253731343  4.9433573635  300           1.3567913628 
0.0455066021  0.0253106890  0.9369057731  1.0000000000  1.0000000000  0.9891611687  0.8226415094  0.9935262757  0.8506097561  0.8641243984  0.8829629630  33.922261484  1200          1.3549673669 
0.0308123350  0.0164475095  0.9642381132  1.0000000000  1.0000000000  0.9934024505  0.8207547170  0.9973343488  0.8521341463  0.8630136986  0.8785185185  42.402826855  1500          1.3580839427 
0.1162786489  0.1141023546  0.6646642024  0.9881565396  0.8886597938  0.9418671249  0.8510882016  0.9862049550  0.9526493799  0.9274239816  0.9242250287  9.8867147271  600           1.3559253939 
0.0300282778  0.0144653359  0.9742904015  1.0000000000  1.0000000000  0.9971724788  0.8264150943  0.9996191927  0.8536585366  0.8585708997  0.8755555556  50.883392226  1800          1.3598823007 
0.0861322386  0.0826568803  0.8228959001  0.9933058702  0.8989690722  0.9576174112  0.8579610538  0.9909909910  0.9594137542  0.9282845668  0.9276693456  14.830072090  900           1.3567383734 
0.0217110137  0.0102845671  0.9724770083  1.0000000000  1.0000000000  0.9981149859  0.8264150943  0.9996191927  0.8521341463  0.8552388004  0.8755555556  59.363957597  2100          1.3575566951 
0.0742049939  0.0607623997  0.9486289499  0.9943357364  0.9010309278  0.9667812142  0.8602520046  0.9940878378  0.9605411499  0.9242685026  0.9265212400  19.773429454  1200          1.3573427566 
0.0185525689  0.0060008071  0.9695266122  1.0000000000  1.0000000000  0.9995287465  0.8226415094  0.9996191927  0.8567073171  0.8500555350  0.8681481481  67.844522968  2400          1.3560403927 
0.0606574378  0.0547422203  0.9979400724  0.9948506694  0.9051546392  0.9745131730  0.8659793814  0.9949324324  0.9594137542  0.9205393001  0.9219288175  24.716786817  1500          1.3582687529 
0.0143624429  0.0052367847  0.9710828769  1.0000000000  1.0000000000  0.9995287465  0.8207547170  0.9996191927  0.8582317073  0.8526471677  0.8681481481  76.325088339  2700          1.3564211726 
0.0486218425  0.0416712449  1.0285356754  0.9953656025  0.9010309278  0.9779495991  0.8682703322  0.9952139640  0.9571589628  0.9191049914  0.9242250287  29.660144181  1800          1.3569839025 
0.0139870563  0.0072825709  0.9694947392  1.0000000000  1.0000000000  1.0000000000  0.8245283019  0.9996191927  0.8582317073  0.8511662347  0.8607407407  84.805653710  3000          1.3555538996 
0.0429656876  0.0402181387  1.0373385292  0.9963954686  0.8989690722  0.9808132875  0.8671248568  0.9963400901  0.9571589628  0.9179575445  0.9230769231  34.603501544  2100          1.3578029855 
0.0359448888  0.0338693327  1.0497583812  0.9963954686  0.8989690722  0.9831042383  0.8694158076  0.9963400901  0.9571589628  0.9191049914  0.9207807118  39.546858908  2400          1.3570728906 
0.0296743880  0.0297041141  1.0598933379  0.9963954686  0.9010309278  0.9839633448  0.8694158076  0.9963400901  0.9560315671  0.9179575445  0.9161882893  44.490216271  2700          1.3580901504 
0.0252971473  0.0277860804  1.0583746771  0.9963954686  0.9010309278  0.9839633448  0.8705612829  0.9969031532  0.9571589628  0.9165232358  0.9150401837  49.433573635  3000          1.3601854269 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1014, in update
    all_x_anchor = torch.cat([data[0].cuda().float() for data in minibatches])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.01 GiB already allocated; 56.62 MiB free; 1.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 300, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 204, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 192, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 39.59 GiB total capacity; 20.20 GiB already allocated; 56.62 MiB free; 20.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1014, in update
    all_x_anchor = torch.cat([data[0].cuda().float() for data in minibatches])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.01 GiB already allocated; 56.62 MiB free; 1.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1019, in update
    image_features_target = self.featurizer(all_x_anchor, all_index, mask=False)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 290, in forward
    x = self.conv1(x)  # shape = [*, width, grid, grid]
        ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.06 GiB already allocated; 24.62 MiB free; 1.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 300, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 204, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 192, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 39.59 GiB total capacity; 20.20 GiB already allocated; 56.62 MiB free; 20.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1019, in update
    image_features_target = self.featurizer(all_x_anchor, all_index, mask=False)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 290, in forward
    x = self.conv1(x)  # shape = [*, width, grid, grid]
        ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.06 GiB already allocated; 24.62 MiB free; 1.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1014, in update
    all_x_anchor = torch.cat([data[0].cuda().float() for data in minibatches])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.01 GiB already allocated; 56.62 MiB free; 1.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 300, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 204, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 192, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 39.59 GiB total capacity; 20.20 GiB already allocated; 56.62 MiB free; 20.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.8656851649  0.6417100430  1.1525470018  0.8316168898  0.8350515464  0.7013172967  0.6907216495  0.9073761261  0.8917700113  0.8978772232  0.9092996556  0.0000000000  0             2.1836445332 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.8189501762  0.6761021018  1.1451427937  0.8331616890  0.8309278351  0.6930126002  0.6941580756  0.8989301802  0.8827508455  0.9024670109  0.9070034443  0.0000000000  0             9.5108416080 
mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1019, in update
    image_features_target = self.featurizer(all_x_anchor, all_index, mask=False)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 290, in forward
    x = self.conv1(x)  # shape = [*, width, grid, grid]
        ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.06 GiB already allocated; 24.62 MiB free; 1.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1014, in update
    all_x_anchor = torch.cat([data[0].cuda().float() for data in minibatches])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.01 GiB already allocated; 56.62 MiB free; 1.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 300, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 204, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 192, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 39.59 GiB total capacity; 20.20 GiB already allocated; 56.62 MiB free; 20.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.8628466129  0.6326796412  1.1053746939  0.8182286303  0.8350515464  0.6935853379  0.6998854525  0.9031531532  0.8928974070  0.8958691910  0.9081515499  0.0000000000  0             13.867487907 
ject/APLCLIP/domainbed/algorithms.py", line 1019, in update
    image_features_target = self.featurizer(all_x_anchor, all_index, mask=False)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 290, in forward
    x = self.conv1(x)  # shape = [*, width, grid, grid]
        ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.06 GiB already allocated; 24.62 MiB free; 1.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1014, in update
    all_x_anchor = torch.cat([data[0].cuda().float() for data in minibatches])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.01 GiB already allocated; 56.62 MiB free; 1.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 300, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 204, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 192, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 39.59 GiB total capacity; 20.20 GiB already allocated; 56.62 MiB free; 20.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1014, in update
    all_x_anchor = torch.cat([data[0].cuda().float() for data in minibatches])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.01 GiB already allocated; 56.62 MiB free; 1.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1019, in update
    image_features_target = self.featurizer(all_x_anchor, all_index, mask=False)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 290, in forward
    x = self.conv1(x)  # shape = [*, width, grid, grid]
        ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.06 GiB already allocated; 24.62 MiB free; 1.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 300, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 204, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 192, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 39.59 GiB total capacity; 20.20 GiB already allocated; 56.62 MiB free; 20.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1014, in update
    all_x_anchor = torch.cat([data[0].cuda().float() for data in minibatches])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.01 GiB already allocated; 56.62 MiB free; 1.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1019, in update
    image_features_target = self.featurizer(all_x_anchor, all_index, mask=False)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 290, in forward
    x = self.conv1(x)  # shape = [*, width, grid, grid]
        ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.06 GiB already allocated; 24.62 MiB free; 1.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 300, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 204, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 192, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 39.59 GiB total capacity; 20.20 GiB already allocated; 56.62 MiB free; 20.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1014, in update
    all_x_anchor = torch.cat([data[0].cuda().float() for data in minibatches])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.01 GiB already allocated; 56.62 MiB free; 1.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1019, in update
    image_features_target = self.featurizer(all_x_anchor, all_index, mask=False)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 290, in forward
    x = self.conv1(x)  # shape = [*, width, grid, grid]
        ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.06 GiB already allocated; 24.62 MiB free; 1.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 300, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 204, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 192, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 39.59 GiB total capacity; 20.20 GiB already allocated; 56.62 MiB free; 20.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1014, in update
    all_x_anchor = torch.cat([data[0].cuda().float() for data in minibatches])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.01 GiB already allocated; 56.62 MiB free; 1.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1019, in update
    image_features_target = self.featurizer(all_x_anchor, all_index, mask=False)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 290, in forward
    x = self.conv1(x)  # shape = [*, width, grid, grid]
        ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.06 GiB already allocated; 24.62 MiB free; 1.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 300, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 204, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 192, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 39.59 GiB total capacity; 20.20 GiB already allocated; 56.62 MiB free; 20.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1014, in update
    all_x_anchor = torch.cat([data[0].cuda().float() for data in minibatches])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.01 GiB already allocated; 56.62 MiB free; 1.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1014, in update
    all_x_anchor = torch.cat([data[0].cuda().float() for data in minibatches])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.01 GiB already allocated; 56.62 MiB free; 1.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 300, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 204, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 192, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 39.59 GiB total capacity; 20.20 GiB already allocated; 56.62 MiB free; 20.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1014, in update
    all_x_anchor = torch.cat([data[0].cuda().float() for data in minibatches])
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 39.59 GiB total capacity; 1.01 GiB already allocated; 56.62 MiB free; 1.13 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 300, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 204, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 192, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 39.59 GiB total capacity; 20.20 GiB already allocated; 56.62 MiB free; 20.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 300, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 204, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 192, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 39.59 GiB total capacity; 20.20 GiB already allocated; 56.62 MiB free; 20.47 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
0.2315982403  0.2972979850  0.4326542915  0.8733264676  0.8639175258  0.9138029782  0.8316151203  0.9743806306  0.9470124014  0.9727481354  0.9322617681  4.9433573635  300           1.4127429525 
0.2336146027  0.3053033177  0.4615082298  0.8707518023  0.8597938144  0.9095074456  0.8419243986  0.9780405405  0.9481397971  0.9727481354  0.9322617681  4.9433573635  300           1.4115308714 
kip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.5177974105  1.1101583242  0.8233779609  0.8268041237  0.6944444444  0.6895761741  0.9000563063  0.8895152198  0.8967297762  0.9070034443  0.0000000000  0             7.9404249191 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.5125753284  1.1414661407  0.8305870237  0.8247422680  0.6961626575  0.6781214204  0.9051238739  0.8928974070  0.8987378084  0.9070034443  0.0000000000  0             3.5975987911 
0.1132886807  0.1244113628  0.7185256996  0.8748712667  0.8659793814  0.9424398625  0.8613974800  0.9878941441  0.9537767756  0.9862306368  0.9322617681  9.8867147271  600           1.4135548043 
0.1128930380  0.1255902423  0.7135905796  0.8728115345  0.8721649485  0.9484536082  0.8533791523  0.9853603604  0.9503945885  0.9850831899  0.9357060850  9.8867147271  600           1.4140379119 
0.1073264599  0.1254204720  0.7368618043  0.8738414006  0.8556701031  0.9415807560  0.8522336770  0.9881756757  0.9515219842  0.9868043603  0.9288174512  9.8867147271  600           1.4175518560 
1.1560101509  0.8269824923  0.8309278351  0.6927262314  0.6861397480  0.9003378378  0.8895152198  0.8958691910  0.9035591274  0.0000000000  0             10.748404264 
0.2260801267  0.5990950899  0.8738414006  0.8639175258  0.9332760596  0.8373424971  0.9867680180  0.9605411499  0.9816408491  0.9253731343  4.9433573635  300           1.4893701927 
0.0869433546  0.0891367483  0.8577676821  0.8748712667  0.8783505155  0.9601947308  0.8739977090  0.9904279279  0.9594137542  0.9922547332  0.9311136625  14.830072090  900           1.4149868226 
0.0809675591  0.0835963630  0.8516913611  0.8738414006  0.8701030928  0.9627720504  0.8659793814  0.9895833333  0.9515219842  0.9902467011  0.9322617681  14.830072090  900           1.4144930919 
0.0818550668  0.0846246471  0.8589642372  0.8728115345  0.8536082474  0.9596219931  0.8591065292  0.9923986486  0.9605411499  0.9908204246  0.9196326062  14.830072090  900           1.4172393521 
0.0811462875  0.8760599732  0.8722966014  0.8701030928  0.9647766323  0.8602520046  0.9932432432  0.9594137542  0.9916810098  0.9299655568  9.8867147271  600           1.3626753298 
0.0912485710  0.8768978179  0.8738414006  0.8659793814  0.9676403207  0.8648339061  0.9926801802  0.9594137542  0.9905335628  0.9299655568  9.8867147271  600           1.3585897017 
1.4152541320 
0.0679136610  0.0689318034  0.9619263295  0.8722966014  0.8721649485  0.9690721649  0.8751431844  0.9923986486  0.9627959414  0.9925415950  0.9322617681  19.773429454  1200          1.4151474635 
0.0578685059  0.0659505706  0.9754441212  0.8717816684  0.8577319588  0.9705040092  0.8659793814  0.9943693694  0.9627959414  0.9931153184  0.9207807118  19.773429454  1200          1.4185345531 
0.0613568118  1.0051361698  0.8697219361  0.8701030928  0.9753722795  0.8682703322  0.9949324324  0.9616685457  0.9948364888  0.9265212400  14.830072090  900           1.3625642625 
0.0577502805  0.0582376818  1.0147023938  0.8743563337  0.8659793814  0.9765177549  0.8728522337  0.9940878378  0.9616685457  0.9934021801  0.9253731343  24.716786817  1500          1.4167592875 
0.0545156055  0.0575335388  1.0184888800  0.8743563337  0.8742268041  0.9768041237  0.8739977090  0.9943693694  0.9616685457  0.9934021801  0.9288174512  24.716786817  1500          1.4173954654 
0.0589924693  1.0011113471  0.8779608651  0.8680412371  0.9759450172  0.8625429553  0.9957770270  0.9639233371  0.9942627653  0.9219288175  14.830072090  900           1.3617626516 
0.0510670258  0.0565210793  1.0214522558  0.8707518023  0.8577319588  0.9762313860  0.8648339061  0.9949324324  0.9639233371  0.9945496271  0.9230769231  24.716786817  1500          1.4189254578 
0.0501490873  1.0536328024  0.8666323378  0.8701030928  0.9793814433  0.8671248568  0.9963400901  0.9616685457  0.9956970740  0.9288174512  19.773429454  1200          1.3647944450 
0.0402050300  0.0423596268  1.0416476387  0.8743563337  0.8639175258  0.9796678121  0.8751431844  0.9946509009  0.9616685457  0.9936890419  0.9253731343  29.660144181  1800          1.4176109497 
0.0459917149  0.0482663095  1.0468455931  0.8748712667  0.8721649485  0.9802405498  0.8774341352  0.9957770270  0.9616685457  0.9942627653  0.9276693456  29.660144181  1800          1.4164026380 
0.0474330776  1.0482593099  0.8748712667  0.8804123711  0.9802405498  0.8613974800  0.9963400901  0.9639233371  0.9954102123  0.9242250287  19.773429454  1200          1.3624303444 
0.0479713032  1.0501526622  0.8743563337  0.8721649485  0.9813860252  0.8705612829  0.9969031532  0.9639233371  0.9951233505  0.9299655568  19.773429454  1200          1.3748477721 
1.4191086117 
0.0395880334  1.0693348608  0.8738414006  0.8742268041  0.9848224513  0.8636884307  0.9969031532  0.9605411499  0.9962707975  0.9322617681  24.716786817  1500          1.3661154985 
1.4156294688 
0.0348338880  0.0400857711  1.0596335340  0.8733264676  0.8701030928  0.9825315006  0.8739977090  0.9952139640  0.9605411499  0.9945496271  0.9242250287  34.603501544  2100          1.4168450546 
0.0366926366  1.0692942087  0.8759011329  0.8762886598  0.9833906071  0.8602520046  0.9963400901  0.9650507328  0.9962707975  0.9242250287  24.716786817  1500          1.3714692577 
0.0401027370  1.0696936186  0.8759011329  0.8762886598  0.9845360825  0.8682703322  0.9971846847  0.9661781285  0.9965576592  0.9322617681  24.716786817  1500          1.3751721406 
0.0315331258  0.0343133725  1.0608650484  0.8707518023  0.8659793814  0.9822451317  0.8659793814  0.9960585586  0.9616685457  0.9962707975  0.9265212400  34.603501544  2100          1.4191928434 
0.0295942044  0.0323653918  1.0661954210  0.8738414006  0.8804123711  0.9831042383  0.8739977090  0.9971846847  0.9627959414  0.9956970740  0.9299655568  39.546858908  2400          1.4165369566 
0.0293886046  0.0340052733  1.0735684756  0.8722966014  0.8701030928  0.9842497136  0.8717067583  0.9954954955  0.9582863585  0.9951233505  0.9242250287  39.546858908  2400          1.4169147031 
0.0324998822  1.0789530255  0.8764160659  0.8783505155  0.9853951890  0.8625429553  0.9971846847  0.9594137542  0.9962707975  0.9334098737  29.660144181  1800          1.3650686725 
0.0336557592  1.0802391992  0.8795056643  0.8783505155  0.9848224513  0.8591065292  0.9969031532  0.9639233371  0.9965576592  0.9242250287  29.660144181  1800          1.3632863100 
0.0339278601  1.0779892431  0.8779608651  0.8783505155  0.9856815578  0.8682703322  0.9971846847  0.9673055242  0.9968445209  0.9299655568  29.660144181  1800          1.3710171183 
0.0279479760  0.0330968601  1.0713297353  0.8697219361  0.8639175258  0.9839633448  0.8682703322  0.9960585586  0.9639233371  0.9965576592  0.9242250287  39.546858908  2400          1.4200492287 
0.0200019506  0.0290000632  1.0757577501  0.8748712667  0.8845360825  0.9851088202  0.8705612829  0.9971846847  0.9639233371  0.9956970740  0.9265212400  44.490216271  2700          1.4169028362 
0.0284043112  0.0292971806  1.0704705447  0.8717816684  0.8659793814  0.9853951890  0.8717067583  0.9966216216  0.9594137542  0.9956970740  0.9230769231  44.490216271  2700          1.4164980928 
0.0278735326  1.0818482176  0.8764160659  0.8783505155  0.9856815578  0.8613974800  0.9974662162  0.9605411499  0.9974182444  0.9311136625  34.603501544  2100          1.3637202048 
0.0324464002  1.0820667152  0.8825952626  0.8804123711  0.9862542955  0.8579610538  0.9971846847  0.9627959414  0.9968445209  0.9253731343  34.603501544  2100          1.3614706691 
0.0306969672  1.0778949179  0.8753861998  0.8804123711  0.9856815578  0.8694158076  0.9977477477  0.9650507328  0.9968445209  0.9299655568  34.603501544  2100          1.3734250180 
0.0209158067  0.0257214353  1.0740208312  0.8722966014  0.8618556701  0.9862542955  0.8694158076  0.9969031532  0.9594137542  0.9959839357  0.9184845006  49.433573635  3000          1.4167894204 
0.0208297241  0.0267250758  1.0755482274  0.8743563337  0.8804123711  0.9865406644  0.8694158076  0.9974662162  0.9639233371  0.9959839357  0.9276693456  49.433573635  3000          1.4164870596 
0.0272892657  0.0263101865  1.0686574980  0.8712667353  0.8639175258  0.9842497136  0.8682703322  0.9966216216  0.9650507328  0.9965576592  0.9230769231  44.490216271  2700          1.4189355930 
/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Using clip_transform ViT-B/16
0.0243616741  1.0843604271  0.8753861998  0.8804123711  0.9865406644  0.8636884307  0.9974662162  0.9605411499  0.9974182444  0.9288174512  39.546858908  2400          1.3634462182 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.6205325127  0.4412071705  1.1436308622  0.8305870237  0.8226804124  0.6847079038  0.6792668958  0.8935810811  0.8827508455  0.8967297762  0.9035591274  0.0000000000  0             4.4969928265 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.8117780089  0.6576359868  1.1882258654  0.8372811535  0.8371134021  0.6907216495  0.6827033219  0.9031531532  0.8928974070  0.9024670109  0.9184845006  0.0000000000  0             2.5959835052 
0.0260696140  1.0839808293  0.8825952626  0.8824742268  0.9879725086  0.8613974800  0.9971846847  0.9627959414  0.9968445209  0.9265212400  39.546858908  2400          1.3632472205 
0.0237794622  1.0837762626  0.8743563337  0.8824742268  0.9868270332  0.8636884307  0.9980292793  0.9627959414  0.9968445209  0.9299655568  39.546858908  2400          1.3681000479 
0.0216109509  0.0256978510  1.0758569437  0.8712667353  0.8618556701  0.9856815578  0.8717067583  0.9969031532  0.9650507328  0.9965576592  0.9219288175  49.433573635  3000          1.4186395486 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.1822142668  0.1920880628  0.5785175715  0.9783728115  0.8969072165  0.7431271478  0.7331042383  0.9780405405  0.9492671928  0.9753298910  0.9380022962  4.9433573635  300           1.4128596147 
0.1887583597  0.1962865262  0.5856376000  0.9768280124  0.8865979381  0.7342497136  0.7262313860  0.9780405405  0.9391206313  0.9730349971  0.9357060850  4.9433573635  300           1.4134946370 
0.0223589551  1.0883387458  0.8815653965  0.8824742268  0.9879725086  0.8613974800  0.9977477477  0.9639233371  0.9971313827  0.9276693456  44.490216271  2700          1.3649777460 
step_time    
0.7168077826  0.4673906565  1.1626155376  0.8311019567  0.8329896907  0.6835624284  0.6827033219  0.8969594595  0.8816234498  0.8961560528  0.9001148106  0.0000000000  0             2.1444849968 
0.0238813472  1.0829155592  0.8738414006  0.8824742268  0.9873997709  0.8659793814  0.9980292793  0.9627959414  0.9968445209  0.9265212400  44.490216271  2700          1.3635226790 
0.0216122620  1.0864754522  0.8733264676  0.8804123711  0.9882588774  0.8636884307  0.9974662162  0.9616685457  0.9974182444  0.9288174512  49.433573635  3000          1.3644576732 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0840076584  0.0682899519  0.7229062961  0.9886714727  0.8989690722  0.7500000000  0.7399770905  0.9878941441  0.9549041714  0.9865174986  0.9380022962  9.8867147271  600           1.4149966311 
0.0886807961  0.0713206313  0.7154229085  0.9881565396  0.8989690722  0.7405498282  0.7376861397  0.9873310811  0.9481397971  0.9862306368  0.9368541906  9.8867147271  600           1.4138798046 
0.1927303059  0.1940806099  0.5699508210  0.9757981462  0.8927835052  0.7462772050  0.7273768614  0.9774774775  0.9515219842  0.9698795181  0.9322617681  4.9433573635  300           1.4151961819 
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4587937593  1.1697406769  0.8300720906  0.8371134021  0.6841351661  0.6758304696  0.8927364865  0.8861330327  0.8952954676  0.9092996556  0.0000000000  0             3.1707594395 
0.0234538386  1.0858249569  0.8815653965  0.8824742268  0.9882588774  0.8602520046  0.9977477477  0.9650507328  0.9971313827  0.9276693456  49.433573635  3000          1.3630630040 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0230362577  1.0878337606  0.8743563337  0.8824742268  0.9882588774  0.8659793814  0.9980292793  0.9627959414  0.9968445209  0.9253731343  49.433573635  3000          1.3686025349 
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.5155941844  1.1742916107  0.8295571576  0.8288659794  0.6867124857  0.6884306987  0.9039977477  0.8996617813  0.9018932874  0.9092996556  0.0000000000  0             2.4386255741 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0712916912  0.0516003947  0.8659207300  0.9943357364  0.9010309278  0.7520045819  0.7399770905  0.9926801802  0.9571589628  0.9916810098  0.9380022962  14.830072090  900           1.4152074035 
0.0627063441  0.0491083833  0.8690472893  0.9927909372  0.9010309278  0.7505727377  0.7353951890  0.9915540541  0.9560315671  0.9908204246  0.9322617681  14.830072090  900           1.4149314197 
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.3731027842  1.1540125608  0.8300720906  0.8371134021  0.6844215349  0.6792668958  0.9000563063  0.8883878241  0.8984509466  0.9058553387  0.0000000000  0             2.1487641335 
0.1337538599  0.6906841325  0.9886714727  0.8969072165  0.7448453608  0.7319587629  0.9884572072  0.9549041714  0.9845094664  0.9265212400  4.9433573635  300           1.3645649044 
1.4160800242 
0.1349360130  0.6879602381  0.9907312049  0.8969072165  0.7428407789  0.7273768614  0.9845157658  0.9549041714  0.9842226047  0.9380022962  4.9433573635  300           1.3603508170 
0.0478301728  0.0396688769  0.9753242892  0.9948506694  0.9030927835  0.7594501718  0.7411225659  0.9949324324  0.9627959414  0.9928284567  0.9322617681  19.773429454  1200          1.4162170434 
0.0598761296  0.0475413726  0.9674206712  0.9953656025  0.9010309278  0.7545819015  0.7331042383  0.9949324324  0.9582863585  0.9936890419  0.9357060850  19.773429454  1200          1.4166901223 
0.1359659587  0.6717576365  0.9897013388  0.8948453608  0.7425544101  0.7159221077  0.9859234234  0.9537767756  0.9830751578  0.9357060850  4.9433573635  300           1.3610113470 
0.0636516472  0.0522358750  0.8704941984  0.9933058702  0.9030927835  0.7465635739  0.7262313860  0.9921171171  0.9616685457  0.9911072863  0.9311136625  14.830072090  900           1.4170457967 
0.0530392493  0.8917090718  0.9943357364  0.9092783505  0.7500000000  0.7365406644  0.9946509009  0.9616685457  0.9911072863  0.9288174512  9.8867147271  600           1.3593784444 
0.0546780844  0.8798718395  0.9958805355  0.8989690722  0.7502863688  0.7353951890  0.9921171171  0.9594137542  0.9908204246  0.9357060850  9.8867147271  600           1.3724485882 
0.0486672094  0.0351690747  1.0168529111  0.9963954686  0.9072164948  0.7611683849  0.7399770905  0.9954954955  0.9616685457  0.9942627653  0.9311136625  24.716786817  1500          1.4164357003 
0.0475799999  0.0359031629  1.0256198637  0.9953656025  0.9072164948  0.7577319588  0.7376861397  0.9957770270  0.9650507328  0.9951233505  0.9322617681  24.716786817  1500          1.4170838316 
0.0503248150  0.8926209247  0.9938208033  0.9010309278  0.7488545246  0.7331042383  0.9938063063  0.9594137542  0.9919678715  0.9357060850  9.8867147271  600           1.3650059438 
0.0534084300  0.0409088724  0.9784827834  0.9948506694  0.9010309278  0.7537227950  0.7273768614  0.9938063063  0.9639233371  0.9931153184  0.9311136625  19.773429454  1200          1.4181904968 
0.0378702061  1.0122990507  0.9963954686  0.9134020619  0.7528636884  0.7411225659  0.9963400901  0.9650507328  0.9939759036  0.9311136625  14.830072090  900           1.3635739923 
0.0404113275  1.0107836437  0.9958805355  0.8989690722  0.7534364261  0.7365406644  0.9949324324  0.9616685457  0.9954102123  0.9345579793  14.830072090  900           1.3647235298 
1.4168689950 
0.0367859592  0.0264594639  1.0427469925  0.9963954686  0.9113402062  0.7585910653  0.7411225659  0.9963400901  0.9616685457  0.9951233505  0.9311136625  29.660144181  1800          1.4168173114 
0.0415455219  1.0108260081  0.9958805355  0.9030927835  0.7494272623  0.7365406644  0.9966216216  0.9639233371  0.9951233505  0.9322617681  14.830072090  900           1.3688351337 
0.0300469416  1.0523385018  0.9958805355  0.9072164948  0.7577319588  0.7399770905  0.9969031532  0.9650507328  0.9954102123  0.9299655568  19.773429454  1200          1.3624980283 
0.0427916020  0.0307397212  1.0230551616  0.9948506694  0.9030927835  0.7585910653  0.7239404353  0.9949324324  0.9627959414  0.9945496271  0.9299655568  24.716786817  1500          1.4184123333 
0.0333105427  0.0268992269  1.0580924841  0.9963954686  0.9134020619  0.7588774341  0.7468499427  0.9974662162  0.9627959414  0.9954102123  0.9299655568  34.603501544  2100          1.4186381737 
0.0236700053  0.0200659525  1.0624959052  0.9963954686  0.9092783505  0.7571592211  0.7319587629  0.9969031532  0.9650507328  0.9962707975  0.9334098737  34.603501544  2100          1.4192532484 
0.0290803739  1.0509683643  0.9958805355  0.9051546392  0.7551546392  0.7457044674  0.9966216216  0.9639233371  0.9965576592  0.9368541906  19.773429454  1200          1.3599384975 
0.0305983905  1.0532296298  0.9963954686  0.9051546392  0.7514318442  0.7388316151  0.9969031532  0.9639233371  0.9956970740  0.9322617681  19.773429454  1200          1.3615538311 
0.0241785812  1.0712450653  0.9958805355  0.9072164948  0.7548682703  0.7365406644  0.9974662162  0.9616685457  0.9968445209  0.9311136625  24.716786817  1500          1.3614449271 
0.0317185263  0.0276561534  1.0459592988  0.9958805355  0.9051546392  0.7577319588  0.7250859107  0.9957770270  0.9627959414  0.9954102123  0.9288174512  29.660144181  1800          1.4236293602 
0.0264183618  0.0225748835  1.0603151512  0.9963954686  0.9113402062  0.7585910653  0.7457044674  0.9974662162  0.9639233371  0.9959839357  0.9299655568  39.546858908  2400          1.4182511171 
0.0218779976  1.0669972440  0.9958805355  0.9092783505  0.7551546392  0.7434135166  0.9971846847  0.9673055242  0.9965576592  0.9357060850  24.716786817  1500          1.3638496121 
0.0226917829  1.0701286807  0.9963954686  0.8989690722  0.7488545246  0.7422680412  0.9971846847  0.9627959414  0.9959839357  0.9345579793  24.716786817  1500          1.3585659615 
0.0182248543  1.0720949360  0.9963954686  0.9072164948  0.7525773196  0.7365406644  0.9977477477  0.9627959414  0.9974182444  0.9288174512  29.660144181  1800          1.3600973384 
0.0190380769  0.0153689167  1.0667962815  0.9963954686  0.9092783505  0.7577319588  0.7468499427  0.9974662162  0.9616685457  0.9968445209  0.9288174512  44.490216271  2700          1.4188599086 
0.0244022620  0.0207615371  1.0638333486  0.9963954686  0.9092783505  0.7574455899  0.7331042383  0.9971846847  0.9673055242  0.9965576592  0.9334098737  44.490216271  2700          1.4199197968 
0.0221457056  1.0731312692  0.9963954686  0.9092783505  0.7560137457  0.7468499427  0.9977477477  0.9684329200  0.9971313827  0.9334098737  29.660144181  1800          1.3644996866 
1.4201245395 
0.0230236099  1.0719525590  0.9963954686  0.8969072165  0.7482817869  0.7445589920  0.9980292793  0.9627959414  0.9965576592  0.9311136625  29.660144181  1800          1.3683895127 
0.0193393604  1.0793085583  0.9963954686  0.9030927835  0.7508591065  0.7388316151  0.9977477477  0.9627959414  0.9974182444  0.9299655568  34.603501544  2100          1.3654026953 
1.4304619718 
0.0160687948  0.0156576122  1.0673407153  0.9963954686  0.9134020619  0.7580183276  0.7319587629  0.9974662162  0.9661781285  0.9971313827  0.9334098737  49.433573635  3000          1.4298295561 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.8496324420  0.6238529682  1.2090851068  0.8290422245  0.8309278351  0.7010309278  0.6918671249  0.8938626126  0.8838782413  0.8967297762  0.9058553387  0.0000000000  0             2.2954902649 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
1.1699959040  0.9436728358  1.1843398809  0.8419155510  0.8391752577  0.7079037801  0.6975945017  0.9028716216  0.8906426156  0.8970166380  0.9150401837  0.0000000000  0             2.2200264931 
0.0168819829  1.0769479473  0.9963954686  0.9030927835  0.7551546392  0.7422680412  0.9983108108  0.9718151071  0.9971313827  0.9322617681  34.603501544  2100          1.3620571494 
0.0237281731  0.0186725083  1.0606045069  0.9963954686  0.9072164948  0.7600229095  0.7285223368  0.9969031532  0.9639233371  0.9971313827  0.9230769231  39.546858908  2400          1.4220930974 
0.0179924611  1.0774577339  0.9963954686  0.8969072165  0.7482817869  0.7388316151  0.9980292793  0.9627959414  0.9968445209  0.9299655568  34.603501544  2100          1.3654907767 
0.0170311901  1.0787274132  0.9963954686  0.8969072165  0.7500000000  0.7434135166  0.9977477477  0.9650507328  0.9974182444  0.9265212400  39.546858908  2400          1.3626408180 
0.2316610556  0.3122146481  0.4613638705  0.9763130793  0.8865979381  0.9092210767  0.8224513173  0.9253941441  0.9199549042  0.9721744119  0.9288174512  4.9433573635  300           1.4135746638 
0.2321311029  0.3140281292  0.4276227409  0.9716786818  0.8907216495  0.9169530355  0.8281786942  0.9287725225  0.9199549042  0.9750430293  0.9276693456  4.9433573635  300           1.4141009831 
0.0161087590  1.0777051834  0.9963954686  0.9092783505  0.7551546392  0.7376861397  0.9983108108  0.9740698985  0.9971313827  0.9322617681  39.546858908  2400          1.3637430056 
0.0181335089  1.0786378300  0.9963954686  0.9010309278  0.7462772050  0.7411225659  0.9980292793  0.9627959414  0.9968445209  0.9265212400  39.546858908  2400          1.3711204513 
0.0198202305  0.0162351863  1.0665793745  0.9963954686  0.9051546392  0.7594501718  0.7296678121  0.9969031532  0.9627959414  0.9974182444  0.9207807118  44.490216271  2700          1.4213991539 
0.0180286559  1.0812159534  0.9963954686  0.8927835052  0.7517182131  0.7479954181  0.9980292793  0.9661781285  0.9974182444  0.9253731343  44.490216271  2700          1.3683917220 
0.1027341084  0.1220328730  0.7563898221  0.9891864058  0.8927835052  0.9438717068  0.8499427262  0.9290540541  0.9165727170  0.9847963282  0.9334098737  9.8867147271  600           1.4141101400 
0.1007563043  0.1151108654  0.7299222441  0.9886714727  0.9010309278  0.9427262314  0.8465063001  0.9245495495  0.9222096956  0.9862306368  0.9242250287  9.8867147271  600           1.4144724941 
0.0161442329  1.0767831190  0.9963954686  0.9113402062  0.7551546392  0.7388316151  0.9983108108  0.9740698985  0.9971313827  0.9322617681  44.490216271  2700          1.3638610101 
0.0166249314  1.0803778283  0.9963954686  0.9010309278  0.7442726231  0.7353951890  0.9980292793  0.9639233371  0.9971313827  0.9253731343  44.490216271  2700          1.3624804481 
0.0735098686  0.0771806176  0.8669387809  0.9933058702  0.9010309278  0.9587628866  0.8625429553  0.9279279279  0.9188275085  0.9899598394  0.9288174512  14.830072090  900           1.4144440087 
0.0658297172  0.0751028317  0.8732249610  0.9917610711  0.9072164948  0.9581901489  0.8625429553  0.9234234234  0.9177001127  0.9899598394  0.9219288175  14.830072090  900           1.4158762264 
0.0147104438  0.0124684499  1.0638852493  0.9963954686  0.9030927835  0.7585910653  0.7319587629  0.9971846847  0.9639233371  0.9974182444  0.9207807118  49.433573635  3000          1.4359424949 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0145063400  1.0806374343  0.9963954686  0.9092783505  0.7545819015  0.7365406644  0.9983108108  0.9740698985  0.9971313827  0.9334098737  49.433573635  3000          1.4394566425 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.8387033343  1.1788179874  0.8290422245  0.8391752577  0.7041809851  0.6907216495  0.8997747748  0.8804960541  0.8964429145  0.9047072331  0.0000000000  0             15.748039960 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
1.0778923035  0.8228257298  1.1800526381  0.8336766220  0.8371134021  0.7001718213  0.6964490263  0.8994932432  0.8816234498  0.8973034997  0.9070034443  0.0000000000  0             12.927984952 
0.0585893175  0.0626439126  0.9850643400  0.9933058702  0.9072164948  0.9702176403  0.8682703322  0.9228603604  0.9131905299  0.9925415950  0.9219288175  19.773429454  1200          1.4169161773 
0.0515544616  0.0623002338  0.9714090945  0.9938208033  0.8927835052  0.9682130584  0.8602520046  0.9265202703  0.9177001127  0.9934021801  0.9253731343  19.773429454  1200          1.4152976171 
0.0146849402  1.0812188033  0.9963954686  0.9010309278  0.7422680412  0.7342497136  0.9980292793  0.9627959414  0.9971313827  0.9242250287  49.433573635  3000          1.3639303184 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.8461599946  1.1779503822  0.8316168898  0.8288659794  0.7024627721  0.6953035510  0.9006193694  0.8951521984  0.8970166380  0.9150401837  0.0000000000  0             12.306239366 
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.5579336286  1.2031840086  0.8357363543  0.8329896907  0.6984536082  0.6941580756  0.8997747748  0.8850056370  0.8987378084  0.9070034443  0.0000000000  0             5.2313177586 
0.2263425556  0.3110389993  0.4381882415  0.9737384140  0.8907216495  0.9123711340  0.8293241695  0.9282094595  0.9222096956  0.9718875502  0.9207807118  4.9433573635  300           1.4214491693 
0.0414908404  0.0499828235  1.0207928741  0.9963954686  0.8989690722  0.9753722795  0.8659793814  0.9251126126  0.9199549042  0.9942627653  0.9230769231  24.716786817  1500          1.4172512960 
0.2339762922  0.5565854452  0.9866117405  0.8865979381  0.9344215349  0.8465063001  0.9268018018  0.9278466742  0.9825014343  0.9265212400  4.9433573635  300           1.5304410521 
0.2280396922  0.5581113777  0.9866117405  0.8927835052  0.9344215349  0.8442153494  0.9290540541  0.9210822999  0.9822145726  0.9345579793  4.9433573635  300           1.4372394085 
0.2307670299  0.5760617831  0.9871266735  0.8907216495  0.9306987400  0.8304696449  0.9276463964  0.9210822999  0.9813539874  0.9265212400  4.9433573635  300           1.3741629831 
0.0363921251  0.0436560490  1.0510030673  0.9953656025  0.9113402062  0.9808132875  0.8682703322  0.9175112613  0.9131905299  0.9951233505  0.9230769231  29.660144181  1800          1.4179095928 
0.0390586247  0.0443245208  1.0323565735  0.9963954686  0.9051546392  0.9799541810  0.8648339061  0.9242680180  0.9199549042  0.9948364888  0.9253731343  29.660144181  1800          1.4170763930 
0.0834370363  0.9041436615  0.9948506694  0.8948453608  0.9639175258  0.8579610538  0.9276463964  0.9267192785  0.9925415950  0.9265212400  9.8867147271  600           1.3608486541 
0.1017395624  0.1141980739  0.7326791338  0.9891864058  0.8907216495  0.9487399771  0.8476517755  0.9279279279  0.9210822999  0.9868043603  0.9276693456  9.8867147271  600           1.4238124768 
0.0850165198  0.8954983185  0.9938208033  0.8989690722  0.9636311569  0.8591065292  0.9262387387  0.9222096956  0.9908204246  0.9345579793  9.8867147271  600           1.3593707124 
0.0828283975  0.8950746467  0.9943357364  0.8989690722  0.9636311569  0.8556701031  0.9262387387  0.9244644870  0.9913941480  0.9288174512  9.8867147271  600           1.3589516632 
0.0254833110  0.0376352821  1.0531734534  0.9958805355  0.9092783505  0.9825315006  0.8705612829  0.9169481982  0.9120631342  0.9954102123  0.9230769231  34.603501544  2100          1.4309322262 
0.0307494224  0.0336073060  1.0516124268  0.9963954686  0.9051546392  0.9822451317  0.8705612829  0.9239864865  0.9177001127  0.9959839357  0.9230769231  34.603501544  2100          1.4305076408 
0.0575880482  1.0118845612  0.9958805355  0.8989690722  0.9762313860  0.8659793814  0.9248310811  0.9244644870  0.9939759036  0.9230769231  14.830072090  900           1.3529353261 
0.0725843397  0.0812065129  0.8682859580  0.9927909372  0.8948453608  0.9650630011  0.8648339061  0.9265202703  0.9154453213  0.9899598394  0.9311136625  14.830072090  900           1.4223888079 
0.0615938518  0.9991947969  0.9948506694  0.9030927835  0.9765177549  0.8613974800  0.9208896396  0.9278466742  0.9936890419  0.9288174512  14.830072090  900           1.3609060152 
1.4559545430 
0.0275898138  0.0339587985  1.0617804857  0.9958805355  0.9092783505  0.9842497136  0.8705612829  0.9163851351  0.9086809470  0.9956970740  0.9230769231  39.546858908  2400          1.4602760379 
0.0494295147  1.0449619669  0.9958805355  0.8989690722  0.9813860252  0.8705612829  0.9222972973  0.9199549042  0.9951233505  0.9299655568  19.773429454  1200          1.3556195998 
0.0499872894  1.0484265433  0.9963954686  0.9092783505  0.9822451317  0.8625429553  0.9228603604  0.9222096956  0.9954102123  0.9322617681  19.773429454  1200          1.3584838430 
1.4246759375 
0.0201796604  0.0270099029  1.0655688685  0.9963954686  0.9030927835  0.9859679267  0.8671248568  0.9220157658  0.9199549042  0.9965576592  0.9219288175  44.490216271  2700          1.4781166442 
0.0203422343  0.0266802174  1.0653438534  0.9963954686  0.9072164948  0.9848224513  0.8671248568  0.9158220721  0.9120631342  0.9962707975  0.9207807118  44.490216271  2700          1.4885223214 
0.0485156434  1.0464740090  0.9953656025  0.9072164948  0.9822451317  0.8636884307  0.9172297297  0.9255918828  0.9945496271  0.9299655568  19.773429454  1200          1.3738619407 
0.0413003602  1.0650734474  0.9963954686  0.9010309278  0.9839633448  0.8728522337  0.9192004505  0.9210822999  0.9959839357  0.9311136625  24.716786817  1500          1.3571146321 
0.0424211419  1.0611458623  0.9963954686  0.9092783505  0.9836769759  0.8659793814  0.9217342342  0.9222096956  0.9959839357  0.9334098737  24.716786817  1500          1.3575112414 
0.0448781806  0.0534149269  1.0071726535  0.9953656025  0.9030927835  0.9802405498  0.8694158076  0.9225788288  0.9165727170  0.9939759036  0.9276693456  24.716786817  1500          1.4232059375 
0.0192077977  0.0264375366  1.0615604838  0.9963954686  0.9010309278  0.9859679267  0.8636884307  0.9203265766  0.9188275085  0.9965576592  0.9207807118  49.433573635  3000          1.4717631952 
0.0209666324  0.0326580508  1.0673535138  0.9963954686  0.9113402062  0.9862542955  0.8659793814  0.9130067568  0.9086809470  0.9968445209  0.9196326062  49.433573635  3000          1.4775070548 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0435786859  1.0641517429  0.9958805355  0.9072164948  0.9839633448  0.8625429553  0.9183558559  0.9244644870  0.9954102123  0.9288174512  24.716786817  1500          1.3596418834 
step_time    
1.2591917515  0.9999723434  1.2000331879  0.8403707518  0.8371134021  0.7073310424  0.7033218786  0.9039977477  0.8906426156  0.8973034997  0.9150401837  0.0000000000  0             9.8375892639 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.8506355286  0.5790019631  1.2755080462  0.8331616890  0.8350515464  0.6987399771  0.6872852234  0.8969594595  0.8850056370  0.8952954676  0.9092996556  0.0000000000  0             2.5234758854 
0.0377740095  1.0718099858  0.9963954686  0.9051546392  0.9853951890  0.8728522337  0.9163851351  0.9210822999  0.9965576592  0.9299655568  29.660144181  1800          1.3537262654 
0.0345246555  1.0736006763  0.9963954686  0.9092783505  0.9851088202  0.8613974800  0.9183558559  0.9154453213  0.9965576592  0.9276693456  29.660144181  1800          1.3918017332 
0.0332247667  0.0406640942  1.0422665652  0.9953656025  0.9030927835  0.9810996564  0.8717067583  0.9217342342  0.9143179256  0.9945496271  0.9276693456  29.660144181  1800          1.4779078023 
0.2511676896  0.3227403092  0.3199635509  0.9716786818  0.8845360825  0.9066437572  0.8258877434  0.9749436937  0.9447576099  0.9248422260  0.9242250287  4.9433573635  300           1.4130338907 
0.2437954760  0.3097062034  0.3434585891  0.9752832132  0.8886597938  0.9092210767  0.8281786942  0.9743806306  0.9447576099  0.9225473322  0.9288174512  4.9433573635  300           1.4124732629 
0.0364531807  1.0707213320  0.9963954686  0.9051546392  0.9845360825  0.8648339061  0.9175112613  0.9233370913  0.9959839357  0.9265212400  29.660144181  1800          1.3573409446 
0.0318055726  1.0738967633  0.9963954686  0.9051546392  0.9856815578  0.8705612829  0.9121621622  0.9199549042  0.9968445209  0.9253731343  34.603501544  2100          1.4878916097 
0.0284115694  1.0779754597  0.9963954686  0.9134020619  0.9856815578  0.8602520046  0.9172297297  0.9120631342  0.9965576592  0.9253731343  34.603501544  2100          1.3569793161 
0.1076946849  0.1188478349  0.6229672467  0.9876416066  0.9030927835  0.9453035510  0.8465063001  0.9853603604  0.9526493799  0.9248422260  0.9265212400  9.8867147271  600           1.4136701973 
0.1108077836  0.1191300639  0.6537220974  0.9871266735  0.8886597938  0.9447308133  0.8442153494  0.9856418919  0.9594137542  0.9259896730  0.9253731343  9.8867147271  600           1.4140336418 
0.0306507215  0.0368360973  1.0471481723  0.9958805355  0.9030927835  0.9831042383  0.8728522337  0.9214527027  0.9143179256  0.9954102123  0.9265212400  34.603501544  2100          1.4204530541 
0.0317441865  1.0749405901  0.9963954686  0.9092783505  0.9862542955  0.8671248568  0.9180743243  0.9222096956  0.9965576592  0.9253731343  34.603501544  2100          1.3552640899 
0.0303991518  1.0784915270  0.9963954686  0.9051546392  0.9859679267  0.8682703322  0.9138513514  0.9210822999  0.9968445209  0.9219288175  39.546858908  2400          1.3509554068 
0.0266994980  1.0803722423  0.9958805355  0.9092783505  0.9862542955  0.8613974800  0.9146959459  0.9131905299  0.9974182444  0.9242250287  39.546858908  2400          1.3533989716 
0.0844444169  0.0839355245  0.8248124609  0.9917610711  0.8989690722  0.9601947308  0.8556701031  0.9895833333  0.9549041714  0.9254159495  0.9265212400  14.830072090  900           1.4149871842 
0.0881928935  0.0960845840  0.7997379708  0.9922760041  0.8824742268  0.9607674685  0.8556701031  0.9907094595  0.9639233371  0.9245553643  0.9253731343  14.830072090  900           1.4147636437 
0.0230109169  0.0329038395  1.0619816945  0.9963954686  0.8989690722  0.9842497136  0.8774341352  0.9211711712  0.9131905299  0.9956970740  0.9288174512  39.546858908  2400          1.4227848005 
0.0257174472  1.0827803739  0.9963954686  0.9072164948  0.9871134021  0.8705612829  0.9130067568  0.9210822999  0.9968445209  0.9207807118  44.490216271  2700          1.3512193418 
0.0242931149  1.0797569215  0.9958805355  0.9092783505  0.9868270332  0.8602520046  0.9144144144  0.9120631342  0.9974182444  0.9230769231  44.490216271  2700          1.3546109494 
1.4138500206 
0.0638613035  0.0626928593  0.9307443454  0.9943357364  0.8804123711  0.9696449026  0.8636884307  0.9935247748  0.9639233371  0.9202524383  0.9265212400  19.773429454  1200          1.4145140139 
0.0252229041  1.0783536561  0.9963954686  0.9113402062  0.9876861397  0.8636884307  0.9183558559  0.9222096956  0.9971313827  0.9219288175  44.490216271  2700          1.3598044387 
0.0210302943  0.0289520146  1.0666375260  0.9963954686  0.8989690722  0.9853951890  0.8774341352  0.9189189189  0.9165727170  0.9962707975  0.9276693456  44.490216271  2700          1.4217790445 
0.0223529270  1.0828818508  0.9963954686  0.9092783505  0.9873997709  0.8717067583  0.9135698198  0.9188275085  0.9974182444  0.9219288175  49.433573635  3000          1.3537261502 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0640535121  0.0613163554  0.9884110542  0.9958805355  0.9051546392  0.9773768614  0.8625429553  0.9943693694  0.9537767756  0.9216867470  0.9196326062  24.716786817  1500          1.4141009823 
0.0568507825  0.0530188523  0.9970854338  0.9948506694  0.8804123711  0.9776632302  0.8671248568  0.9940878378  0.9616685457  0.9208261618  0.9265212400  24.716786817  1500          1.4139508231 
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.9624989629  1.2486757040  0.8285272915  0.8391752577  0.7027491409  0.6895761741  0.9073761261  0.8906426156  0.8975903614  0.9104477612  0.0000000000  0             3.0588295460 
0.0251196907  1.0796959384  0.9963954686  0.9113402062  0.9876861397  0.8613974800  0.9177927928  0.9233370913  0.9971313827  0.9219288175  49.433573635  3000          1.3615177544 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4827093780  1.2673333883  0.8321318229  0.8391752577  0.7004581901  0.6895761741  0.8978040541  0.8804960541  0.8970166380  0.9104477612  0.0000000000  0             2.6968383789 
0.0231042343  1.0793713868  0.9958805355  0.9113402062  0.9876861397  0.8602520046  0.9146959459  0.9120631342  0.9974182444  0.9219288175  49.433573635  3000          2.6838605237 
1.4230668863 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16

0.0446149967  0.0472022925  1.0278979905  0.9958805355  0.9030927835  0.9819587629  0.8694158076  0.9946509009  0.9549041714  0.9202524383  0.9173363949  29.660144181  1800          1.4136056582 
0.0365779975  0.0388121691  1.0353213787  0.9953656025  0.8845360825  0.9810996564  0.8694158076  0.9952139640  0.9594137542  0.9199655766  0.9242250287  29.660144181  1800          1.4140054075 
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.8556661606  1.2068600655  0.8331616890  0.8268041237  0.7001718213  0.6849942726  0.9025900901  0.8895152198  0.8990246701  0.9161882893  0.0000000000  0             4.9619777203 
0.2326911562  0.4541352727  0.9850669413  0.8886597938  0.9352806415  0.8396334479  0.9859234234  0.9503945885  0.9259896730  0.9299655568  4.9433573635  300           1.3794407153 
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1058, in update
    self.ema.update()
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1130, in update
    self.shadow[name] = new_average.clone()
                        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1882141) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
1.0450415611  0.9256698489  1.2563272715  0.8362512873  0.8412371134  0.7007445590  0.6941580756  0.9039977477  0.8883878241  0.8975903614  0.9070034443  0.0000000000  0             5.2138390541 
0.2325084795  0.4896897759  0.9871266735  0.8865979381  0.9304123711  0.8350515464  0.9828265766  0.9447576099  0.9259896730  0.9219288175  4.9433573635  300           1.3585177430 
0.0377342027  0.0404426041  1.0420417056  0.9958805355  0.8989690722  0.9836769759  0.8682703322  0.9960585586  0.9560315671  0.9179575445  0.9150401837  34.603501544  2100          1.4127173781 
0.0311240029  0.0319079836  1.0506098938  0.9958805355  0.8824742268  0.9831042383  0.8682703322  0.9963400901  0.9571589628  0.9188181297  0.9219288175  34.603501544  2100          1.4125938034 
0.2536124299  0.3228927174  0.3059412706  0.9727085479  0.8845360825  0.9092210767  0.8407789233  0.9771959459  0.9470124014  0.9265633964  0.9253731343  4.9433573635  300           1.4207963165 
0.0881238698  0.8385977644  0.9938208033  0.8865979381  0.9619129439  0.8648339061  0.9921171171  0.9503945885  0.9254159495  0.9276693456  9.8867147271  600           1.3490189131 
0.0273966231  0.0377489587  1.0542205242  0.9958805355  0.8969072165  0.9853951890  0.8625429553  0.9963400901  0.9582863585  0.9165232358  0.9127439724  39.546858908  2400          1.4129330730 
0.0351033557  0.0351109449  1.0455994286  0.9958805355  0.8804123711  0.9845360825  0.8694158076  0.9963400901  0.9605411499  0.9196787149  0.9173363949  39.546858908  2400          1.4134076254 
0.2352448761  0.4900932022  0.9876416066  0.8865979381  0.9321305842  0.8350515464  0.9833896396  0.9481397971  0.9236947791  0.9230769231  4.9433573635  300           3.0986503601 
0.1162854644  0.1255067995  0.6712051324  0.9860968074  0.8969072165  0.9410080183  0.8510882016  0.9864864865  0.9560315671  0.9245553643  0.9230769231  9.8867147271  600           1.4210772427 
0.0265280837  0.0315764202  1.0502407859  0.9958805355  0.8865979381  0.9856815578  0.8671248568  0.9963400901  0.9594137542  0.9193918531  0.9150401837  44.490216271  2700          1.4146808894 
0.0300724477  0.0303280240  1.0539161936  0.9963954686  0.8948453608  0.9862542955  0.8636884307  0.9966216216  0.9582863585  0.9150889271  0.9138920781  44.490216271  2700          1.4153211220 
0.0626851035  0.9806029852  0.9953656025  0.9010309278  0.9742268041  0.8625429553  0.9952139640  0.9560315671  0.9248422260  0.9288174512  14.830072090  900           1.3517624068 
0.0757210790  0.0804045661  0.8445414380  0.9922760041  0.9030927835  0.9584765178  0.8613974800  0.9907094595  0.9605411499  0.9234079174  0.9242250287  14.830072090  900           1.4204638704 
0.0207244351  0.0257748928  1.0550851138  0.9958805355  0.8886597938  0.9873997709  0.8682703322  0.9966216216  0.9582863585  0.9168100975  0.9173363949  49.433573635  3000          1.4149159726 
0.0472015402  1.0340452298  0.9963954686  0.9092783505  0.9810996564  0.8671248568  0.9960585586  0.9582863585  0.9248422260  0.9288174512  19.773429454  1200          1.3518794513 
0.0836353829  0.8455795306  0.9948506694  0.8948453608  0.9633447881  0.8510882016  0.9929617117  0.9537767756  0.9254159495  0.9299655568  9.8867147271  600           3.1256132714 
0.0386094325  1.0567014255  0.9963954686  0.9092783505  0.9825315006  0.8728522337  0.9963400901  0.9639233371  0.9216867470  0.9265212400  24.716786817  1500          1.3482105184 
0.0668977648  0.0640261178  0.9443585422  0.9943357364  0.9010309278  0.9693585338  0.8682703322  0.9921171171  0.9650507328  0.9251290878  0.9322617681  19.773429454  1200          1.4230218641 
0.0346274010  1.0659594379  0.9963954686  0.9072164948  0.9853951890  0.8671248568  0.9971846847  0.9616685457  0.9219736087  0.9242250287  29.660144181  1800          1.3488907496 
0.0523948677  0.0557345505  0.9974799448  0.9943357364  0.8989690722  0.9762313860  0.8728522337  0.9940878378  0.9627959414  0.9245553643  0.9276693456  24.716786817  1500          1.4247141512 
0.0637060994  0.9830438993  0.9958805355  0.8969072165  0.9747995418  0.8602520046  0.9952139640  0.9560315671  0.9236947791  0.9322617681  14.830072090  900           3.1228727857 
0.0312195757  1.0724421726  0.9963954686  0.9072164948  0.9859679267  0.8694158076  0.9974662162  0.9616685457  0.9213998853  0.9253731343  34.603501544  2100          1.3497284031 
0.0396854684  0.0440964970  1.0379505030  0.9948506694  0.8969072165  0.9802405498  0.8694158076  0.9943693694  0.9605411499  0.9208261618  0.9276693456  29.660144181  1800          1.4212288690 
0.0282693024  1.0719891888  0.9963954686  0.9051546392  0.9868270332  0.8659793814  0.9974662162  0.9594137542  0.9188181297  0.9276693456  39.546858908  2400          1.3492115402 
0.0516702937  1.0379346269  0.9963954686  0.8989690722  0.9785223368  0.8625429553  0.9963400901  0.9571589628  0.9219736087  0.9276693456  19.773429454  1200          3.1324220117 
0.0288083623  1.0813677557  0.9963954686  0.9072164948  0.9873997709  0.8625429553  0.9974662162  0.9627959414  0.9185312679  0.9265212400  44.490216271  2700          1.3478599087 
0.0376469017  0.0428367144  1.0465102567  0.9953656025  0.8948453608  0.9816723940  0.8694158076  0.9949324324  0.9594137542  0.9185312679  0.9230769231  34.603501544  2100          1.4208247884 
0.0225269553  1.0786969008  0.9963954686  0.9051546392  0.9876861397  0.8636884307  0.9977477477  0.9639233371  0.9182444062  0.9288174512  49.433573635  3000          1.3448324021 
0.0306294733  0.0319454484  1.0515844200  0.9953656025  0.8927835052  0.9825315006  0.8751431844  0.9954954955  0.9582863585  0.9173838210  0.9184845006  39.546858908  2400          1.4230652038 
0.0424171276  1.0595048424  0.9963954686  0.8989690722  0.9816723940  0.8602520046  0.9969031532  0.9605411499  0.9185312679  0.9288174512  24.716786817  1500          3.1403687580 
0.0280814938  0.0321183439  1.0573640778  0.9958805355  0.8969072165  0.9845360825  0.8717067583  0.9960585586  0.9571589628  0.9182444062  0.9173363949  44.490216271  2700          1.4376309339 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0355199723  1.0658987127  0.9963954686  0.9030927835  0.9831042383  0.8613974800  0.9971846847  0.9627959414  0.9173838210  0.9299655568  29.660144181  1800          3.1470608147 
1.4245086233 
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.9624991417  1.2486758232  0.8285272915  0.8391752577  0.7027491409  0.6895761741  0.9073761261  0.8906426156  0.8975903614  0.9104477612  0.0000000000  0             2.6279809475 
0.2325256351  0.4490306189  0.9845520082  0.8824742268  0.9329896907  0.8373424971  0.9853603604  0.9515219842  0.9259896730  0.9299655568  4.9433573635  300           1.3507750122 
0.0878432161  0.8464402779  0.9953656025  0.8907216495  0.9639175258  0.8602520046  0.9921171171  0.9515219842  0.9279977051  0.9334098737  9.8867147271  600           1.3499953651 
0.0313744033  1.0694952863  0.9963954686  0.8969072165  0.9856815578  0.8602520046  0.9971846847  0.9605411499  0.9179575445  0.9288174512  34.603501544  2100          3.1204813552 
0.0606360360  0.9778379790  0.9963954686  0.8907216495  0.9765177549  0.8694158076  0.9946509009  0.9594137542  0.9279977051  0.9311136625  14.830072090  900           1.3539300919 
0.0478325255  1.0353040204  0.9963954686  0.8927835052  0.9816723940  0.8717067583  0.9966216216  0.9627959414  0.9248422260  0.9334098737  19.773429454  1200          1.3541484793 
0.0275325607  1.0712910209  0.9963954686  0.9010309278  0.9862542955  0.8648339061  0.9971846847  0.9605411499  0.9153757889  0.9288174512  39.546858908  2400          3.1290198000 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1019, in update
    image_features_target = self.featurizer(all_x_anchor, all_index, mask=False)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 290, in forward
    x = self.conv1(x)  # shape = [*, width, grid, grid]
        ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0408314932  1.0550220831  0.9963954686  0.8989690722  0.9833906071  0.8717067583  0.9966216216  0.9605411499  0.9231210557  0.9311136625  24.716786817  1500          1.3539716236 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1019, in update
    image_features_target = self.featurizer(all_x_anchor, all_index, mask=False)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 290, in forward
    x = self.conv1(x)  # shape = [*, width, grid, grid]
        ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 300, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 204, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 192, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 39.59 GiB total capacity; 25.08 GiB already allocated; 22.62 MiB free; 25.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1019, in update
    image_features_target = self.featurizer(all_x_anchor, all_index, mask=False)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 290, in forward
    x = self.conv1(x)  # shape = [*, width, grid, grid]
        ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 300, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 204, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 192, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 39.59 GiB total capacity; 25.08 GiB already allocated; 22.62 MiB free; 25.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1019, in update
    image_features_target = self.featurizer(all_x_anchor, all_index, mask=False)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 290, in forward
    x = self.conv1(x)  # shape = [*, width, grid, grid]
        ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 183, in <module>
    algorithm_class = algorithms.get_algorithm_class(args.algorithm)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 59, in get_algorithm_class
    raise NotImplementedError("Algorithm not found: {}".format(algorithm_name))
NotImplementedError: Algorithm not found: CMSANwoMatch
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 183, in <module>
    algorithm_class = algorithms.get_algorithm_class(args.algorithm)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 59, in get_algorithm_class
    raise NotImplementedError("Algorithm not found: {}".format(algorithm_name))
NotImplementedError: Algorithm not found: CMSANwoMatch
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1019, in update
    image_features_target = self.featurizer(all_x_anchor, all_index, mask=False)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 290, in forward
    x = self.conv1(x)  # shape = [*, width, grid, grid]
        ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.6412347555  1.1438646317  0.9982332155  1.0000000000  0.6847313855  0.6981132075  0.7703731912  0.7500000000  0.8626434654  0.8814814815  0.0000000000  0             8.9504358768 
 File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 300, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 204, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 192, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 39.59 GiB total capacity; 25.08 GiB already allocated; 22.62 MiB free; 25.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1019, in update
    image_features_target = self.featurizer(all_x_anchor, all_index, mask=False)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 290, in forward
    x = self.conv1(x)  # shape = [*, width, grid, grid]
        ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 300, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 204, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 192, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 39.59 GiB total capacity; 25.08 GiB already allocated; 22.62 MiB free; 25.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1019, in update
    image_features_target = self.featurizer(all_x_anchor, all_index, mask=False)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 290, in forward
    x = self.conv1(x)  # shape = [*, width, grid, grid]
        ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1019, in update
    image_features_target = self.featurizer(all_x_anchor, all_index, mask=False)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 290, in forward
    x = self.conv1(x)  # shape = [*, width, grid, grid]
        ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 463, in forward
    return self._conv_forward(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/conv.py", line 459, in _conv_forward
    return F.conv2d(input, weight, bias, self.stride,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 300, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 204, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 192, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 39.59 GiB total capacity; 25.08 GiB already allocated; 22.62 MiB free; 25.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 300, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 204, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 192, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 190.00 MiB (GPU 0; 39.59 GiB total capacity; 25.08 GiB already allocated; 22.62 MiB free; 25.31 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.7075567842  1.0771906376  0.9982332155  1.0000000000  0.6239396795  0.6113207547  0.7901751714  0.7637195122  0.8493150685  0.8711111111  0.0000000000  0             8.7776582241 
0.0348592145  1.0642380089  0.9963954686  0.8989690722  0.9851088202  0.8705612829  0.9971846847  0.9616685457  0.9208261618  0.9276693456  29.660144181  1800          1.3520012506 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.1660444276  0.3875349194  0.9946996466  0.9964664311  0.9491046183  0.8150943396  0.9748667174  0.8765243902  0.9822288041  0.9392592593  8.4805653710  300           1.3457972288 
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.6791520715  1.0443742275  0.9973498233  1.0000000000  0.7200754006  0.7132075472  0.7281035796  0.7118902439  0.8759718623  0.8888888889  0.0000000000  0             12.593783140 
0.0263424568  1.0760924248  0.9963954686  0.8969072165  0.9865406644  0.8659793814  0.9974662162  0.9605411499  0.9139414802  0.9288174512  44.490216271  2700          3.1234192745 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/connection.py", line 930, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1928128) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/queues.py", line 113, in get
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/connection.py", line 256, in poll
    return self._poll(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/connection.py", line 423, in _poll
    r = wait([self], timeout)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/connection.py", line 922, in wait
    with _WaitSelector() as selector:
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/selectors.py", line 202, in __exit__
    def __exit__(self, *args):

  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1929538) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1133, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/queues.py", line 120, in get
    self._rlock.release()
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1930050) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 634, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1329, in _next_data
    idx, data = self._get_data()
                ^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1295, in _get_data
    success, data = self._try_get_data()
                    ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1140, in _try_get_data
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    for worker_id, w in enumerate(self._workers):
                        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
    step_vals = algorithm.update(minibatches_device, uda_device)RuntimeError
: DataLoader worker (pid 1925190) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
    File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/misc.py", line 115, in accuracy
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    for x, _, y, z in loader:
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/fast_data_loader.py", line 70, in __iter__
    yield next(self._infinite_iterator)
          ^^^^^^^^^^^^    ^image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)^
^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 630, in __next__
    with torch.autograd.profiler.record_function(self._profile_name):
    File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/autograd/profiler.py", line 495, in __exit__
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any):

  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1927238) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 253, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/misc.py", line 114, in accuracy
    return forward_call(*args, **kwargs)
    with torch.no_grad():
    File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/autograd/grad_mode.py", line 57, in __exit__
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 297, in forward
    def __exit__(self, exc_type: Any, exc_value: Any, traceback: Any) -> None:

  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1928451) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
    x = self.remove_patches(x, z)
        ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 255, in remove_patches
    mask_idx = torch.where(mask[i]==False)[0] 
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1910146) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1020, in update
    image_features_anchor = self.featurizer(all_x_anchor, all_index, mask=True)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 297, in forward
    x = self.remove_patches(x, z)
        ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/clip/model.py", line 255, in remove_patches
    mask_idx = torch.where(mask[i]==False)[0] 
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1897058) is killed by signal: Bus error. It is possible that dataloader's workers are out of shared memory. Please try to raise your shared memory limit.
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 183, in <module>
    algorithm_class = algorithms.get_algorithm_class(args.algorithm)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 59, in get_algorithm_class
    raise NotImplementedError("Algorithm not found: {}".format(algorithm_name))
NotImplementedError: Algorithm not found: CMSANwoMatch
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4475048780  1.1632546186  0.9982332155  1.0000000000  0.6611687088  0.6603773585  0.7932216299  0.7728658537  0.8533876342  0.8696296296  0.0000000000  0             2.1249637604 
int: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 183, in <module>
    algorithm_class = algorithms.get_algorithm_class(args.algorithm)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 59, in get_algorithm_class
    raise NotImplementedError("Algorithm not found: {}".format(algorithm_name))
NotImplementedError: Algorithm not found: CMSANwoMatch
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 183, in <module>
    algorithm_class = algorithms.get_algorithm_class(args.algorithm)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 59, in get_algorithm_class
    raise NotImplementedError("Algorithm not found: {}".format(algorithm_name))
NotImplementedError: Algorithm not found: CMSANwoMatch
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 183, in <module>
    algorithm_class = algorithms.get_algorithm_class(args.algorithm)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 59, in get_algorithm_class
    raise NotImplementedError("Algorithm not found: {}".format(algorithm_name))
NotImplementedError: Algorithm not found: CMSANwoMatch
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0542809786  0.8335895130  0.9849823322  0.9929328622  0.9844486334  0.8132075472  0.9927646611  0.8780487805  0.9955572010  0.9348148148  16.961130742  600           1.3450988094 
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.6791517138  1.0443742275  0.9973498233  1.0000000000  0.7200754006  0.7132075472  0.7281035796  0.7118902439  0.8759718623  0.8888888889  0.0000000000  0             2.9374568462 
0.0491694623  0.8061737189  0.9876325088  1.0000000000  0.9830348728  0.8094339623  0.9942878903  0.8841463415  0.9955572010  0.9407407407  16.961130742  600           1.3455402772 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       0.0373417332  0.9715945925  0.9787985866  0.9787985866  0.9957587182  0.8132075472  0.9977151561  0.8780487805  0.9981488338  0.9392592593  25.441696113  900           1.3437889878 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  0.1624393798  0.3704638825  0.9893992933  1.0000000000  0.9575871819  0.8264150943  0.9775323686  0.8826219512  0.9844502036  0.9377777778  8.4805653710  300           1.3435403903 
0.0364038599  0.9682879126  0.9805653710  0.9858657244  0.9957587182  0.8188679245  0.9977151561  0.8780487805  0.9988893003  0.9422222222  25.441696113  900           1.3437351449 
                                                                                                                                                                                      0.0284767967  1.0002972315  0.9761484099  0.9752650177  0.9985862394  0.8150943396  0.9988575781  0.8795731707  0.9996297668  0.9392592593  33.922261484  1200          1.3436311356 
0.0517916736  0.7803113947  0.9805653710  0.9858657244  0.9868049010  0.8283018868  0.9912414318  0.8871951220  0.9977786005  0.9362962963  16.961130742  600           1.3451542266 
0.0272979420  0.9993632428  0.9743816254  0.9787985866  0.9985862394  0.8113207547  0.9984767708  0.8795731707  0.9992595335  0.9333333333  33.922261484  1200          1.3462508225 
0.0567270332  0.7934853033  0.9867491166  0.9893992933  0.9863336475  0.8094339623  0.9900990099  0.8810975610  0.9974083673  0.9362962963  16.961130742  600           1.5777233561 
0.1632441412  0.3854018215  0.9964664311  1.0000000000  0.9571159284  0.8037735849  0.9813404417  0.8795731707  0.9833395039  0.9377777778  8.4805653710  300           1.5755468758 
0.0346040365  0.8599979181  1.0000000000  1.0000000000  0.6899151744  0.6886792453  0.9935262757  0.8871951220  0.9974083673  0.9377777778  16.961130742  600           1.3514022144 
0.0237107406  1.0071087579  0.9726148410  0.9752650177  0.9990574929  0.8094339623  0.9992383854  0.8826219512  1.0000000000  0.9422222222  42.402826855  1500          1.3426154129 
0.0365710722  0.9662065955  0.9770318021  0.9858657244  0.9962299717  0.8169811321  0.9954303123  0.8887195122  0.9988893003  0.9318518519  25.441696113  900           1.3443415324 
0.0220450064  1.0071972070  0.9708480565  0.9752650177  0.9985862394  0.8094339623  0.9988575781  0.8810975610  0.9996297668  0.9288888889  42.402826855  1500          1.3435699789 
0.0506604022  0.8161913707  0.9849823322  0.9929328622  0.9820923657  0.8037735849  0.9939070830  0.8750000000  0.9966679008  0.9348148148  16.961130742  600           1.3734760404 
0.0394481245  0.9708365862  0.9805653710  0.9823321555  0.9943449576  0.8075471698  0.9954303123  0.8780487805  0.9985190670  0.9318518519  25.441696113  900           1.5417595752 
0.0278103274  1.0204384180  1.0000000000  1.0000000000  0.6908576814  0.6886792453  0.9961919269  0.8917682927  0.9981488338  0.9422222222  25.441696113  900           1.3523052176 
0.0212000799  1.0091593671  0.9708480565  0.9752650177  0.9995287465  0.8094339623  0.9996191927  0.8795731707  1.0000000000  0.9466666667  50.883392226  1800          1.3436390471 
0.0272711107  1.0017943648  0.9717314488  0.9752650177  0.9981149859  0.8094339623  0.9977151561  0.8810975610  0.9992595335  0.9333333333  33.922261484  1200          1.3450173561 
0.0210896375  1.0080329708  0.9690812721  0.9717314488  1.0000000000  0.8150943396  1.0000000000  0.8810975610  0.9996297668  0.9288888889  50.883392226  1800          1.3441811005 
0.0325765550  0.9608909130  0.9743816254  0.9858657244  0.9924599434  0.8056603774  0.9965727342  0.8841463415  0.9996297668  0.9348148148  25.441696113  900           1.3584986663 
0.0209893364  1.0323210518  1.0000000000  1.0000000000  0.6889726673  0.6849056604  0.9980959634  0.8902439024  0.9996297668  0.9466666667  33.922261484  1200          1.3515099859 
0.0278358479  1.0021685100  0.9699646643  0.9752650177  0.9976437323  0.7981132075  0.9984767708  0.8810975610  0.9988893003  0.9288888889  33.922261484  1200          1.5613842694 
0.0235982349  1.0213308142  0.9699646643  0.9752650177  1.0000000000  0.8113207547  1.0000000000  0.8826219512  1.0000000000  0.9392592593  59.363957597  2100          1.3449956711 
0.0242634706  1.0115252280  0.9681978799  0.9752650177  0.9990574929  0.8132075472  0.9988575781  0.8826219512  0.9996297668  0.9318518519  42.402826855  1500          1.3450388392 
0.0211732617  1.0132353510  0.9673144876  0.9717314488  1.0000000000  0.8188679245  1.0000000000  0.8795731707  1.0000000000  0.9318518519  59.363957597  2100          1.3441032720 
0.0274853270  1.0009231548  0.9717314488  0.9787985866  0.9985862394  0.8094339623  0.9977151561  0.8826219512  0.9996297668  0.9362962963  33.922261484  1200          1.3599913414 
0.0161219101  1.0236507155  1.0000000000  1.0000000000  0.6852026390  0.6792452830  0.9988575781  0.8917682927  0.9996297668  0.9407407407  42.402826855  1500          1.3522036036 
0.0245526623  1.0117692747  0.9664310954  0.9646643110  0.9990574929  0.8018867925  0.9988575781  0.8810975610  0.9996297668  0.9348148148  42.402826855  1500          1.6054933619 
0.0260075307  1.0267422068  0.9673144876  0.9752650177  1.0000000000  0.8169811321  1.0000000000  0.8810975610  1.0000000000  0.9377777778  67.844522968  2400          1.3448442920 
0.0206160794  1.0180339849  0.9655477032  0.9752650177  1.0000000000  0.8094339623  0.9992383854  0.8810975610  1.0000000000  0.9288888889  50.883392226  1800          1.3451022959 
0.0212986866  1.0176991636  0.9637809187  0.9717314488  1.0000000000  0.8169811321  1.0000000000  0.8795731707  1.0000000000  0.9333333333  67.844522968  2400          1.3442416763 
                                                                                                                                                                                                                                                                                                                                                                            0.0324811081  1.0451446221  0.9673144876  0.9752650177  1.0000000000  0.8150943396  1.0000000000  0.8810975610  1.0000000000  0.9348148148  76.325088339  2700          1.3444689035 
                                                                                                                                                                                      0.0204846653  1.0226419085  0.9620141343  0.9717314488  1.0000000000  0.8113207547  0.9996191927  0.8795731707  1.0000000000  0.9288888889  59.363957597  2100          1.3431528838 
0.0246876579  1.0319987158  0.9628975265  0.9717314488  1.0000000000  0.8207547170  1.0000000000  0.8810975610  1.0000000000  0.9303703704  76.325088339  2700          1.3438919981 
                                                                                                                                                                                                                                                                                                                                                                            0.0240585022  1.0752665269  0.9637809187  0.9752650177  1.0000000000  0.8207547170  1.0000000000  0.8780487805  1.0000000000  0.9377777778  84.805653710  3000          1.3439160808 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0264662866  1.0454007212  0.9584805654  0.9646643110  1.0000000000  0.8094339623  0.9996191927  0.8795731707  1.0000000000  0.9303703704  67.844522968  2400          1.3425841395 
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4475047886  1.1632547379  0.9982332155  1.0000000000  0.6611687088  0.6603773585  0.7932216299  0.7728658537  0.8533876342  0.8696296296  0.0000000000  0             2.0659132004 
0.0232567939  1.0771929693  0.9602473498  0.9717314488  1.0000000000  0.8226415094  1.0000000000  0.8826219512  1.0000000000  0.9348148148  84.805653710  3000          1.3430809148 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4636222720  0.3116921186  1.0762680769  0.9973498233  1.0000000000  0.6861451461  0.6886792453  0.8107387662  0.7804878049  0.8800444280  0.8962962963  0.0000000000  0             2.1215436459 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  0.0196051187  1.0898453548  0.9558303887  0.9646643110  1.0000000000  0.8075471698  0.9996191927  0.8795731707  1.0000000000  0.9333333333  76.325088339  2700          1.3421540705 
0.0818669144  0.3335852108  1.0000000000  1.0000000000  0.6771913289  0.6867924528  0.9832444783  0.8887195122  0.9892632358  0.9348148148  8.4805653710  300           1.3426099364 
0.0703510032  0.1091867216  0.2114580430  1.0000000000  1.0000000000  0.6734213007  0.6867924528  0.9657273420  0.8948170732  0.9785264717  0.9525925926  8.4805653710  300           1.4078117077 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  0.0184311629  1.0925920892  0.9522968198  0.9646643110  1.0000000000  0.8056603774  1.0000000000  0.8765243902  1.0000000000  0.9303703704  84.805653710  3000          1.3405804364 
0.0359841959  0.8851747300  1.0000000000  1.0000000000  0.6837888784  0.6849056604  0.9927646611  0.8917682927  0.9970381340  0.9377777778  16.961130742  600           1.3422968539 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4794997871  1.0948046446  0.9982332155  1.0000000000  0.6852026390  0.6905660377  0.7802741813  0.7560975610  0.8737504628  0.8933333333  0.0000000000  0             2.0146100521 
0.0429735288  0.0454101082  0.6033260909  1.0000000000  1.0000000000  0.6729500471  0.6773584906  0.9855293222  0.8917682927  0.9903739356  0.9437037037  16.961130742  600           1.4072490517 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  0.0260665425  1.0193149745  1.0000000000  1.0000000000  0.6837888784  0.6830188679  0.9969535415  0.8948170732  0.9988893003  0.9377777778  25.441696113  900           1.3413396772 
0.0806753700  0.3310227033  1.0000000000  1.0000000000  0.6819038643  0.6867924528  0.9862909368  0.8932926829  0.9855609034  0.9466666667  8.4805653710  300           1.3421529357 
0.0338060689  0.0340416023  0.9485382054  1.0000000000  1.0000000000  0.6729500471  0.6792452830  0.9931454684  0.8887195122  0.9951869678  0.9392592593  25.441696113  900           1.4082769497 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              0.0186201680  1.0201067714  1.0000000000  1.0000000000  0.6823751178  0.6773584906  0.9992383854  0.8932926829  0.9996297668  0.9377777778  33.922261484  1200          1.3408577402 
0.0320620996  0.8567529655  1.0000000000  1.0000000000  0.6786050895  0.6811320755  0.9973343488  0.8887195122  0.9959274343  0.9511111111  16.961130742  600           1.3423912422 
0.0172676852  1.0986740484  0.9602473498  0.9611307420  1.0000000000  0.8207547170  1.0000000000  0.8734756098  1.0000000000  0.9333333333  84.805653710  3000          1.8432583602 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.5490260124  0.5135700107  1.0922180414  0.9964664311  1.0000000000  0.7323279925  0.7396226415  0.7509520183  0.7225609756  0.8793039615  0.8903703704  0.0000000000  0             2.5592179298 
0.0207719569  1.0808744321  0.9593639576  0.9681978799  1.0000000000  0.8037735849  1.0000000000  0.8719512195  1.0000000000  0.9422222222  84.805653710  3000          1.5202285719 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0931822072  0.1670831485  0.1557435127  1.0000000000  1.0000000000  0.9194156456  0.8283018868  0.8221629855  0.7911585366  0.9677897075  0.9303703704  8.4805653710  300           1.4611455282 
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.3369661868  1.0679515600  0.9982332155  1.0000000000  0.6079170594  0.6094339623  0.7798933740  0.7530487805  0.8422806368  0.8622222222  0.0000000000  0             2.3084955215 
0.0170250318  1.0180739387  1.0000000000  1.0000000000  0.6819038643  0.6698113208  0.9996191927  0.8917682927  0.9996297668  0.9407407407  42.402826855  1500          1.3410640478 
0.0249469489  0.9947728231  1.0000000000  1.0000000000  0.6786050895  0.6754716981  0.9980959634  0.8856707317  0.9992595335  0.9437037037  25.441696113  900           1.3411756404 
0.0724759419  0.1096924503  0.2379139260  1.0000000000  1.0000000000  0.6757775683  0.6867924528  0.9645849200  0.8917682927  0.9755646057  0.9407407407  8.4805653710  300           1.4129650863 
0.0481331786  0.0459832879  0.4625820327  1.0000000000  1.0000000000  0.9651272385  0.8415094340  0.8156892612  0.7881097561  0.9844502036  0.9288888889  16.961130742  600           1.4158557781 
0.0861309274  0.3632612803  1.0000000000  1.0000000000  0.6724787936  0.6754716981  0.9813404417  0.8902439024  0.9870418364  0.9481481481  8.4805653710  300           1.3527781638 
0.0152106910  1.0224945809  1.0000000000  1.0000000000  0.6800188501  0.6679245283  0.9996191927  0.8932926829  1.0000000000  0.9437037037  50.883392226  1800          1.3430775738 
0.0160853466  1.0170915461  1.0000000000  1.0000000000  0.6786050895  0.6716981132  0.9992383854  0.8856707317  0.9996297668  0.9422222222  33.922261484  1200          1.3428674348 
0.0398892755  0.0361115991  0.5004240193  1.0000000000  1.0000000000  0.6729500471  0.6735849057  0.9843869002  0.8948170732  0.9907441688  0.9422222222  16.961130742  600           1.4096812797 
0.0389354359  0.8816200972  1.0000000000  1.0000000000  0.6776625825  0.6735849057  0.9946686976  0.9009146341  0.9974083673  0.9466666667  16.961130742  600           1.3512312508 
0.0456248878  0.0435476788  0.8985743395  1.0000000000  1.0000000000  0.9849198869  0.8377358491  0.8172124905  0.7926829268  0.9940762680  0.9318518519  25.441696113  900           1.4158778063 
0.0119350417  1.0199912395  1.0000000000  1.0000000000  0.6748350613  0.6679245283  1.0000000000  0.8993902439  0.9996297668  0.9451851852  59.363957597  2100          1.3420403330 
0.0188144991  1.0180290687  1.0000000000  1.0000000000  0.6762488219  0.6735849057  1.0000000000  0.8810975610  0.9996297668  0.9422222222  42.402826855  1500          1.3440536698 
0.0095972580  0.0065473350  1.0147184954  1.0000000000  1.0000000000  0.6819038643  0.6792452830  0.9996191927  0.8871951220  0.9988893003  0.9333333333  59.363957597  2100          1.4083090123 
0.0340759989  0.0304801766  0.8884754701  1.0000000000  1.0000000000  0.6701225259  0.6735849057  0.9927646611  0.8902439024  0.9951869678  0.9437037037  25.441696113  900           1.4110021027 
0.0146938418  1.0188332742  1.0000000000  1.0000000000  0.6724787936  0.6679245283  1.0000000000  0.8963414634  0.9996297668  0.9437037037  67.844522968  2400          1.3416827488 
0.0146949659  1.0168099529  1.0000000000  0.9964664311  0.6720075401  0.6698113208  1.0000000000  0.8810975610  0.9996297668  0.9422222222  50.883392226  1800          1.3433266258 
0.0281892172  1.0119257275  1.0000000000  1.0000000000  0.6762488219  0.6792452830  0.9973343488  0.8993902439  0.9988893003  0.9451851852  25.441696113  900           1.4883115999 
0.0307392155  0.0223335937  1.0035455312  1.0000000000  1.0000000000  0.9938737041  0.8301886792  0.8137852247  0.7911585366  0.9974083673  0.9274074074  33.922261484  1200          1.6322026181 
0.0111242874  0.0077285812  1.0087521827  1.0000000000  1.0000000000  0.6837888784  0.6811320755  0.9996191927  0.8887195122  0.9992595335  0.9377777778  67.844522968  2400          1.4078853019 
0.0138810899  1.0198253053  1.0000000000  1.0000000000  0.6710650330  0.6679245283  1.0000000000  0.8932926829  0.9996297668  0.9407407407  76.325088339  2700          1.3423785361 
0.0119116267  1.0164288888  1.0000000000  0.9929328622  0.6729500471  0.6698113208  1.0000000000  0.8841463415  1.0000000000  0.9407407407  59.363957597  2100          1.3439912875 
0.0214044848  0.0181421545  0.9951221013  1.0000000000  1.0000000000  0.6672950047  0.6735849057  0.9961919269  0.8871951220  0.9966679008  0.9451851852  33.922261484  1200          1.4200225798 
0.0178145579  1.0254541067  1.0000000000  1.0000000000  0.6786050895  0.6811320755  0.9984767708  0.8963414634  0.9992595335  0.9437037037  33.922261484  1200          1.3794101453 
0.0282291533  0.0221341853  1.0025869675  1.0000000000  1.0000000000  0.9967012253  0.8264150943  0.8137852247  0.7942073171  0.9992595335  0.9303703704  42.402826855  1500          1.5535704613 
0.0094574714  0.0073041278  1.0040501072  1.0000000000  1.0000000000  0.6847313855  0.6811320755  0.9996191927  0.8902439024  1.0000000000  0.9392592593  76.325088339  2700          1.4072593991 
0.0114156414  1.0221861215  1.0000000000  1.0000000000  0.6705937795  0.6679245283  1.0000000000  0.8932926829  1.0000000000  0.9407407407  84.805653710  3000          1.3417675376 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0114198601  1.0171769591  1.0000000000  0.9929328622  0.6715362865  0.6641509434  1.0000000000  0.8841463415  1.0000000000  0.9392592593  67.844522968  2400          1.3428389247 
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.5078281164  1.1546359062  0.9982332155  1.0000000000  0.6517436381  0.6377358491  0.6264280274  0.6051829268  0.8567197334  0.8666666667  0.0000000000  0             2.4227972031 
                                                                                                                                                                                                                                                                                                                                                                                          0.0077901302  0.0048489814  1.0082360683  1.0000000000  1.0000000000  0.6847313855  0.6792452830  0.9996191927  0.8932926829  0.9996297668  0.9392592593  84.805653710  3000          1.4077008518 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
                                                                                                                                                                                                    bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4686766863  0.5293561220  1.0965782404  0.9973498233  1.0000000000  0.7389255419  0.7169811321  0.6488956588  0.6341463415  0.8578304332  0.8651851852  0.0000000000  0             2.0772557259 
0.0122328782  1.0168802897  1.0000000000  0.9893992933  0.6687087653  0.6622641509  1.0000000000  0.8795731707  1.0000000000  0.9392592593  76.325088339  2700          1.3442288224 
0.1196548514  0.2406402261  1.0000000000  1.0000000000  0.9590009425  0.8264150943  0.8233054075  0.7926829268  0.9829692706  0.9348148148  8.4805653710  300           1.3431622227 
                                                                                                                                                                                                                                                                                                                                                                                          0.0925145381  0.1615766941  0.1346746698  1.0000000000  1.0000000000  0.9156456173  0.8301886792  0.8149276466  0.7728658537  0.9674194743  0.9333333333  8.4805653710  300           1.4130487283 
0.0147576423  1.0183413774  1.0000000000  0.9893992933  0.6682375118  0.6660377358  1.0000000000  0.8795731707  1.0000000000  0.9348148148  84.805653710  3000          1.3415608104 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0446064238  0.8896060099  1.0000000000  1.0000000000  0.9905749293  0.8358490566  0.8248286367  0.7942073171  0.9940762680  0.9362962963  16.961130742  600           1.3423249602 
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.6102439761  1.1703894138  0.9982332155  1.0000000000  0.6281809614  0.6094339623  0.6001523229  0.5701219512  0.8511662347  0.8681481481  0.0000000000  0             2.0100710392 
                                                                                                                                                                                                                                                                                                                                                                                          0.0525240216  0.0527094269  0.4493524138  1.0000000000  1.0000000000  0.9594721960  0.8245283019  0.8206397563  0.7759146341  0.9885227693  0.9348148148  16.961130742  600           1.4117715859 
0.0310029225  1.0122637518  1.0000000000  1.0000000000  0.9971724788  0.8301886792  0.8225437928  0.8018292683  0.9970381340  0.9333333333  25.441696113  900           1.3411116751 
0.1175176575  0.2243718584  1.0000000000  1.0000000000  0.9637134779  0.8358490566  0.8172124905  0.7942073171  0.9803776379  0.9377777778  8.4805653710  300           1.3408633614 
                                                                                                                                                                                                    0.0396731686  0.0372011804  0.9426774898  1.0000000000  1.0000000000  0.9853911404  0.8264150943  0.8210205636  0.7835365854  0.9940762680  0.9288888889  25.441696113  900           1.4085083389 
                                                                                                                                                                                                    0.0209867168  1.0309927688  1.0000000000  1.0000000000  0.9981149859  0.8245283019  0.8244478294  0.7987804878  0.9985190670  0.9392592593  33.922261484  1200          1.3412023520 
0.0462431855  0.8538027833  1.0000000000  1.0000000000  0.9877474081  0.8245283019  0.8160700685  0.7911585366  0.9940762680  0.9362962963  16.961130742  600           1.3411625886 
                                                                                                                                                                                                    0.0271745021  0.0242963437  0.9970226612  1.0000000000  1.0000000000  0.9924599434  0.8226415094  0.8221629855  0.7820121951  0.9955572010  0.9333333333  33.922261484  1200          1.4091171034 
0.0198427760  1.0294844645  1.0000000000  1.0000000000  0.9985862394  0.8150943396  0.8240670221  0.7987804878  0.9996297668  0.9362962963  42.402826855  1500          1.3425692129 
0.0320961564  1.0188475263  1.0000000000  1.0000000000  0.9967012253  0.8132075472  0.8156892612  0.8033536585  0.9970381340  0.9348148148  25.441696113  900           1.3412012974 
0.0081594506  0.0047255475  1.0091845862  1.0000000000  0.9964664311  0.6738925542  0.6735849057  0.9996191927  0.8887195122  1.0000000000  0.9392592593  76.325088339  2700          1.4461268703 
0.0143038097  1.0276127823  1.0000000000  1.0000000000  0.6738925542  0.6773584906  1.0000000000  0.8826219512  1.0000000000  0.9422222222  76.325088339  2700          1.3800157873 
0.0101872055  0.0046139543  1.0123082119  1.0000000000  1.0000000000  1.0000000000  0.8245283019  0.8168316832  0.8033536585  1.0000000000  0.9274074074  84.805653710  3000          1.5590460602 
0.0220303150  0.0168452552  1.0074456833  1.0000000000  1.0000000000  0.9962299717  0.8245283019  0.8179741051  0.7911585366  0.9985190670  0.9318518519  42.402826855  1500          1.4095764192 
0.0211446743  1.0320911733  1.0000000000  1.0000000000  0.9995287465  0.8150943396  0.8206397563  0.8018292683  0.9996297668  0.9392592593  50.883392226  1800          1.3417840481 
0.0230827336  1.0330483703  1.0000000000  1.0000000000  0.9990574929  0.8169811321  0.8145468393  0.7987804878  0.9985190670  0.9318518519  33.922261484  1200          1.3408380095 
0.0072651470  0.0046553788  1.0089041265  1.0000000000  0.9964664311  0.6734213007  0.6754716981  1.0000000000  0.8887195122  1.0000000000  0.9377777778  84.805653710  3000          1.4731036075 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0106677731  1.0226876249  1.0000000000  1.0000000000  0.6738925542  0.6773584906  1.0000000000  0.8810975610  1.0000000000  0.9377777778  84.805653710  3000          1.3687997651 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4085497856  0.6769766212  1.1789813042  0.9982332155  1.0000000000  0.7210179076  0.6849056604  0.6207159177  0.6219512195  0.8541281007  0.8637037037  0.0000000000  0             8.2993304729 
0.0176923701  1.0393760431  1.0000000000  1.0000000000  1.0000000000  0.8056603774  0.8172124905  0.8048780488  1.0000000000  0.9392592593  59.363957597  2100          1.3424082454 
0.4935956299  1.0899044275  0.9973498233  1.0000000000  0.6314797361  0.6094339623  0.5929169840  0.5731707317  0.8537578675  0.8651851852  0.0000000000  0             4.7706916332 
0.0203884285  1.0296828508  1.0000000000  1.0000000000  0.9995287465  0.8150943396  0.8183549124  0.7942073171  0.9985190670  0.9318518519  42.402826855  1500          1.3414640069 
0.0138606912  0.0118400387  1.0192525375  1.0000000000  1.0000000000  0.9990574929  0.8132075472  0.8111195735  0.7911585366  0.9988893003  0.9288888889  59.363957597  2100          1.4078403227 
0.0184044953  1.0399743879  1.0000000000  1.0000000000  1.0000000000  0.8094339623  0.8164508759  0.8018292683  1.0000000000  0.9377777778  67.844522968  2400          1.3422595708 
                                                                                                                                                                                                    0.0194082880  1.0343488737  1.0000000000  1.0000000000  1.0000000000  0.8188679245  0.8221629855  0.7957317073  0.9988893003  0.9303703704  50.883392226  1800          1.3404475546 
0.0124785991  0.0088874713  1.0193018669  1.0000000000  1.0000000000  0.9995287465  0.8056603774  0.8111195735  0.7911585366  0.9992595335  0.9274074074  67.844522968  2400          1.4055313810 
0.0256103191  1.0655089587  1.0000000000  0.9964664311  1.0000000000  0.8207547170  0.8156892612  0.8018292683  1.0000000000  0.9333333333  76.325088339  2700          1.3444634994 
0.0174210294  1.0397738783  1.0000000000  1.0000000000  1.0000000000  0.8188679245  0.8187357197  0.7987804878  1.0000000000  0.9259259259  59.363957597  2100          1.3425470551 
                                                                                                                                                                                                    0.0216837909  1.1054814253  1.0000000000  0.9964664311  1.0000000000  0.8132075472  0.8141660320  0.7987804878  1.0000000000  0.9348148148  84.805653710  3000          1.3436826372 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0101221536  0.0053121582  1.0162360744  1.0000000000  1.0000000000  0.9995287465  0.8075471698  0.8122619954  0.7881097561  0.9992595335  0.9274074074  76.325088339  2700          1.4064667749 
0.0221514528  1.0558612488  1.0000000000  1.0000000000  1.0000000000  0.8207547170  0.8214013709  0.7957317073  1.0000000000  0.9303703704  67.844522968  2400          1.3430153116 
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4934204519  1.1354993582  0.9973498233  1.0000000000  0.6762488219  0.6566037736  0.6005331302  0.5914634146  0.8463532025  0.8503703704  0.0000000000  0             2.1629166603 
                                                                                                                                                                                                                                                                                                                                                                                          0.0109528448  0.0054871363  1.0113166857  1.0000000000  0.9964664311  0.9995287465  0.8075471698  0.8107387662  0.7926829268  0.9992595335  0.9288888889  84.805653710  3000          1.4059653544 
0.0226347610  1.0909651613  1.0000000000  1.0000000000  1.0000000000  0.8245283019  0.8233054075  0.7957317073  1.0000000000  0.9288888889  76.325088339  2700          1.3421510498 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.1303755595  0.2064087313  1.0000000000  1.0000000000  0.9566446748  0.8377358491  0.9786747906  0.8795731707  0.8785634950  0.8933333333  8.4805653710  300           1.3437600700 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.5209374428  0.5493335724  1.1070476770  0.9964664311  1.0000000000  0.7502356268  0.7377358491  0.7376237624  0.7134146341  0.8715290633  0.8800000000  0.0000000000  0             2.1159577370 
                                                                                                                                                                                                    0.0182147977  1.0917348216  1.0000000000  1.0000000000  1.0000000000  0.8207547170  0.8233054075  0.7911585366  1.0000000000  0.9303703704  84.805653710  3000          1.3421030744 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0602817456  0.8426187064  1.0000000000  1.0000000000  0.9868049010  0.8226415094  0.9935262757  0.8841463415  0.8748611625  0.8933333333  16.961130742  600           1.3441476583 
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.5947057605  1.2016640902  0.9973498233  1.0000000000  0.6517436381  0.6283018868  0.6039603960  0.5853658537  0.8526471677  0.8681481481  0.0000000000  0             2.0997498035 
0.1044267817  0.1809855792  0.1219659240  1.0000000000  1.0000000000  0.9180018850  0.8245283019  0.9565879665  0.8719512195  0.8859681599  0.9096296296  8.4805653710  300           1.4077024635 
                                                                                                                                                                                                    0.0344580193  0.9729038741  1.0000000000  1.0000000000  0.9957587182  0.8283018868  0.9977151561  0.8765243902  0.8681969641  0.8933333333  25.441696113  900           1.3429225540 
0.0168729417  1.0291147681  1.0000000000  1.0000000000  0.9575871819  0.8358490566  0.9782939832  0.8750000000  0.8815253610  0.9037037037  8.4805653710  300           1.3424575488 
0.0535997382  0.0549658120  0.4641651119  1.0000000000  1.0000000000  0.9590009425  0.8301886792  0.9821020564  0.8689024390  0.8785634950  0.9022222222  16.961130742  600           1.4064486527 
                                                                                                                                                                                                    0.0308630144  0.9794539328  1.0000000000  1.0000000000  0.9985862394  0.8226415094  0.9984767708  0.8673780488  0.8600518327  0.8903703704  33.922261484  1200          1.3428824520 
0.0566718489  0.8723460062  1.0000000000  1.0000000000  0.9853911404  0.8301886792  0.9935262757  0.8765243902  0.8781932618  0.9051851852  16.961130742  600           1.3416671570 
0.0439941441  0.0432817969  0.8875213293  1.0000000000  1.0000000000  0.9783223374  0.8226415094  0.9908606245  0.8643292683  0.8748611625  0.8918518519  25.441696113  900           1.4063556814 
                                                                                                                                                                                                    0.0247365255  0.9920393246  1.0000000000  1.0000000000  0.9995287465  0.8150943396  0.9984767708  0.8689024390  0.8582006664  0.8844444444  42.402826855  1500          1.3421407135 
0.0385114411  0.9527547447  1.0000000000  1.0000000000  0.9957587182  0.8132075472  0.9969535415  0.8750000000  0.8737504628  0.8962962963  25.441696113  900           1.3414144182 
0.0338465226  0.0261002592  0.9574648974  1.0000000000  1.0000000000  0.9896324222  0.8226415094  0.9961919269  0.8628048780  0.8711588301  0.8800000000  33.922261484  1200          1.4080188123 
                                                                                                                                                                                                    0.0238281389  0.9976196637  1.0000000000  1.0000000000  0.9995287465  0.8188679245  0.9988575781  0.8582317073  0.8544983340  0.8829629630  50.883392226  1800          1.3432568796 
0.0280671162  0.9939581543  1.0000000000  1.0000000000  0.9981149859  0.8075471698  0.9992383854  0.8750000000  0.8689374306  0.8903703704  33.922261484  1200          1.3414262231 
0.0238501269  0.0175253537  0.9669653028  1.0000000000  1.0000000000  0.9948162111  0.8301886792  0.9977151561  0.8612804878  0.8659755646  0.8770370370  42.402826855  1500          1.4074256873 
                                                                                                                                                                                                    0.0345685717  1.0178141636  1.0000000000  1.0000000000  1.0000000000  0.8188679245  0.9992383854  0.8643292683  0.8541281007  0.8785185185  59.363957597  2100          1.3433906158 
0.0300030032  0.9904085312  1.0000000000  1.0000000000  0.9990574929  0.8075471698  0.9992383854  0.8704268293  0.8641243984  0.8888888889  42.402826855  1500          1.3425417336 
                                                                                                                                                                                      0.0207552089  0.0183197635  0.9735393212  1.0000000000  1.0000000000  0.9962299717  0.8264150943  0.9992383854  0.8689024390  0.8626434654  0.8755555556  50.883392226  1800          1.4081525183 
l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.5216522813  1.1069601774  0.9973498233  1.0000000000  0.6658812441  0.6490566038  0.6858339680  0.6676829268  0.8663457979  0.8740740741  0.0000000000  0             2.1074814796 
0.0093036016  0.0060468507  1.0196166660  1.0000000000  1.0000000000  1.0000000000  0.8094339623  0.8206397563  0.7835365854  1.0000000000  0.9259259259  84.805653710  3000          1.4251824474 
0.0349860825  1.0561427925  1.0000000000  1.0000000000  1.0000000000  0.8113207547  0.9992383854  0.8597560976  0.8541281007  0.8800000000  67.844522968  2400          1.3448605323 
0.0266298969  0.9982489413  1.0000000000  1.0000000000  0.9995287465  0.7924528302  0.9992383854  0.8750000000  0.8633839319  0.8859259259  50.883392226  1800          1.3421762427 
0.0193358690  0.0153832467  0.9693542975  1.0000000000  1.0000000000  0.9971724788  0.8245283019  0.9996191927  0.8689024390  0.8596815994  0.8770370370  59.363957597  2100          1.4073006789 
0.1295371771  0.1721573192  1.0000000000  1.0000000000  0.9547596607  0.8339622642  0.9801980198  0.8719512195  0.8785634950  0.8977777778  8.4805653710  300           1.3455818486 
0.0246212322  1.0942523162  1.0000000000  1.0000000000  1.0000000000  0.8094339623  0.9992383854  0.8582317073  0.8544983340  0.8814814815  76.325088339  2700          1.3432348776 
0.0318426770  1.0230326251  1.0000000000  1.0000000000  0.9995287465  0.8000000000  0.9996191927  0.8734756098  0.8641243984  0.8814814815  59.363957597  2100          1.3424083447 
0.0137771479  0.0080932794  0.9710010781  1.0000000000  1.0000000000  0.9981149859  0.8226415094  0.9996191927  0.8719512195  0.8596815994  0.8755555556  67.844522968  2400          1.4074529537 
0.0572351552  0.8967767307  1.0000000000  1.0000000000  0.9868049010  0.8245283019  0.9920030465  0.8719512195  0.8670862643  0.8933333333  16.961130742  600           1.3447267095 
0.0154378790  1.1066164903  1.0000000000  0.9964664311  1.0000000000  0.8094339623  0.9996191927  0.8582317073  0.8519067012  0.8770370370  84.805653710  3000          1.3437406683 
0.0391484469  1.0619559143  1.0000000000  1.0000000000  0.9995287465  0.7981132075  0.9996191927  0.8750000000  0.8604220659  0.8785185185  67.844522968  2400          1.3420203733 
0.0427611410  0.9506328702  1.0000000000  1.0000000000  0.9934024505  0.8245283019  0.9965727342  0.8719512195  0.8533876342  0.8903703704  25.441696113  900           1.3416560308 
1.4074970126 
0.0278845118  1.1138076947  1.0000000000  1.0000000000  0.9995287465  0.8018867925  1.0000000000  0.8750000000  0.8563495002  0.8785185185  76.325088339  2700          1.3436464071 
0.0094644017  0.0046336284  0.9662732969  1.0000000000  1.0000000000  0.9995287465  0.8188679245  1.0000000000  0.8750000000  0.8593113662  0.8696296296  84.805653710  3000          1.4103942267 
0.0305578374  0.9897850660  1.0000000000  1.0000000000  0.9985862394  0.8207547170  0.9996191927  0.8750000000  0.8522769345  0.8844444444  33.922261484  1200          1.3446855410 
0.0228115363  0.9881917147  1.0000000000  1.0000000000  0.9990574929  0.8169811321  0.9996191927  0.8704268293  0.8507960015  0.8829629630  42.402826855  1500          1.3420959091 
0.0263493878  0.9927352633  1.0000000000  1.0000000000  0.9995287465  0.8113207547  1.0000000000  0.8689024390  0.8474639023  0.8785185185  50.883392226  1800          1.3425471528 
0.0243609699  1.0079508372  1.0000000000  1.0000000000  1.0000000000  0.8132075472  1.0000000000  0.8658536585  0.8426508700  0.8740740741  59.363957597  2100          1.3419233537 
0.0398357816  1.0478465088  1.0000000000  1.0000000000  1.0000000000  0.8150943396  1.0000000000  0.8689024390  0.8411699371  0.8696296296  67.844522968  2400          1.3418532785 
0.0279995828  1.0982553323  1.0000000000  1.0000000000  1.0000000000  0.8113207547  1.0000000000  0.8704268293  0.8407997038  0.8637037037  76.325088339  2700          1.3421866687 
0.0239081885  1.1073561825  1.0000000000  1.0000000000  1.0000000000  0.8094339623  1.0000000000  0.8734756098  0.8422806368  0.8592592593  84.805653710  3000          1.3427678219 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.6643251777  0.6726179719  1.0811266899  0.9973498233  1.0000000000  0.6969839774  0.6924528302  0.8122619954  0.7728658537  0.8826360607  0.8948148148  0.0000000000  0             4.5047075748 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.7120100856  0.7267367840  1.0429655313  0.9964664311  1.0000000000  0.7403393025  0.7433962264  0.7136329018  0.6905487805  0.8700481303  0.8844444444  0.0000000000  0             2.4070403576 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.6510517597  0.6876097322  1.1514877081  0.9973498233  1.0000000000  0.6875589067  0.6849056604  0.8073115004  0.7789634146  0.8696778971  0.8874074074  0.0000000000  0             12.498008251 
0.1373664966  0.2323805246  0.2728756745  0.9893992933  1.0000000000  0.9090480679  0.8283018868  0.9535415080  0.8780487805  0.9707515735  0.9451851852  8.4805653710  300           1.4068122737 
0.1372413719  0.2401795064  0.2704397878  0.9885159011  0.9964664311  0.9137606032  0.8283018868  0.9512566641  0.8704268293  0.9718622732  0.9437037037  8.4805653710  300           1.4059671418 
0.1414912731  0.2406477131  0.2846099868  0.9938162544  1.0000000000  0.9170593779  0.8245283019  0.9527798934  0.8673780488  0.9689004073  0.9437037037  8.4805653710  300           1.4118015210 
0.0650760534  0.0662466740  0.4832902823  0.9858657244  0.9964664311  0.9655984920  0.8339622642  0.9821020564  0.8689024390  0.9877823029  0.9496296296  16.961130742  600           1.4063195380 
0.0606577695  0.0687256640  0.5043800663  0.9823321555  0.9929328622  0.9561734213  0.8320754717  0.9775323686  0.8704268293  0.9892632358  0.9422222222  16.961130742  600           1.4069496282 
0.0680155099  0.0694811833  0.5093075899  0.9840989399  0.9929328622  0.9594721960  0.8283018868  0.9752475248  0.8582317073  0.9881525361  0.9437037037  16.961130742  600           1.4114048155 
0.0478377812  0.0420711342  0.8238153160  0.9832155477  0.9964664311  0.9816211122  0.8339622642  0.9920030465  0.8582317073  0.9940762680  0.9422222222  25.441696113  900           1.4071276395 
0.0547625912  0.0448163308  0.8048324856  0.9779151943  0.9823321555  0.9787935910  0.8245283019  0.9885757807  0.8658536585  0.9929655683  0.9392592593  25.441696113  900           1.4080194799 
0.0506421925  0.0442903867  0.7936599261  0.9823321555  0.9893992933  0.9797360980  0.8226415094  0.9878141660  0.8551829268  0.9944465013  0.9392592593  25.441696113  900           1.4119228427 
0.0454358662  0.0351667980  0.9339863086  0.9787985866  0.9823321555  0.9901036758  0.8245283019  0.9954303123  0.8597560976  0.9981488338  0.9392592593  33.922261484  1200          1.4073189839 
0.0406779826  0.0320514941  0.9220897524  0.9743816254  0.9823321555  0.9905749293  0.8169811321  0.9935262757  0.8582317073  0.9962976675  0.9362962963  33.922261484  1200          1.4079321718 
0.0408816467  0.0284931049  0.9190319075  0.9770318021  0.9823321555  0.9886899152  0.8245283019  0.9920030465  0.8582317073  0.9974083673  0.9377777778  33.922261484  1200          1.4127515173 
0.0352291009  0.0241549230  0.9678322329  0.9743816254  0.9787985866  0.9948162111  0.8301886792  0.9969535415  0.8612804878  0.9992595335  0.9362962963  42.402826855  1500          1.4078853989 
0.0323908269  0.0252408838  0.9584359239  0.9726148410  0.9681978799  0.9938737041  0.8113207547  0.9965727342  0.8567073171  0.9977786005  0.9422222222  42.402826855  1500          1.4074571991 
0.0339538884  0.0237979771  0.9452631013  0.9761484099  0.9752650177  0.9929311970  0.8169811321  0.9954303123  0.8597560976  0.9992595335  0.9377777778  42.402826855  1500          1.4137673720 
0.0263107586  0.0188409320  0.9858414386  0.9637809187  0.9646643110  0.9957587182  0.8132075472  0.9969535415  0.8597560976  0.9992595335  0.9437037037  50.883392226  1800          1.4077912060 
0.0246073847  0.0153448199  0.9910602615  0.9734982332  0.9717314488  0.9967012253  0.8358490566  0.9988575781  0.8597560976  0.9992595335  0.9348148148  50.883392226  1800          1.4081769697 
0.0217348891  0.0167799501  0.9764560415  0.9734982332  0.9717314488  0.9962299717  0.8075471698  0.9973343488  0.8551829268  0.9992595335  0.9392592593  50.883392226  1800          1.4145676064 
0.0222463097  0.0161284140  0.9889207975  0.9628975265  0.9646643110  0.9971724788  0.8132075472  0.9980959634  0.8597560976  0.9992595335  0.9392592593  59.363957597  2100          1.4074856186 
0.0187841857  0.0144178285  0.9941141105  0.9717314488  0.9752650177  0.9990574929  0.8320754717  0.9996191927  0.8597560976  0.9992595335  0.9377777778  59.363957597  2100          1.4075793386 
                                                                                                                                                                                                    0.0168886969  0.0121089506  0.9998555142  0.9611307420  0.9646643110  0.9981149859  0.8132075472  0.9988575781  0.8612804878  0.9996297668  0.9407407407  67.844522968  2400          1.4087602782 
0.0148023154  0.0084333029  0.9896008124  0.9717314488  0.9717314488  1.0000000000  0.8320754717  0.9996191927  0.8597560976  0.9996297668  0.9392592593  67.844522968  2400          1.4086019524 
                                                                                                                                                                                                    0.0173250981  0.0111591789  0.9992372837  0.9602473498  0.9611307420  0.9990574929  0.8113207547  0.9988575781  0.8597560976  0.9996297668  0.9437037037  76.325088339  2700          1.4085192482 
0.0138875151  0.0111576140  0.9967962519  0.9708480565  0.9717314488  1.0000000000  0.8245283019  0.9996191927  0.8597560976  0.9996297668  0.9377777778  76.325088339  2700          1.4084507267 
                                                                                                                                                                                                    0.0139343217  0.0076461995  1.0034961019  0.9584805654  0.9611307420  0.9995287465  0.8150943396  0.9992383854  0.8628048780  0.9996297668  0.9437037037  84.805653710  3000          1.4079289055 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0129533669  0.0078269625  0.9965425605  0.9699646643  0.9717314488  1.0000000000  0.8226415094  0.9996191927  0.8597560976  0.9996297668  0.9392592593  84.805653710  3000          1.4088401850 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.5490260124  0.5135696530  1.0922180414  0.9964664311  1.0000000000  0.7323279925  0.7396226415  0.7509520183  0.7225609756  0.8793039615  0.8903703704  0.0000000000  0             2.1064598560 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4636221826  0.3116923273  1.0762679577  0.9973498233  1.0000000000  0.6861451461  0.6886792453  0.8107387662  0.7804878049  0.8800444280  0.8962962963  0.0000000000  0             2.2400162220 
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  0.0720031533  0.1082228880  0.2175031187  1.0000000000  1.0000000000  0.6842601320  0.6830188679  0.9657273420  0.8917682927  0.9740836727  0.9466666667  8.4805653710  300           1.4067237385 
0.0682267209  0.1075523539  0.2287058009  1.0000000000  1.0000000000  0.6748350613  0.6943396226  0.9653465347  0.8932926829  0.9763050722  0.9525925926  8.4805653710  300           1.4059739542 
                                                                                                                                                                                                    0.0370291434  0.0382652479  0.5706948274  1.0000000000  1.0000000000  0.6842601320  0.6830188679  0.9859101295  0.8902439024  0.9903739356  0.9437037037  16.961130742  600           1.4068889586 
0.0398190111  0.0378358615  0.5700527811  1.0000000000  1.0000000000  0.6776625825  0.6849056604  0.9855293222  0.8993902439  0.9900037023  0.9422222222  16.961130742  600           1.4058853722 
                                                                                                                                                                                                    0.0387193221  0.0369413298  0.9096812397  1.0000000000  1.0000000000  0.6814326107  0.6811320755  0.9920030465  0.8917682927  0.9944465013  0.9481481481  25.441696113  900           1.4069446127 
0.0306548558  0.0300119131  0.9036121911  1.0000000000  1.0000000000  0.6762488219  0.6830188679  0.9950495050  0.8948170732  0.9948167345  0.9407407407  25.441696113  900           1.4063256176 
                                                                                                                                                                                                    0.0259137809  0.0210149962  0.9893246390  1.0000000000  1.0000000000  0.6781338360  0.6792452830  0.9950495050  0.8932926829  0.9974083673  0.9466666667  33.922261484  1200          1.4074699982 
0.0270732615  0.0218071054  0.9739210204  1.0000000000  1.0000000000  0.6771913289  0.6811320755  0.9969535415  0.8856707317  0.9962976675  0.9422222222  33.922261484  1200          1.4061011895 
                                                                                                                                                                                                    0.0186566930  0.0135364221  1.0074647417  1.0000000000  1.0000000000  0.6734213007  0.6773584906  0.9961919269  0.8902439024  0.9985190670  0.9451851852  42.402826855  1500          1.4069230517 
0.0214693959  0.0161860153  0.9939245230  1.0000000000  1.0000000000  0.6767200754  0.6773584906  0.9980959634  0.8841463415  0.9977786005  0.9407407407  42.402826855  1500          1.4077744468 
                                                                                                                                                                                                    0.0157655623  0.0147861759  1.0107072902  1.0000000000  1.0000000000  0.6734213007  0.6754716981  0.9980959634  0.8948170732  0.9996297668  0.9466666667  50.883392226  1800          1.4062825402 
0.0132308508  0.0094466164  1.0041757693  1.0000000000  1.0000000000  0.6771913289  0.6792452830  0.9984767708  0.8841463415  0.9988893003  0.9407407407  50.883392226  1800          1.4079636002 
                                                                                                                                                                                                    0.0128086671  0.0084732419  1.0040131696  1.0000000000  1.0000000000  0.6724787936  0.6716981132  0.9980959634  0.8917682927  1.0000000000  0.9437037037  59.363957597  2100          1.4082785360 
0.0127257616  0.0063673967  1.0118367843  1.0000000000  1.0000000000  0.6804901037  0.6792452830  0.9992383854  0.8826219512  0.9992595335  0.9407407407  59.363957597  2100          1.4069367186 
                                                                                                                                                                                                    0.0109090884  0.0065866990  1.0017967910  1.0000000000  0.9964664311  0.6738925542  0.6735849057  0.9984767708  0.8963414634  1.0000000000  0.9407407407  67.844522968  2400          1.4086841154 
0.0110711416  0.0068207767  1.0017963707  1.0000000000  1.0000000000  0.6790763431  0.6773584906  0.9992383854  0.8856707317  0.9992595335  0.9407407407  67.844522968  2400          1.4065499187 
                                                                                                                                                                                                    0.0085324457  0.0048875696  1.0112341368  1.0000000000  0.9964664311  0.6743638077  0.6716981132  0.9988575781  0.8963414634  1.0000000000  0.9392592593  76.325088339  2700          1.4082198326 
0.0111452053  0.0066973394  1.0044366487  1.0000000000  1.0000000000  0.6776625825  0.6792452830  0.9992383854  0.8826219512  0.9996297668  0.9407407407  76.325088339  2700          1.4073773472 
                                                                                                                                                                                                    0.0087945175  0.0055604000  1.0122510697  1.0000000000  0.9964664311  0.6738925542  0.6735849057  0.9988575781  0.8978658537  1.0000000000  0.9437037037  84.805653710  3000          1.4064456900 
0.0090277906  0.0056766500  1.0038188700  1.0000000000  1.0000000000  0.6786050895  0.6773584906  0.9992383854  0.8841463415  1.0000000000  0.9407407407  84.805653710  3000          1.4077468657 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4085499644  0.6769766212  1.1789811850  0.9982332155  1.0000000000  0.7210179076  0.6849056604  0.6203351104  0.6219512195  0.8541281007  0.8637037037  0.0000000000  0             2.1784512997 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4686767757  0.5293559432  1.0965782404  0.9973498233  1.0000000000  0.7389255419  0.7169811321  0.6488956588  0.6341463415  0.8578304332  0.8651851852  0.0000000000  0             2.1698493958 
0.0088192736  0.0067730290  1.0079155006  1.0000000000  0.9964664311  0.6809613572  0.6641509434  0.9988575781  0.8871951220  1.0000000000  0.9481481481  84.805653710  3000          1.4129118284 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4300787449  0.5399425030  1.1457265615  0.9973498233  1.0000000000  0.7393967955  0.7150943396  0.6690784463  0.6432926829  0.8604220659  0.8755555556  0.0000000000  0             4.3237726688 
0.0932495288  0.1613400519  0.1382230754  1.0000000000  1.0000000000  0.9245994345  0.8301886792  0.8221629855  0.7865853659  0.9651980748  0.9274074074  8.4805653710  300           1.4077668063 
0.0889965518  0.1625832066  0.1313909335  1.0000000000  1.0000000000  0.9189443921  0.8339622642  0.8164508759  0.7774390244  0.9689004073  0.9377777778  8.4805653710  300           1.4080427504 
0.0927644159  0.1617981258  0.1404423052  1.0000000000  1.0000000000  0.9203581527  0.8320754717  0.8164508759  0.7896341463  0.9700111070  0.9348148148  8.4805653710  300           1.4127021019 
0.0445143854  0.0499766674  0.4213422352  1.0000000000  1.0000000000  0.9646559849  0.8283018868  0.8244478294  0.7896341463  0.9851906701  0.9333333333  16.961130742  600           1.4064310074 
0.0462682197  0.0524934356  0.4626059879  1.0000000000  1.0000000000  0.9655984920  0.8339622642  0.8160700685  0.7743902439  0.9874120696  0.9348148148  16.961130742  600           1.4068020233 
0.0460478116  0.0487338878  0.4648535509  1.0000000000  1.0000000000  0.9660697455  0.8433962264  0.8122619954  0.7835365854  0.9844502036  0.9333333333  16.961130742  600           1.4119522158 
0.0410107731  0.0394636249  0.9207410016  1.0000000000  1.0000000000  0.9849198869  0.8320754717  0.8194973343  0.7774390244  0.9933358016  0.9362962963  25.441696113  900           1.4063604808 
0.0428788512  0.0335583149  0.8778384542  1.0000000000  1.0000000000  0.9797360980  0.8301886792  0.8217821782  0.7865853659  0.9933358016  0.9318518519  25.441696113  900           1.4074928013 
0.0426617286  0.0404646335  0.9208174934  1.0000000000  1.0000000000  0.9820923657  0.8415094340  0.8126428027  0.7942073171  0.9914846353  0.9333333333  25.441696113  900           1.4121490550 
0.0286709545  0.0205943624  0.9837184336  1.0000000000  1.0000000000  0.9915174364  0.8320754717  0.8198781417  0.7759146341  0.9951869678  0.9348148148  33.922261484  1200          1.4066781497 
0.0368646529  0.0292915995  0.9737468034  1.0000000000  1.0000000000  0.9915174364  0.8377358491  0.8210205636  0.7896341463  0.9962976675  0.9348148148  33.922261484  1200          1.4074073474 
0.0318313152  0.0269111207  0.9895569680  1.0000000000  1.0000000000  0.9929311970  0.8339622642  0.8092155369  0.8003048780  0.9966679008  0.9362962963  33.922261484  1200          1.4117167584 
0.0203128552  0.0178867050  1.0164140163  1.0000000000  1.0000000000  0.9967012253  0.8339622642  0.8175932978  0.7728658537  0.9970381340  0.9333333333  42.402826855  1500          1.4079640579 
0.0228196785  0.0158931373  1.0070426021  1.0000000000  0.9964664311  0.9948162111  0.8264150943  0.8172124905  0.7850609756  0.9981488338  0.9333333333  42.402826855  1500          1.4066114998 
0.0240636664  0.0196346111  1.0160618625  1.0000000000  1.0000000000  0.9957587182  0.8301886792  0.8076923077  0.8003048780  0.9974083673  0.9333333333  42.402826855  1500          1.4125233261 
0.0210364177  0.0172799675  1.0161542040  1.0000000000  0.9964664311  0.9971724788  0.8226415094  0.8137852247  0.7835365854  0.9992595335  0.9318518519  50.883392226  1800          1.4073769037 
0.0203935083  0.0124468982  1.0203550925  1.0000000000  1.0000000000  0.9976437323  0.8339622642  0.8179741051  0.7804878049  0.9985190670  0.9288888889  50.883392226  1800          1.4056635706 
0.0182096637  0.0118934466  1.0112386411  1.0000000000  1.0000000000  0.9971724788  0.8283018868  0.8107387662  0.7987804878  0.9988893003  0.9303703704  50.883392226  1800          1.4125774598 
0.0129670492  0.0094087821  1.0206940329  1.0000000000  1.0000000000  0.9976437323  0.8396226415  0.8183549124  0.7865853659  0.9992595335  0.9288888889  59.363957597  2100          1.4065275820 
0.0121202118  0.0085596427  1.0234853929  1.0000000000  0.9964664311  0.9985862394  0.8188679245  0.8099771516  0.7850609756  1.0000000000  0.9318518519  59.363957597  2100          1.4080433027 
0.0144376471  0.0095267633  1.0130462720  1.0000000000  1.0000000000  0.9985862394  0.8207547170  0.8115003808  0.7926829268  1.0000000000  0.9318518519  59.363957597  2100          1.4131458195 
0.0157429594  0.0080319792  1.0186966105  1.0000000000  0.9964664311  1.0000000000  0.8207547170  0.8095963442  0.7804878049  1.0000000000  0.9318518519  67.844522968  2400          1.4081583317 
0.0148315820  0.0078732250  1.0196772929  1.0000000000  1.0000000000  0.9985862394  0.8339622642  0.8202589490  0.7820121951  0.9996297668  0.9288888889  67.844522968  2400          1.4080812867 
0.0116875821  0.0060314779  1.0211447412  1.0000000000  1.0000000000  0.9990574929  0.8320754717  0.8130236101  0.7972560976  1.0000000000  0.9303703704  67.844522968  2400          1.4129518223 
0.0097338506  0.0073440091  1.0176193062  1.0000000000  1.0000000000  0.9990574929  0.8339622642  0.8194973343  0.7804878049  0.9996297668  0.9288888889  76.325088339  2700          1.4070030554 
0.0101502684  0.0066893942  1.0184052213  1.0000000000  0.9964664311  1.0000000000  0.8226415094  0.8095963442  0.7804878049  1.0000000000  0.9318518519  76.325088339  2700          1.4074233572 
0.0093510358  0.0068238000  1.0229350388  1.0000000000  1.0000000000  1.0000000000  0.8320754717  0.8130236101  0.7896341463  0.9996297668  0.9303703704  76.325088339  2700          1.4132055124 
0.0087915211  0.0044470651  1.0180177144  1.0000000000  1.0000000000  1.0000000000  0.8283018868  0.8191165270  0.7804878049  0.9996297668  0.9244444444  84.805653710  3000          1.4082047240 
0.0090371603  0.0051375487  1.0190646485  1.0000000000  1.0000000000  1.0000000000  0.8188679245  0.8076923077  0.7804878049  1.0000000000  0.9288888889  84.805653710  3000          1.4079447953 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.5329860449  0.6413689852  1.2106680870  0.9973498233  1.0000000000  0.7450518379  0.7358490566  0.6824067022  0.6524390244  0.8593113662  0.8740740741  0.0000000000  0             2.2241189480 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.5209374428  0.5493333936  1.1070475578  0.9964664311  1.0000000000  0.7502356268  0.7377358491  0.7376237624  0.7134146341  0.8715290633  0.8800000000  0.0000000000  0             2.1830670834 
0.0075124908  0.0062468860  1.0139149296  1.0000000000  0.9964664311  1.0000000000  0.8301886792  0.8111195735  0.7896341463  0.9996297668  0.9288888889  84.805653710  3000          1.4115094860 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMask
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.4956889153  0.5268250704  1.1394190788  0.9973498233  1.0000000000  0.7455230914  0.7264150943  0.6778370145  0.6493902439  0.8596815994  0.8725925926  0.0000000000  0             2.7569398880 
0.1048223100  0.1813721826  0.0876262019  0.9991166078  1.0000000000  0.9208294062  0.8283018868  0.9630616908  0.8780487805  0.8822658275  0.9111111111  8.4805653710  300           1.4569546103 
0.1040648620  0.1804223769  0.1280004428  1.0000000000  1.0000000000  0.9161168709  0.8339622642  0.9588728104  0.8689024390  0.8855979267  0.9096296296  8.4805653710  300           1.4558302291 
0.1038027675  0.1829729654  0.1106191179  1.0000000000  1.0000000000  0.9203581527  0.8396226415  0.9603960396  0.8719512195  0.8815253610  0.9111111111  8.4805653710  300           1.4093506463 
0.0579221791  0.0686999244  0.6383396060  0.9991166078  1.0000000000  0.9637134779  0.8264150943  0.9821020564  0.8704268293  0.8822658275  0.9066666667  16.961130742  600           1.4070522610 
0.0550077372  0.0557435551  0.4658269233  1.0000000000  1.0000000000  0.9627709708  0.8320754717  0.9798172125  0.8567073171  0.8807848945  0.9125925926  16.961130742  600           1.4065591955 
0.0485334563  0.0492146875  0.4159821985  1.0000000000  1.0000000000  0.9651272385  0.8452830189  0.9847677075  0.8689024390  0.8733802295  0.8977777778  16.961130742  600           1.4090296857 
0.0444026837  0.0440404973  0.8930171201  1.0000000000  1.0000000000  0.9806786051  0.8283018868  0.9900990099  0.8643292683  0.8763420955  0.8992592593  25.441696113  900           1.4062311546 
0.0448473212  0.0396488760  0.8835672052  1.0000000000  1.0000000000  0.9816211122  0.8245283019  0.9912414318  0.8612804878  0.8763420955  0.9051851852  25.441696113  900           1.4061109773 
0.0473895163  0.0421376866  0.8350778089  1.0000000000  1.0000000000  0.9835061263  0.8320754717  0.9927646611  0.8795731707  0.8678267308  0.8874074074  25.441696113  900           1.4097351940 
0.0339936214  0.0309487871  0.9564637009  1.0000000000  1.0000000000  0.9896324222  0.8301886792  0.9942878903  0.8643292683  0.8733802295  0.8918518519  33.922261484  1200          1.4070633292 
0.0334945577  0.0281423415  0.9493207939  1.0000000000  1.0000000000  0.9901036758  0.8264150943  0.9961919269  0.8612804878  0.8681969641  0.8933333333  33.922261484  1200          1.4069852908 
0.0363958505  0.0297073092  0.9482953701  1.0000000000  1.0000000000  0.9919886899  0.8226415094  0.9965727342  0.8810975610  0.8644946316  0.8874074074  33.922261484  1200          1.4095666488 
0.0225374509  0.0196823264  0.9727405155  1.0000000000  1.0000000000  0.9943449576  0.8226415094  0.9961919269  0.8551829268  0.8693076638  0.8755555556  42.402826855  1500          1.4076802651 
0.0268606654  0.0203460323  0.9681491111  1.0000000000  1.0000000000  0.9943449576  0.8301886792  0.9973343488  0.8643292683  0.8685671973  0.8888888889  42.402826855  1500          1.4080444495 
0.0266881288  0.0233546855  0.9679498897  1.0000000000  1.0000000000  0.9967012253  0.8169811321  0.9977151561  0.8765243902  0.8622732321  0.8785185185  42.402826855  1500          1.4095514146 
0.0219567085  0.0157091357  0.9765858426  1.0000000000  1.0000000000  0.9967012253  0.8207547170  0.9984767708  0.8536585366  0.8663457979  0.8755555556  50.883392226  1800          1.4082024964 
0.0204412644  0.0141593952  0.9661398526  1.0000000000  1.0000000000  0.9957587182  0.8283018868  0.9980959634  0.8658536585  0.8619029989  0.8829629630  50.883392226  1800          1.4078825879 
0.0220808557  0.0182549307  0.9722074715  1.0000000000  1.0000000000  0.9981149859  0.8169811321  0.9988575781  0.8750000000  0.8559792669  0.8755555556  50.883392226  1800          1.4100131321 
0.0160569952  0.0093174427  0.9784191221  1.0000000000  1.0000000000  0.9985862394  0.8207547170  0.9984767708  0.8506097561  0.8596815994  0.8725925926  59.363957597  2100          1.4079812002 
0.0187924501  0.0121737450  0.9652809586  1.0000000000  1.0000000000  0.9976437323  0.8226415094  0.9996191927  0.8643292683  0.8589411329  0.8770370370  59.363957597  2100          1.4073761519 
0.0168975811  0.0115491768  0.9803125985  1.0000000000  1.0000000000  0.9990574929  0.8169811321  0.9984767708  0.8490853659  0.8548685672  0.8696296296  67.844522968  2400          1.4072961036 
0.0122252700  0.0096128107  0.9721661747  1.0000000000  1.0000000000  0.9981149859  0.8226415094  0.9996191927  0.8612804878  0.8563495002  0.8725925926  67.844522968  2400          1.4071282665 
0.0123105371  0.0070005841  0.9738720032  1.0000000000  1.0000000000  0.9995287465  0.8169811321  1.0000000000  0.8689024390  0.8507960015  0.8666666667  67.844522968  2400          1.4103736997 
0.0141350135  0.0091279066  0.9628743887  1.0000000000  1.0000000000  0.9990574929  0.8226415094  1.0000000000  0.8612804878  0.8559792669  0.8711111111  76.325088339  2700          1.4063186653 
0.0096922991  0.0056971329  0.9694762270  1.0000000000  1.0000000000  1.0000000000  0.8188679245  0.9992383854  0.8567073171  0.8489448352  0.8651851852  84.805653710  3000          1.4064638424 
0.0119743989  0.0070269291  0.9674857750  1.0000000000  1.0000000000  0.9995287465  0.8188679245  1.0000000000  0.8643292683  0.8544983340  0.8725925926  84.805653710  3000          1.4068962590 
0.0117911506  0.0057388939  0.9727205412  1.0000000000  1.0000000000  0.9995287465  0.8169811321  1.0000000000  0.8673780488  0.8478341355  0.8637037037  76.325088339  2700          1.4105618596 
0.0122622534  0.0077590558  0.9648681625  1.0000000000  1.0000000000  1.0000000000  0.8150943396  0.9996191927  0.8704268293  0.8445020363  0.8651851852  84.805653710  3000          1.4106577190 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANwoMatch
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
5.4683876038  0.6761019826  1.2104073763  0.8274974253  0.8391752577  0.6912943872  0.6872852234  0.8978040541  0.8771138670  0.8961560528  0.9127439724  0.0000000000  0             2.0834066868 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
5.2485375404  0.6417099833  1.1245180368  0.8295571576  0.8309278351  0.6858533792  0.6815578465  0.8989301802  0.8838782413  0.8952954676  0.9104477612  0.0000000000  0             8.3532686234 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
5.0679435730  0.6326796412  1.1734918356  0.8233779609  0.8371134021  0.6924398625  0.6849942726  0.8983671171  0.8838782413  0.8967297762  0.9138920781  0.0000000000  0             11.604248762 
0.9519597088  0.3245794557  0.4389248409  0.8619979403  0.8597938144  0.8808705613  0.8132875143  0.9687500000  0.9436302142  0.9604130809  0.9334098737  4.9433573635  300           1.3532583761 
0.9281589724  0.3309048271  0.4508223694  0.8650875386  0.8536082474  0.8765750286  0.8213058419  0.9676238739  0.9391206313  0.9575444636  0.9253731343  4.9433573635  300           1.3525203832 
0.8985467433  0.3323605147  0.4468123426  0.8656024717  0.8597938144  0.8814432990  0.8109965636  0.9721283784  0.9436302142  0.9618473896  0.9322617681  4.9433573635  300           1.3514999715 
0.3029046641  0.1346654736  0.5727717611  0.8661174047  0.8597938144  0.9215349370  0.8373424971  0.9836711712  0.9549041714  0.9753298910  0.9311136625  9.8867147271  600           1.3496317601 
0.3035954902  0.1301904864  0.5811215699  0.8666323378  0.8597938144  0.9238258877  0.8304696449  0.9828265766  0.9571589628  0.9776247849  0.9288174512  9.8867147271  600           1.3484342957 
0.3128307845  0.1281915109  0.5697214338  0.8661174047  0.8639175258  0.9229667812  0.8361970218  0.9814189189  0.9515219842  0.9750430293  0.9299655568  9.8867147271  600           1.3506188305 
0.1928647187  0.0813759465  0.7644252665  0.8697219361  0.8618556701  0.9464490263  0.8361970218  0.9895833333  0.9627959414  0.9845094664  0.9253731343  14.830072090  900           1.3480879052 
0.1946715458  0.0751852348  0.7655819599  0.8671472709  0.8721649485  0.9441580756  0.8476517755  0.9884572072  0.9616685457  0.9839357430  0.9299655568  14.830072090  900           1.3530482419 
0.1913604291  0.0732567189  0.7608114281  0.8661174047  0.8618556701  0.9458762887  0.8499427262  0.9878941441  0.9571589628  0.9839357430  0.9299655568  14.830072090  900           1.3512782518 
0.1454071610  0.0534215185  0.9054060123  0.8650875386  0.8618556701  0.9587628866  0.8522336770  0.9909909910  0.9616685457  0.9882386690  0.9288174512  19.773429454  1200          1.3514712342 
0.1456601959  0.0569994772  0.9108573767  0.8661174047  0.8618556701  0.9636311569  0.8510882016  0.9921171171  0.9616685457  0.9876649455  0.9230769231  19.773429454  1200          1.3520092869 
0.1468670172  0.0558632066  0.8996738019  0.8661174047  0.8639175258  0.9613402062  0.8533791523  0.9909909910  0.9627959414  0.9885255307  0.9265212400  19.773429454  1200          1.3520163321 
0.1115427484  0.0484220156  0.9853155589  0.8645726056  0.8639175258  0.9693585338  0.8568155785  0.9932432432  0.9673055242  0.9911072863  0.9265212400  24.716786817  1500          1.3528439442 
0.1130629636  0.0430786606  0.9807932756  0.8671472709  0.8618556701  0.9707903780  0.8579610538  0.9929617117  0.9627959414  0.9913941480  0.9230769231  24.716786817  1500          1.3512685776 
0.1107740633  0.0400605916  0.9802968866  0.8656024717  0.8597938144  0.9676403207  0.8568155785  0.9926801802  0.9684329200  0.9916810098  0.9242250287  24.716786817  1500          1.3516914940 
0.0846852552  0.0320998947  1.0276736218  0.8656024717  0.8618556701  0.9753722795  0.8602520046  0.9943693694  0.9661781285  0.9934021801  0.9242250287  29.660144181  1800          1.3545820276 
0.0889138000  0.0368491630  1.0273861106  0.8640576725  0.8515463918  0.9742268041  0.8671248568  0.9943693694  0.9695603157  0.9934021801  0.9219288175  29.660144181  1800          1.3581103373 
0.0880324457  0.0343619188  1.0266081170  0.8630278064  0.8659793814  0.9756586483  0.8579610538  0.9946509009  0.9684329200  0.9939759036  0.9242250287  29.660144181  1800          1.3543913857 
0.0655380891  0.0306991896  1.0495410132  0.8650875386  0.8639175258  0.9802405498  0.8613974800  0.9952139640  0.9661781285  0.9948364888  0.9219288175  34.603501544  2100          1.3589432478 
0.0666866094  0.0303801257  1.0490410278  0.8630278064  0.8515463918  0.9785223368  0.8648339061  0.9943693694  0.9695603157  0.9951233505  0.9230769231  34.603501544  2100          1.3618151522 
0.0660448585  0.0316142008  1.0453894987  0.8635427394  0.8639175258  0.9785223368  0.8602520046  0.9954954955  0.9695603157  0.9939759036  0.9219288175  34.603501544  2100          1.3613154427 
0.0523668744  0.0254141522  1.0589233371  0.8640576725  0.8659793814  0.9825315006  0.8613974800  0.9960585586  0.9684329200  0.9956970740  0.9196326062  39.546858908  2400          1.3619867984 
0.0535776726  0.0265960127  1.0635780458  0.8635427394  0.8536082474  0.9808132875  0.8671248568  0.9952139640  0.9695603157  0.9959839357  0.9219288175  39.546858908  2400          1.3610051910 
0.0526564747  0.0278105989  1.0613698747  0.8650875386  0.8618556701  0.9819587629  0.8625429553  0.9957770270  0.9661781285  0.9948364888  0.9230769231  39.546858908  2400          1.3603191956 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.8711194992  0.6576356292  1.1742914915  0.8383110196  0.8391752577  0.6892898053  0.6838487973  0.9039977477  0.8940248027  0.9016064257  0.9173363949  0.0000000000  0             2.0956151485 
0.0430241130  0.0248601605  1.0697550521  0.8661174047  0.8618556701  0.9833906071  0.8648339061  0.9963400901  0.9650507328  0.9954102123  0.9230769231  44.490216271  2700          1.3578432639 
0.0430481411  0.0210318915  1.0659870428  0.8645726056  0.8659793814  0.9842497136  0.8602520046  0.9963400901  0.9684329200  0.9959839357  0.9184845006  44.490216271  2700          1.3610105371 
0.2014788019  0.1928070939  0.6133286900  0.9732234809  0.8969072165  0.7388316151  0.7205040092  0.9763513514  0.9458850056  0.9701663798  0.9391504018  4.9433573635  300           1.3588208262 
0.0343265087  0.0212005387  1.0732943108  0.8645726056  0.8618556701  0.9848224513  0.8648339061  0.9966216216  0.9661781285  0.9962707975  0.9207807118  49.433573635  3000          1.3574935691 
0.0354190658  0.0214075266  1.0747522974  0.8656024717  0.8659793814  0.9845360825  0.8602520046  0.9969031532  0.9695603157  0.9962707975  0.9207807118  49.433573635  3000          1.3595590766 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.7419704199  0.4673906267  1.1697407961  0.8295571576  0.8329896907  0.6852806415  0.6827033219  0.8972409910  0.8838782413  0.8958691910  0.9024110218  0.0000000000  0             2.0879235268 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
0.6458772421  0.4412072003  1.1540125608  0.8290422245  0.8268041237  0.6852806415  0.6781214204  0.8949887387  0.8838782413  0.8970166380  0.9047072331  0.0000000000  0             5.0518627167 
0.0904629510  0.0762664174  0.7361041321  0.9881565396  0.9072164948  0.7482817869  0.7353951890  0.9856418919  0.9582863585  0.9839357430  0.9357060850  9.8867147271  600           1.3618114940 
0.2078621204  0.1931555315  0.5878329236  0.9742533471  0.8886597938  0.7479954181  0.7319587629  0.9771959459  0.9481397971  0.9707401033  0.9322617681  4.9433573635  300           1.3510522779 
0.2084478733  0.1967608690  0.5688213731  0.9809474768  0.8886597938  0.7339633448  0.7342497136  0.9760698198  0.9413754228  0.9710269650  0.9334098737  4.9433573635  300           1.3539440171 
0.0716145391  0.0486926503  0.8632084244  0.9938208033  0.9113402062  0.7502863688  0.7353951890  0.9926801802  0.9627959414  0.9896729776  0.9357060850  14.830072090  900           1.3530648168 
0.0998137795  0.0742419717  0.7205591596  0.9866117405  0.8969072165  0.7459908362  0.7388316151  0.9867680180  0.9537767756  0.9856569134  0.9345579793  9.8867147271  600           1.3515596604 
0.0923479019  0.0709184769  0.7402892147  0.9907312049  0.8989690722  0.7419816724  0.7399770905  0.9890202703  0.9526493799  0.9850831899  0.9380022962  9.8867147271  600           1.3537843593 
0.0581856621  0.0375588615  0.9806814826  0.9938208033  0.9113402062  0.7534364261  0.7365406644  0.9949324324  0.9639233371  0.9922547332  0.9334098737  19.773429454  1200          1.3532235686 
0.0704736194  0.0534875509  0.8678790389  0.9933058702  0.9030927835  0.7502863688  0.7411225659  0.9932432432  0.9627959414  0.9913941480  0.9357060850  14.830072090  900           1.3601284750 
0.0708511289  0.0508041889  0.8674331985  0.9927909372  0.9010309278  0.7465635739  0.7376861397  0.9932432432  0.9560315671  0.9913941480  0.9288174512  14.830072090  900           1.3575761779 
0.0491121504  0.0321400481  1.0239759525  0.9943357364  0.9134020619  0.7565864834  0.7388316151  0.9954954955  0.9639233371  0.9942627653  0.9345579793  24.716786817  1500          1.3604734143 
0.0541502201  0.0373576950  0.9749644677  0.9948506694  0.9092783505  0.7500000000  0.7434135166  0.9954954955  0.9650507328  0.9934021801  0.9334098737  19.773429454  1200          1.3587501582 
0.0627293421  0.0394338457  0.9732469992  0.9948506694  0.9030927835  0.7488545246  0.7422680412  0.9943693694  0.9582863585  0.9931153184  0.9265212400  19.773429454  1200          1.3571126072 
0.0338717693  0.0268836229  1.0485197906  0.9958805355  0.9092783505  0.7554410080  0.7365406644  0.9960585586  0.9684329200  0.9948364888  0.9345579793  29.660144181  1800          1.3602617836 
0.0406805817  0.0290773355  1.0349025826  0.9948506694  0.9092783505  0.7525773196  0.7411225659  0.9957770270  0.9639233371  0.9951233505  0.9357060850  24.716786817  1500          1.3647380710 
0.0439552366  0.0339074377  1.0246234290  0.9963954686  0.9010309278  0.7491408935  0.7445589920  0.9952139640  0.9616685457  0.9936890419  0.9288174512  24.716786817  1500          1.3607463503 
0.0216024391  0.0175461144  1.0633929882  0.9958805355  0.9072164948  0.7565864834  0.7319587629  0.9966216216  0.9695603157  0.9959839357  0.9322617681  34.603501544  2100          1.3636214471 
0.0333828103  0.0230996023  1.0475446628  0.9963954686  0.8989690722  0.7517182131  0.7502863688  0.9954954955  0.9639233371  0.9951233505  0.9334098737  29.660144181  1800          1.3596743170 
0.0353925589  0.0245639514  1.0524398184  0.9963954686  0.9134020619  0.7577319588  0.7434135166  0.9969031532  0.9616685457  0.9951233505  0.9357060850  29.660144181  1800          1.3669904820 
0.0211272075  0.0224941939  1.0620862516  0.9958805355  0.9113402062  0.7540091638  0.7388316151  0.9966216216  0.9684329200  0.9968445209  0.9311136625  39.546858908  2400          1.3709974694 
0.0245854812  0.0240309146  1.0613119606  0.9963954686  0.8948453608  0.7528636884  0.7457044674  0.9960585586  0.9661781285  0.9956970740  0.9322617681  34.603501544  2100          1.3608151166 
0.0319772155  0.0234411454  1.0616206511  0.9963954686  0.9154639175  0.7568728522  0.7399770905  0.9969031532  0.9650507328  0.9956970740  0.9345579793  34.603501544  2100          1.4028522142 
0.0198543229  0.0203245170  1.0686088741  0.9963954686  0.9154639175  0.7508591065  0.7388316151  0.9969031532  0.9695603157  0.9971313827  0.9334098737  44.490216271  2700          1.3913358959 
0.0225653589  0.0179299692  1.0613638977  0.9963954686  0.8948453608  0.7540091638  0.7457044674  0.9963400901  0.9661781285  0.9962707975  0.9322617681  39.546858908  2400          1.3595713274 
0.0226185447  0.0170642000  1.0633285149  0.9963954686  0.9113402062  0.7565864834  0.7388316151  0.9974662162  0.9650507328  0.9965576592  0.9311136625  39.546858908  2400          1.4160248510 
0.0167195525  0.0144153327  1.0671350177  0.9963954686  0.9175257732  0.7494272623  0.7399770905  0.9969031532  0.9684329200  0.9971313827  0.9345579793  49.433573635  3000          1.7739983479 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 1
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0195554089  0.0167877849  1.0675555511  0.9963954686  0.8948453608  0.7545819015  0.7445589920  0.9969031532  0.9661781285  0.9962707975  0.9334098737  44.490216271  2700          1.5368982347 
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
1.2367696762  0.9436724782  1.1779503822  0.8424304840  0.8412371134  0.7073310424  0.7021764032  0.9023085586  0.8917700113  0.8984509466  0.9150401837  0.0000000000  0             4.9859936237 
0.2530952898  0.3106334575  0.4177538633  0.9737384140  0.8927835052  0.9095074456  0.8350515464  0.9279279279  0.9244644870  0.9701663798  0.9288174512  4.9433573635  300           2.1290670379 
0.0222226377  0.0174714954  1.0652007850  0.9963954686  0.8948453608  0.7551546392  0.7457044674  0.9974662162  0.9673055242  0.9968445209  0.9322617681  49.433573635  3000          2.3247187694 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         dloss_anchor  env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         step          step_time    
1.1167805195  0.8228256702  1.1788181067  0.8321318229  0.8371134021  0.7004581901  0.6975945017  0.9009009009  0.8827508455  0.8990246701  0.9058553387  0.0000000000  0             2.0967817307 
0.1109375287  0.1121976565  0.7436321810  0.9897013388  0.8969072165  0.9438717068  0.8533791523  0.9282094595  0.9267192785  0.9856569134  0.9322617681  9.8867147271  600           1.4472794135 
0.2555475159  0.3112919509  0.4192089546  0.9763130793  0.8907216495  0.9120847652  0.8304696449  0.9259572072  0.9210822999  0.9684452094  0.9299655568  4.9433573635  300           1.3531779512 
0.0803406835  0.0763266356  0.8738183808  0.9927909372  0.9092783505  0.9579037801  0.8579610538  0.9265202703  0.9289740699  0.9893861159  0.9380022962  14.830072090  900           1.4404852716 
0.1129247810  0.1131159023  0.7487341188  0.9881565396  0.8969072165  0.9453035510  0.8522336770  0.9293355856  0.9222096956  0.9847963282  0.9299655568  9.8867147271  600           1.3529529246 
0.0665638135  0.0624597149  0.9568995295  0.9948506694  0.9154639175  0.9713631157  0.8636884307  0.9287725225  0.9278466742  0.9922547332  0.9334098737  19.773429454  1200          1.4517354004 
0.0822498273  0.0761907358  0.8772091542  0.9927909372  0.9051546392  0.9642038946  0.8568155785  0.9265202703  0.9244644870  0.9905335628  0.9265212400  14.830072090  900           1.3591384228 
0.0498338144  0.0459567661  1.0102333273  0.9963954686  0.9154639175  0.9773768614  0.8671248568  0.9270833333  0.9255918828  0.9934021801  0.9334098737  24.716786817  1500          1.4473514795 
0.0672551537  0.0605914614  0.9627020794  0.9943357364  0.9113402062  0.9722222222  0.8648339061  0.9237049550  0.9188275085  0.9919678715  0.9253731343  19.773429454  1200          1.3573996798 
0.0414931521  0.0496728571  1.0434721253  0.9963954686  0.9175257732  0.9793814433  0.8625429553  0.9259572072  0.9233370913  0.9939759036  0.9311136625  29.660144181  1800          1.4499924771 
0.0546075744  0.0533432169  1.0124783369  0.9953656025  0.9154639175  0.9756586483  0.8671248568  0.9194819820  0.9177001127  0.9936890419  0.9230769231  24.716786817  1500          1.3547112703 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANCoral
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1180, in update
    domain_features = [self.featurizer(x_anchor, mask=True) for x_anchor in all_x_anchor]
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1180, in <listcomp>
    domain_features = [self.featurizer(x_anchor, mask=True) for x_anchor in all_x_anchor]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: VisionTransformer.forward() missing 1 required positional argument: 'z'
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANCoral
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1180, in update
    domain_features = [self.featurizer(x_anchor, mask=True) for x_anchor in all_x_anchor]
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1180, in <listcomp>
    domain_features = [self.featurizer(x_anchor, mask=True) for x_anchor in all_x_anchor]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: VisionTransformer.forward() missing 1 required positional argument: 'z'
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANCoral
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1180, in update
    domain_features = [self.featurizer(x_anchor, mask=True) for x_anchor in all_x_anchor]
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1180, in <listcomp>
    domain_features = [self.featurizer(x_anchor, mask=True) for x_anchor in all_x_anchor]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: VisionTransformer.forward() missing 1 required positional argument: 'z'
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANCoral
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1180, in update
    domain_features = [self.featurizer(x_anchor, mask=True) for x_anchor in all_x_anchor]
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 1180, in <listcomp>
    domain_features = [self.featurizer(x_anchor, mask=True) for x_anchor in all_x_anchor]
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: VisionTransformer.forward() missing 1 required positional argument: 'z'
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANCoral
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <module>
    train_loaders = [InfiniteDataLoader(
                    ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <listcomp>
    train_loaders = [InfiniteDataLoader(
                     ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/fast_data_loader.py", line 35, in __init__
    self._infinite_iterator = iter(torch.utils.data.DataLoader(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 442, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1016, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/queues.py", line 49, in __init__
    self._sem = ctx.BoundedSemaphore(maxsize)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 88, in BoundedSemaphore
    return BoundedSemaphore(value, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 145, in __init__
    SemLock.__init__(self, SEMAPHORE, value, value, ctx=ctx)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANCoral
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <module>
    train_loaders = [InfiniteDataLoader(
                    ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <listcomp>
    train_loaders = [InfiniteDataLoader(
                     ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/fast_data_loader.py", line 35, in __init__
    self._infinite_iterator = iter(torch.utils.data.DataLoader(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 442, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1016, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/queues.py", line 49, in __init__
    self._sem = ctx.BoundedSemaphore(maxsize)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 88, in BoundedSemaphore
    return BoundedSemaphore(value, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 145, in __init__
    SemLock.__init__(self, SEMAPHORE, value, value, ctx=ctx)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANCoral
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <module>
    train_loaders = [InfiniteDataLoader(
                    ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <listcomp>
    train_loaders = [InfiniteDataLoader(
                     ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/fast_data_loader.py", line 35, in __init__
    self._infinite_iterator = iter(torch.utils.data.DataLoader(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 442, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1016, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/queues.py", line 49, in __init__
    self._sem = ctx.BoundedSemaphore(maxsize)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 88, in BoundedSemaphore
    return BoundedSemaphore(value, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 145, in __init__
    SemLock.__init__(self, SEMAPHORE, value, value, ctx=ctx)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANCoral
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <module>
    train_loaders = [InfiniteDataLoader(
                    ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <listcomp>
    train_loaders = [InfiniteDataLoader(
                     ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/fast_data_loader.py", line 35, in __init__
    self._infinite_iterator = iter(torch.utils.data.DataLoader(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 442, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1016, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/queues.py", line 49, in __init__
    self._sem = ctx.BoundedSemaphore(maxsize)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 88, in BoundedSemaphore
    return BoundedSemaphore(value, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 145, in __init__
    SemLock.__init__(self, SEMAPHORE, value, value, ctx=ctx)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANCoral
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <module>
    train_loaders = [InfiniteDataLoader(
                    ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <listcomp>
    train_loaders = [InfiniteDataLoader(
                     ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/fast_data_loader.py", line 35, in __init__
    self._infinite_iterator = iter(torch.utils.data.DataLoader(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 442, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1016, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/queues.py", line 49, in __init__
    self._sem = ctx.BoundedSemaphore(maxsize)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 88, in BoundedSemaphore
    return BoundedSemaphore(value, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 145, in __init__
    SemLock.__init__(self, SEMAPHORE, value, value, ctx=ctx)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANCoral
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <module>
    train_loaders = [InfiniteDataLoader(
                    ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <listcomp>
    train_loaders = [InfiniteDataLoader(
                     ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/fast_data_loader.py", line 35, in __init__
    self._infinite_iterator = iter(torch.utils.data.DataLoader(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 442, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1016, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/queues.py", line 49, in __init__
    self._sem = ctx.BoundedSemaphore(maxsize)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 88, in BoundedSemaphore
    return BoundedSemaphore(value, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 145, in __init__
    SemLock.__init__(self, SEMAPHORE, value, value, ctx=ctx)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANCoral
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <module>
    train_loaders = [InfiniteDataLoader(
                    ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <listcomp>
    train_loaders = [InfiniteDataLoader(
                     ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/fast_data_loader.py", line 35, in __init__
    self._infinite_iterator = iter(torch.utils.data.DataLoader(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 442, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1016, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/queues.py", line 49, in __init__
    self._sem = ctx.BoundedSemaphore(maxsize)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 88, in BoundedSemaphore
    return BoundedSemaphore(value, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 145, in __init__
    SemLock.__init__(self, SEMAPHORE, value, value, ctx=ctx)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANCoral
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <module>
    train_loaders = [InfiniteDataLoader(
                    ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <listcomp>
    train_loaders = [InfiniteDataLoader(
                     ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/fast_data_loader.py", line 35, in __init__
    self._infinite_iterator = iter(torch.utils.data.DataLoader(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 442, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1016, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/queues.py", line 49, in __init__
    self._sem = ctx.BoundedSemaphore(maxsize)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 88, in BoundedSemaphore
    return BoundedSemaphore(value, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 145, in __init__
    SemLock.__init__(self, SEMAPHORE, value, value, ctx=ctx)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANCoral
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <module>
    train_loaders = [InfiniteDataLoader(
                    ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <listcomp>
    train_loaders = [InfiniteDataLoader(
                     ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/fast_data_loader.py", line 35, in __init__
    self._infinite_iterator = iter(torch.utils.data.DataLoader(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 442, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1016, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/queues.py", line 49, in __init__
    self._sem = ctx.BoundedSemaphore(maxsize)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 88, in BoundedSemaphore
    return BoundedSemaphore(value, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 145, in __init__
    SemLock.__init__(self, SEMAPHORE, value, value, ctx=ctx)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANCoral
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <module>
    train_loaders = [InfiniteDataLoader(
                    ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <listcomp>
    train_loaders = [InfiniteDataLoader(
                     ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/fast_data_loader.py", line 35, in __init__
    self._infinite_iterator = iter(torch.utils.data.DataLoader(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 442, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1016, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/queues.py", line 49, in __init__
    self._sem = ctx.BoundedSemaphore(maxsize)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 88, in BoundedSemaphore
    return BoundedSemaphore(value, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 145, in __init__
    SemLock.__init__(self, SEMAPHORE, value, value, ctx=ctx)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANCoral
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <module>
    train_loaders = [InfiniteDataLoader(
                    ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <listcomp>
    train_loaders = [InfiniteDataLoader(
                     ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/fast_data_loader.py", line 35, in __init__
    self._infinite_iterator = iter(torch.utils.data.DataLoader(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 442, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1016, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/queues.py", line 49, in __init__
    self._sem = ctx.BoundedSemaphore(maxsize)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 88, in BoundedSemaphore
    return BoundedSemaphore(value, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 145, in __init__
    SemLock.__init__(self, SEMAPHORE, value, value, ctx=ctx)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANCoral
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <module>
    train_loaders = [InfiniteDataLoader(
                    ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 154, in <listcomp>
    train_loaders = [InfiniteDataLoader(
                     ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/fast_data_loader.py", line 35, in __init__
    self._infinite_iterator = iter(torch.utils.data.DataLoader(
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 442, in __iter__
    return self._get_iterator()
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 388, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1016, in __init__
    self._worker_result_queue = multiprocessing_context.Queue()  # type: ignore[var-annotated]
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 103, in Queue
    return Queue(maxsize, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/queues.py", line 49, in __init__
    self._sem = ctx.BoundedSemaphore(maxsize)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/context.py", line 88, in BoundedSemaphore
    return BoundedSemaphore(value, ctx=self.get_context())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 145, in __init__
    SemLock.__init__(self, SEMAPHORE, value, value, ctx=ctx)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/synchronize.py", line 57, in __init__
    sl = self._semlock = _multiprocessing.SemLock(
                         ^^^^^^^^^^^^^^^^^^^^^^^^^
OSError: [Errno 28] No space left on device
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANCoral
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: OfficeHome
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
bloss         closs         env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         penalty       step          step_time    
0.8585159183  0.6326794028  0.8197734295  0.8329896907  0.6967353952  0.6918671249  0.9039977477  0.8895152198  0.8961560528  0.9104477612  0.0000000000  0.0285329130  0             6.9391305447 
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSANCoral
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 2
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
0.0363414708  0.0353138516  1.0514190847  0.9963954686  0.9216494845  0.9825315006  0.8625429553  0.9242680180  0.9222096956  0.9948364888  0.9288174512  34.603501544  2100          1.4477746479 
0.0391662513  0.0386747498  1.0405156251  0.9958805355  0.9175257732  0.9793814433  0.8659793814  0.9180743243  0.9177001127  0.9945496271  0.9219288175  29.660144181  1800          1.3571440967 
bloss         closs         env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         penalty       step          step_time    
0.6583386064  0.6876096129  0.9973498233  1.0000000000  0.6870876532  0.6867924528  0.8092155369  0.7774390244  0.8704183636  0.8844444444  0.0000000000  0.0301514156  0             7.6981024742 
0.2579759261  0.3063726124  0.8707518023  0.8742268041  0.9115120275  0.8270332188  0.9766328829  0.9515219842  0.9713138267  0.9288174512  4.9433573635  0.0111867515  300           1.3661774397 
0.1568586676  0.2414010578  0.9946996466  1.0000000000  0.9090480679  0.8264150943  0.9504950495  0.8734756098  0.9689004073  0.9377777778  8.4805653710  0.0162099637  300           1.3700977151 
0.0296902660  0.0334834322  1.0610722568  0.9963954686  0.9257731959  0.9851088202  0.8579610538  0.9231418919  0.9233370913  0.9951233505  0.9311136625  39.546858908  2400          1.4521578892 
0.1177556062  0.1124128969  0.8707518023  0.8762886598  0.9453035510  0.8522336770  0.9870495495  0.9571589628  0.9873780838  0.9322617681  9.8867147271  0.0085299640  600           1.3622749925 
0.0658429857  0.0560303065  0.9885159011  0.9964664311  0.9552309142  0.8339622642  0.9763899467  0.8826219512  0.9859311366  0.9392592593  16.961130742  0.0093635004  600           1.3689784519 
0.0738683797  0.0666594943  0.8722966014  0.8804123711  0.9642038946  0.8613974800  0.9912725225  0.9616685457  0.9919678715  0.9311136625  14.830072090  0.0074760011  900           1.3621604276 
0.0370663571  0.0199123973  0.9849823322  0.9964664311  0.9750235627  0.8301886792  0.9889565880  0.8856707317  0.9929655683  0.9392592593  25.441696113  0.0066944628  900           1.3863025236 
                                                                                                                                                                                                    0.0526094147  0.0493246014  0.8722966014  0.8824742268  0.9719358534  0.8717067583  0.9938063063  0.9650507328  0.9936890419  0.9311136625  19.773429454  0.0065983578  1200          1.3620262504 
0.0264995249  0.0123447589  0.9849823322  0.9964664311  0.9853911404  0.8301886792  0.9961919269  0.8841463415  0.9962976675  0.9377777778  33.922261484  0.0052844213  1200          1.3693207304 
