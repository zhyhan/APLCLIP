./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.001
	lr_g: 0.0001
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 1024
	momentum: 0.1
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "cause the presence of"
Number of context words (tokens): 4
ViT-B/16
Using ViT-B/16...
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         lr            step          step_time    
5.3034548759  1.0944803953  1.0000000000  1.0000000000  0.6666666667  0.6716981132  0.7535545024  0.7530487805  0.7982889108  0.8278931751  0.0000000000  0.0001000000  0             2.4177317619 
4.3740994811  0.4043687685  0.9921507064  0.9787234043  0.6624843162  0.6792452830  0.8913337847  0.8750000000  0.9269496545  0.9317507418  7.5353218210  0.0000999238  300           1.2952340110 
4.3515930955  0.5708016276  0.9968602826  1.0000000000  0.6637390213  0.6679245283  0.9116452268  0.8902439024  0.9210266535  0.9080118694  15.070643642  0.0000994763  600           1.3037350416 
4.0702257554  0.5148619156  0.9968602826  1.0000000000  0.6700125471  0.6641509434  0.9333107651  0.8993902439  0.9437314906  0.9139465875  22.605965463  0.0000985896  900           1.2129653804 
4.1673177338  0.6929981557  0.9960753532  0.9929078014  0.6750313676  0.6679245283  0.9512525389  0.9176829268  0.9631457716  0.9287833828  30.141287284  0.0000972718  1200          1.2160028346 
4.2384053135  0.6979218783  0.9882260597  0.9858156028  0.6679213718  0.6566037736  0.9583615437  0.9054878049  0.9713721619  0.9169139466  37.676609105  0.0000955343  1500          1.2157279793 
4.1753529374  0.7450033881  1.0000000000  1.0000000000  0.6687578419  0.6641509434  0.9712254570  0.9146341463  0.9759789404  0.9198813056  45.211930926  0.0000933928  1800          1.2123583372 
4.0993049526  0.6802535807  0.9945054945  0.9929078014  0.6683396069  0.6566037736  0.9793500339  0.9146341463  0.9845343863  0.9287833828  52.747252747  0.0000908662  2100          1.2160004656 
4.0109781726  0.6819460603  0.9890109890  0.9858156028  0.6608113760  0.6528301887  0.9857819905  0.9115853659  0.9878249424  0.9228486647  60.282574568  0.0000879768  2400          1.2143075720 
4.1204060300  0.6996069967  0.9952904239  0.9929078014  0.6762860728  0.6716981132  0.9908598510  0.9146341463  0.9897992761  0.9258160237  67.817896389  0.0000847504  2700          1.2421445910 
4.1583116484  0.7509971491  0.9945054945  1.0000000000  0.6758678377  0.6603773585  0.9918754232  0.9207317073  0.9921026654  0.9287833828  75.353218210  0.0000812157  3000          1.2147404075 
3.9977027337  0.7238928022  0.9992150706  1.0000000000  0.6687578419  0.6641509434  0.9908598510  0.9207317073  0.9950641658  0.9228486647  82.888540031  0.0000774039  3300          1.2240914957 
4.0175573810  0.7117222146  0.9992150706  1.0000000000  0.6633207863  0.6528301887  0.9928909953  0.9054878049  0.9957222771  0.9317507418  90.423861852  0.0000733489  3600          1.2152115321 
4.1406448221  0.7340372799  0.9984301413  1.0000000000  0.6783772480  0.6716981132  0.9939065674  0.8902439024  0.9973675551  0.9198813056  97.959183673  0.0000690867  3900          1.2125380723 
4.0037373598  0.7346873027  1.0000000000  1.0000000000  0.6787954831  0.6641509434  0.9959377116  0.9085365854  0.9980256663  0.9198813056  105.49450549  0.0000646551  4200          1.2131371681 
4.0066957156  0.7489572773  0.9945054945  0.9929078014  0.6775407779  0.6566037736  0.9966147596  0.9085365854  0.9980256663  0.9228486647  113.02982731  0.0000600935  4500          1.2239116637 
4.1582481003  0.7568937689  0.9960753532  0.9929078014  0.6716854872  0.6528301887  0.9983073798  0.9085365854  0.9986837776  0.9198813056  120.56514913  0.0000554423  4800          1.2127991859 
4.0842366672  0.7516009113  1.0000000000  1.0000000000  0.6746131326  0.6566037736  0.9979688558  0.8993902439  0.9976966107  0.9287833828  125.58869701  0.0000515280  5000          1.2153772771 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.001
	lr_g: 0.0001
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 1024
	momentum: 0.1
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "cause the presence of"
Number of context words (tokens): 4
ViT-B/16
Using ViT-B/16...
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         lr            step          step_time    
5.2439346313  1.0934036970  1.0000000000  1.0000000000  0.7193642827  0.7584905660  0.7119160460  0.7103658537  0.8012504113  0.8427299703  0.0000000000  0.0001000000  0             2.4911582470 
4.3986340618  0.2890144570  0.9992150706  1.0000000000  0.8243412798  0.8301886792  0.7911306703  0.7560975610  0.9081934847  0.9347181009  7.5353218210  0.0000999238  300           1.2140533121 
4.2949399996  0.4854974401  0.9968602826  0.9929078014  0.8456712673  0.8415094340  0.8050101557  0.7530487805  0.9276077657  0.9376854599  15.070643642  0.0000994763  600           1.2148978845 
4.0493644301  0.5315613523  0.9866562009  0.9858156028  0.8306148055  0.8000000000  0.8419092756  0.8231707317  0.9364922672  0.9347181009  22.605965463  0.0000985896  900           1.2461005147 
4.3419404030  0.6218763288  0.9984301413  1.0000000000  0.8644918444  0.8301886792  0.8249830738  0.8140243902  0.9499835472  0.9406528190  30.141287284  0.0000972718  1200          1.2565143188 
4.1926894108  0.6407147191  0.9968602826  0.9929078014  0.8590547888  0.8113207547  0.8385240352  0.8231707317  0.9575518263  0.9406528190  37.676609105  0.0000955343  1500          1.2581225212 
4.0769355838  0.6570669763  0.9968602826  0.9929078014  0.9054788791  0.8339622642  0.8077183480  0.7774390244  0.9664363277  0.9317507418  45.211930926  0.0000933928  1800          1.2505966393 
4.0454710690  0.6394404021  0.9976452119  0.9929078014  0.9188624007  0.8226415094  0.8094109682  0.7896341463  0.9759789404  0.9376854599  52.747252747  0.0000908662  2100          1.2606221143 
4.1217164342  0.6507503060  0.9976452119  1.0000000000  0.9422835634  0.8377358491  0.8080568720  0.8018292683  0.9766370517  0.9376854599  60.282574568  0.0000879768  2400          1.2148410376 
4.0137386751  0.6572047044  0.9992150706  1.0000000000  0.9510664994  0.8377358491  0.7965470548  0.7865853659  0.9884830536  0.9347181009  67.817896389  0.0000847504  2700          1.2163528204 
4.0392117771  0.7064286568  0.9984301413  1.0000000000  0.9677959013  0.8339622642  0.7975626269  0.7713414634  0.9881539980  0.9376854599  75.353218210  0.0000812157  3000          1.2158265297 
4.1369301812  0.7230878526  0.9968602826  1.0000000000  0.9824341280  0.8339622642  0.7951929587  0.7621951220  0.9874958868  0.9228486647  82.888540031  0.0000774039  3300          1.2152026677 
4.0304388332  0.7424810676  0.9976452119  0.9929078014  0.9891258887  0.8339622642  0.8019634394  0.7774390244  0.9947351102  0.9406528190  90.423861852  0.0000733489  3600          1.2157382472 
4.1364333439  0.7477249451  0.9984301413  1.0000000000  0.9887076537  0.8188679245  0.8060257278  0.7682926829  0.9934188878  0.9376854599  97.959183673  0.0000690867  3900          1.2138233511 
4.0763064210  0.7142669471  1.0000000000  0.9929078014  0.9949811794  0.8150943396  0.8083953961  0.7804878049  0.9960513327  0.9376854599  105.49450549  0.0000646551  4200          1.2158896486 
3.9966896264  0.7391859476  0.9992150706  1.0000000000  0.9920535341  0.8264150943  0.8148273527  0.7926829268  0.9973675551  0.9436201780  113.02982731  0.0000600935  4500          1.2122952469 
3.9217879645  0.7169470253  0.9992150706  1.0000000000  0.9958176495  0.8150943396  0.8060257278  0.7774390244  0.9976966107  0.9287833828  120.56514913  0.0000554423  4800          1.2149362826 
4.0754001880  0.7642639840  0.9984301413  1.0000000000  0.9970723547  0.8188679245  0.8023019634  0.7652439024  0.9976966107  0.9436201780  125.58869701  0.0000515280  5000          1.2124041772 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.001
	lr_g: 0.0001
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 1024
	momentum: 0.1
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "cause the presence of"
Number of context words (tokens): 4
ViT-B/16
Using ViT-B/16...
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         lr            step          step_time    
5.1894483566  1.0900083780  1.0000000000  1.0000000000  0.7122542869  0.7509433962  0.7237643873  0.7256097561  0.7811780191  0.7982195846  0.0000000000  0.0001000000  0             2.5857665539 
4.4194358746  0.3349169236  0.9992150706  1.0000000000  0.8180677541  0.7962264151  0.8896411645  0.9054878049  0.8680487002  0.8991097923  7.5353218210  0.0000999238  300           1.2239612548 
4.2134950368  0.4672086894  0.9905808477  0.9858156028  0.8373065663  0.7924528302  0.9116452268  0.9115853659  0.8282329714  0.8516320475  15.070643642  0.0000994763  600           1.2370892445 
4.0193107351  0.4799718714  0.9992150706  1.0000000000  0.8272689251  0.7735849057  0.9285714286  0.9207317073  0.8565317539  0.8753709199  22.605965463  0.0000985896  900           1.2257722060 
4.0426072868  0.4876861322  0.9984301413  1.0000000000  0.8745294856  0.8188679245  0.9383886256  0.9054878049  0.8404080290  0.8575667656  30.141287284  0.0000972718  1200          1.2258011778 
4.2646797768  0.6285110638  0.9937205651  1.0000000000  0.9012965286  0.8037735849  0.9519295870  0.9176829268  0.8486344192  0.8605341246  37.676609105  0.0000955343  1500          1.2266673851 
4.1008849923  0.6382195683  0.9803767661  0.9716312057  0.9142618151  0.8226415094  0.9556533514  0.9054878049  0.8354721948  0.8516320475  45.211930926  0.0000933928  1800          1.2268010052 
4.0520061874  0.6904644093  0.9960753532  1.0000000000  0.9418653283  0.8264150943  0.9691943128  0.8993902439  0.8022375782  0.8308605341  52.747252747  0.0000908662  2100          1.2278877171 
4.0560599724  0.6877237219  0.9960753532  0.9929078014  0.9615223756  0.8301886792  0.9783344617  0.8963414634  0.8361303060  0.8664688427  60.282574568  0.0000879768  2400          1.2283865984 
4.0265686385  0.7177031906  0.9984301413  1.0000000000  0.9347553325  0.7962264151  0.9790115098  0.8993902439  0.8269167489  0.8635014837  67.817896389  0.0000847504  2700          1.2272505530 
3.9948870436  0.7226064600  0.9976452119  1.0000000000  0.9615223756  0.8037735849  0.9871360867  0.9146341463  0.8265876933  0.8605341246  75.353218210  0.0000812157  3000          1.2286602004 
3.9756847636  0.7142301691  0.9984301413  1.0000000000  0.9744876621  0.7962264151  0.9949221395  0.9115853659  0.8440276407  0.8664688427  82.888540031  0.0000774039  3300          1.2297414986 
3.9254251544  0.7332784775  0.9960753532  1.0000000000  0.9836888331  0.8301886792  0.9952606635  0.9024390244  0.8351431392  0.8456973294  90.423861852  0.0000733489  3600          1.2288017813 
4.2024839894  0.7364171714  0.9968602826  1.0000000000  0.9916352990  0.8075471698  0.9976303318  0.8932926829  0.8265876933  0.8575667656  97.959183673  0.0000690867  3900          1.2286099450 
3.9904078849  0.7266550308  0.9992150706  0.9929078014  0.9907988289  0.8188679245  0.9969532837  0.8932926829  0.8213228036  0.8486646884  105.49450549  0.0000646551  4200          1.2277742179 
3.8700705322  0.7259617994  0.9874411303  0.9929078014  0.9878711836  0.7698113208  0.9928909953  0.8902439024  0.8292201382  0.8516320475  113.02982731  0.0000600935  4500          1.2276088818 
4.0613303789  0.7303411750  0.9976452119  0.9929078014  0.9945629444  0.8000000000  0.9972918077  0.8963414634  0.8160579138  0.8456973294  120.56514913  0.0000554423  4800          1.2259101264 
4.1289851165  0.7440560144  0.9945054945  0.9929078014  0.9958176495  0.8037735849  0.9979688558  0.8963414634  0.8134254689  0.8338278932  125.58869701  0.0000515280  5000          1.2255607533 
