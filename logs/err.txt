WARNING:root:The WILDS package is out of date. Your version is 1.2.2, while the latest version is 2.0.0.
/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py:187: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  torch.nn.init.xavier_uniform(m.weight)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 235, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 213, in update
    _mean_domain_features = [feature.repeat_interleave(len(self.hparams['class_names']), dim=0) for feature in mean_domain_features]
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 213, in <listcomp>
    _mean_domain_features = [feature.repeat_interleave(len(self.hparams['class_names']), dim=0) for feature in mean_domain_features]
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
WARNING:root:The WILDS package is out of date. Your version is 1.2.2, while the latest version is 2.0.0.
WARNING:root:The WILDS package is out of date. Your version is 1.2.2, while the latest version is 2.0.0.
WARNING:root:The WILDS package is out of date. Your version is 1.2.2, while the latest version is 2.0.0.
WARNING:root:The WILDS package is out of date. Your version is 1.2.2, while the latest version is 2.0.0.
/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py:189: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  torch.nn.init.xavier_uniform(m.weight)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 235, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 215, in update
    _mean_domain_features = [feature.repeat_interleave(len(self.hparams['class_names']), dim=0) for feature in mean_domain_features]
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 215, in <listcomp>
    _mean_domain_features = [feature.repeat_interleave(len(self.hparams['class_names']), dim=0) for feature in mean_domain_features]
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py:190: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.
  torch.nn.init.xavier_uniform(m.weight)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 235, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 228, in update
    loss.backward()
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
WARNING:root:The WILDS package is out of date. Your version is 1.2.2, while the latest version is 2.0.0.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 183, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 157, in __init__
    super(DPLCLIP, self).__init__(input_shape, num_classes, num_domains, hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 132, in __init__
    self.clip_model = clip.load(self.hparams['clip_backbone'])[0].float()
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/clip.py", line 139, in load
    model = build_model(state_dict or model.state_dict()).to(device)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 434, in build_model
    convert_weights(model)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 396, in convert_weights
    model.apply(_convert_weights_to_fp16)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 884, in apply
    module.apply(fn)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 884, in apply
    module.apply(fn)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 884, in apply
    module.apply(fn)
  [Previous line repeated 3 more times]
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 885, in apply
    fn(self)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 380, in _convert_weights_to_fp16
    l.weight.data = l.weight.data.half()
                    ^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 229, in <module>
    for x,y in next(train_minibatches_iterator)]
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/fast_data_loader.py", line 43, in __iter__
    yield next(self._infinite_iterator)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 634, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1329, in _next_data
    idx, data = self._get_data()
                ^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1295, in _get_data
    success, data = self._try_get_data()
                    ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1133, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/queues.py", line 122, in get
    return _ForkingPickler.loads(res)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/multiprocessing/reductions.py", line 307, in rebuild_storage_fd
    fd = df.detach()
         ^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/resource_sharer.py", line 86, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/connection.py", line 507, in Client
    answer_challenge(c, authkey)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/connection.py", line 751, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/connection.py", line 215, in recv_bytes
    buf = self._recv_bytes(maxlength)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/connection.py", line 413, in _recv_bytes
    buf = self._recv(4)
          ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/connection.py", line 378, in _recv
    chunk = read(handle, remaining)
            ^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
WARNING:root:The WILDS package is out of date. Your version is 1.2.2, while the latest version is 2.0.0.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 228, in <module>
    minibatches_device = [(x.to(device), y.to(device))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 228, in <listcomp>
    minibatches_device = [(x.to(device), y.to(device))
                           ^^^^^^^^^^^^
KeyboardInterrupt
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 235, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 228, in update
    loss.backward()
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 235, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 208, in update
    print(image_features.shape())
          ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'list' object has no attribute 'shape'
WARNING:root:The WILDS package is out of date. Your version is 1.2.2, while the latest version is 2.0.0.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 235, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 213, in update
    mean_domain_features = [feature.mean(dim=0, keepdim=True) for feature in domain_features]
          ^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'torch.Size' object is not callable
WARNING:root:The WILDS package is out of date. Your version is 1.2.2, while the latest version is 2.0.0.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 252, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/misc.py", line 115, in accuracy
    for x, y in loader:
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/fast_data_loader.py", line 70, in __iter__
    yield next(self._infinite_iterator)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 634, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1329, in _next_data
    idx, data = self._get_data()
                ^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1295, in _get_data
    success, data = self._try_get_data()
                    ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1133, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/queues.py", line 113, in get
    if not self._poll(timeout):
           ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/connection.py", line 256, in poll
    return self._poll(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/connection.py", line 423, in _poll
    r = wait([self], timeout)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/connection.py", line 930, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
WARNING:root:The WILDS package is out of date. Your version is 1.2.2, while the latest version is 2.0.0.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 252, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/misc.py", line 118, in accuracy
    p = network.predict(x)
        ^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 255, in predict
    mean_domain_feature = torch.mean(domain_feature, dim=0, keepdim=True).repeat_interleave(len(self.hparams['class_names']), dim=0)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
WARNING:root:The WILDS package is out of date. Your version is 1.2.2, while the latest version is 2.0.0.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 252, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/misc.py", line 115, in accuracy
    for x, y in loader:
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/fast_data_loader.py", line 70, in __iter__
    yield next(self._infinite_iterator)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 634, in __next__
    data = self._next_data()
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1329, in _next_data
    idx, data = self._get_data()
                ^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1295, in _get_data
    success, data = self._try_get_data()
                    ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/utils/data/dataloader.py", line 1133, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/queues.py", line 113, in get
    if not self._poll(timeout):
           ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/connection.py", line 256, in poll
    return self._poll(timeout)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/connection.py", line 423, in _poll
    r = wait([self], timeout)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/multiprocessing/connection.py", line 930, in wait
    ready = selector.select(timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 190, in <module>
    algorithm.to(device)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1145, in to
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 797, in _apply
    module._apply(fn)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 820, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1143, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 338, in update
    return {'disc_loss': disc_loss.item()}
                         ^^^^^^^^^^^^^^^^
KeyboardInterrupt
WARNING:root:The WILDS package is out of date. Your version is 1.2.2, while the latest version is 2.0.0.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 307, in update
    all_z = self.featurizer(all_x)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/networks.py", line 256, in forward
    return self.clip_model.encode_image(x)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 341, in encode_image
    return self.visual(image.type(self.dtype))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 232, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 203, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 191, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 222.00 MiB (GPU 0; 23.62 GiB total capacity; 5.79 GiB already allocated; 106.25 MiB free; 5.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 307, in update
    all_z = self.featurizer(all_x)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/networks.py", line 256, in forward
    return self.clip_model.encode_image(x)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 341, in encode_image
    return self.visual(image.type(self.dtype))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 232, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 203, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 191, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 222.00 MiB (GPU 0; 23.62 GiB total capacity; 5.79 GiB already allocated; 106.25 MiB free; 5.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
WARNING:root:The WILDS package is out of date. Your version is 1.2.2, while the latest version is 2.0.0.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 110, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 240, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 214, in __init__
    clip_transform = clip.load(hparams['clip_backbone'])[1]
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/clip.py", line 129, in load
    model = torch.jit.load(opened_file, map_location=device if jit else "cpu").eval()
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/jit/_serialization.py", line 165, in load
    cu, f.read(), map_location, _extra_files, _restore_shapes
        ^^^^^^^^
KeyboardInterrupt
/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
WARNING:root:The WILDS package is out of date. Your version is 1.2.2, while the latest version is 2.0.0.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 253, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/misc.py", line 124, in accuracy
    batch_weights = batch_weights.to(device)
                    ^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 110, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 240, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 213, in __init__
    print('Using clip_transform', hparams['clip_backbone'])
                                  ~~~~~~~^^^^^^^^^^^^^^^^^
KeyError: 'clip_backbone'
Exception ignored in: <module 'threading' from '/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/threading.py'>
Traceback (most recent call last):
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/threading.py", line 1583, in _shutdown
    lock.acquire()
KeyboardInterrupt: 
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 110, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 240, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 213, in __init__
    print('Using clip_transform', hparams['clip_backbone'])
                                  ~~~~~~~^^^^^^^^^^^^^^^^^
KeyError: 'clip_backbone'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 307, in update
    all_z = self.featurizer(all_x)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/networks.py", line 256, in forward
    return self.clip_model.encode_image(x)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 341, in encode_image
    return self.visual(image.type(self.dtype))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 232, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 203, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 191, in forward
    x = x + self.mlp(self.ln_2(x))
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 222.00 MiB (GPU 0; 23.62 GiB total capacity; 5.79 GiB already allocated; 105.75 MiB free; 5.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 183, in <module>
    algorithm_class = algorithms.get_algorithm_class(args.algorithm)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 40, in get_algorithm_class
    raise NotImplementedError("Algorithm not found: {}".format(algorithm_name))
NotImplementedError: Algorithm not found: CLPS
WARNING:root:The WILDS package is out of date. Your version is 1.2.2, while the latest version is 2.0.0.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 164, in __init__
    self.prompt_generator = CoCoOpPromptLearner()
                            ^^^^^^^^^^^^^^^^^^^^^
TypeError: CoCoOpPromptLearner.__init__() missing 4 required positional arguments: 'input_shape', 'num_classes', 'num_domains', and 'hparams'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 164, in __init__
    self.prompt_generator = CoCoOpPromptLearner(input_shape, num_classes, num_domains, hparams)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 228, in __init__
    super(DPLCLIP, self).__init__(input_shape, num_classes, num_domains, hparams)
    ^^^^^^^^^^^^^^^^^^^^
TypeError: super(type, obj): obj must be an instance or subtype of type
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 166, in __init__
    self.dtype = self.clip.dtype
                 ^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'CoCoOpCLIP' object has no attribute 'clip'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 166, in __init__
    self.dtype = self.clip.dtype
                 ^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'CoCoOpCLIP' object has no attribute 'clip'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 169, in __init__
    [self.prompt_generator.meta_net.parameters(), self.prompt_generator.ctx.parameters()],
                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Parameter' object has no attribute 'parameters'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 171, in __init__
    momentum=self.hparams["momentum"]
             ~~~~~~~~~~~~^^^^^^^^^^^^
KeyError: 'momentum'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 83, in <module>
    hparams = hparams_registry.default_hparams(args.algorithm, args.dataset)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/hparams_registry.py", line 147, in default_hparams
    _hparams(algorithm, dataset, 0).items()}
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/hparams_registry.py", line 107, in _hparams
    _hparam('weight_decay', 0., lambda r: 0.)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/hparams_registry.py", line 21, in _hparam
    assert(name not in hparams)
           ^^^^^^^^^^^^^^^^^^^
AssertionError
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 168, in __init__
    self.optimizer = torch.optim.SGD(
                     ^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/sgd.py", line 27, in __init__
    super().__init__(params, defaults)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 192, in __init__
    self.add_param_group(param_group)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 512, in add_param_group
    raise TypeError("optimizer can only optimize Tensors, "
TypeError: optimizer can only optimize Tensors, but one of the params is Module.parameters
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 237, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 175, in update
    all_x = [data[0].cuda().float() for data in minibatches]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 175, in <listcomp>
    all_x = [data[0].cuda().float() for data in minibatches]
             ^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/cuda/__init__.py", line 247, in _lazy_init
    torch._C._cuda_init()
RuntimeError: No CUDA GPUs are available
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 168, in __init__
    self.optimizer = torch.optim.SGD(
                     ^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/sgd.py", line 27, in __init__
    super().__init__(params, defaults)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 192, in __init__
    self.add_param_group(param_group)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 512, in add_param_group
    raise TypeError("optimizer can only optimize Tensors, "
TypeError: optimizer can only optimize Tensors, but one of the params is torch.nn.modules.container.Sequential
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 169, in __init__
    (list(self.prompt_generator.meta_net.parameters()) + self.prompt_generator.ctx),
     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~
TypeError: can only concatenate list (not "Parameter") to list
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 168, in __init__
    self.optimizer = torch.optim.SGD(
                     ^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/sgd.py", line 27, in __init__
    super().__init__(params, defaults)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 192, in __init__
    self.add_param_group(param_group)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 515, in add_param_group
    raise ValueError("can't optimize a non-leaf Tensor")
ValueError: can't optimize a non-leaf Tensor
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 169, in __init__
    self.optimizer = torch.optim.SGD(
                     ^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/sgd.py", line 27, in __init__
    super().__init__(params, defaults)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 192, in __init__
    self.add_param_group(param_group)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 515, in add_param_group
    raise ValueError("can't optimize a non-leaf Tensor")
ValueError: can't optimize a non-leaf Tensor
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 169, in __init__
    self.optimizer = torch.optim.SGD(
                     ^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/sgd.py", line 27, in __init__
    super().__init__(params, defaults)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 192, in __init__
    self.add_param_group(param_group)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 515, in add_param_group
    raise ValueError("can't optimize a non-leaf Tensor")
ValueError: can't optimize a non-leaf Tensor
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 169, in __init__
    self.prompt_generator.meta_net.parameters().requires_grad = True
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'generator' object has no attribute 'requires_grad'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 170, in __init__
    self.optimizer = torch.optim.SGD(
                     ^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/sgd.py", line 27, in __init__
    super().__init__(params, defaults)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 192, in __init__
    self.add_param_group(param_group)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 515, in add_param_group
    raise ValueError("can't optimize a non-leaf Tensor")
ValueError: can't optimize a non-leaf Tensor
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 175, in __init__
    self.optimizer = torch.optim.SGD(
                     ^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/sgd.py", line 27, in __init__
    super().__init__(params, defaults)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 192, in __init__
    self.add_param_group(param_group)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 512, in add_param_group
    raise TypeError("optimizer can only optimize Tensors, "
TypeError: optimizer can only optimize Tensors, but one of the params is Module.parameters
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 175, in __init__
    self.optimizer = torch.optim.SGD(
                     ^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/sgd.py", line 27, in __init__
    super().__init__(params, defaults)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 192, in __init__
    self.add_param_group(param_group)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 512, in add_param_group
    raise TypeError("optimizer can only optimize Tensors, "
TypeError: optimizer can only optimize Tensors, but one of the params is Module.parameters
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 164, in __init__
    self.prompt_generator = CoCoOpPromptLearner(input_shape, num_classes, num_domains, hparams)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 260, in __init__
    self.meta_net = nn.Sequential([
                    ^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 104, in __init__
    self.add_module(str(idx), module)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 596, in add_module
    raise TypeError("{} is not a Module subclass".format(
TypeError: list is not a Module subclass
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 164, in __init__
    self.prompt_generator = CoCoOpPromptLearner(input_shape, num_classes, num_domains, hparams)
                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 260, in __init__
    self.meta_net = nn.Sequential(
                    ^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 104, in __init__
    self.add_module(str(idx), module)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 596, in add_module
    raise TypeError("{} is not a Module subclass".format(
TypeError: tuple is not a Module subclass
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 237, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 182, in update
    all_x = [data[0].self.device().float() for data in minibatches]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 182, in <listcomp>
    all_x = [data[0].self.device().float() for data in minibatches]
             ^^^^^^^^^^^^
AttributeError: 'Tensor' object has no attribute 'self'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 170, in __init__
    self.optimizer = torch.optim.SGD(
                     ^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/sgd.py", line 27, in __init__
    super().__init__(params, defaults)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 192, in __init__
    self.add_param_group(param_group)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 515, in add_param_group
    raise ValueError("can't optimize a non-leaf Tensor")
ValueError: can't optimize a non-leaf Tensor
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 170, in __init__
    self.optimizer = torch.optim.SGD(
                     ^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/sgd.py", line 27, in __init__
    super().__init__(params, defaults)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 192, in __init__
    self.add_param_group(param_group)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 515, in add_param_group
    raise ValueError("can't optimize a non-leaf Tensor")
ValueError: can't optimize a non-leaf Tensor
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 175, in __init__
    self.optimizer = torch.optim.SGD(
                     ^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/sgd.py", line 27, in __init__
    super().__init__(params, defaults)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 178, in __init__
    raise TypeError("params argument given to the optimizer should be "
TypeError: params argument given to the optimizer should be an iterable of Tensors or dicts, but got torch.FloatTensor
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 237, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 182, in update
    all_x = [data[0].self.device().float() for data in minibatches]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 182, in <listcomp>
    all_x = [data[0].self.device().float() for data in minibatches]
             ^^^^^^^^^^^^
AttributeError: 'Tensor' object has no attribute 'self'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 237, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 182, in update
    all_x = [data[0].self.device().float() for data in minibatches]
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 182, in <listcomp>
    all_x = [data[0].self.device().float() for data in minibatches]
             ^^^^^^^^^^^^
AttributeError: 'Tensor' object has no attribute 'self'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 237, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 192, in update
    ctx = ctx.permute(1, 0, 2, 3)
          ^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 3 is not equal to len(dims) = 4
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 254, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/misc.py", line 118, in accuracy
    p = network.predict(x)
        ^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 227, in predict
    domain_feature = self.network(image_feature)
                     ^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1614, in __getattr__
    raise AttributeError("'{}' object has no attribute '{}'".format(
AttributeError: 'CoCoOpCLIP' object has no attribute 'network'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 254, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/misc.py", line 118, in accuracy
    p = network.predict(x)
        ^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 223, in predict
    image_feature = self.clip_model.encode_image(x)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 341, in encode_image
    return self.visual(image.type(self.dtype))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 232, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 203, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 190, in forward
    x = x + self.attention(self.ln_1(x))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 187, in attention
    return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 1189, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/functional.py", line 5188, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/functional.py", line 4765, in _in_projection_packed
    proj = linear(q, w, b)
           ^^^^^^^^^^^^^^^
KeyboardInterrupt
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 240, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 213, in __init__
    print('Using clip_transform', hparams['clip_backbone'])
                                  ~~~~~~~^^^^^^^^^^^^^^^^^
KeyError: 'clip_backbone'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 240, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 213, in __init__
    print('Using clip_transform', hparams['clip_backbone'])
                                  ~~~~~~~^^^^^^^^^^^^^^^^^
KeyError: 'clip_backbone'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 240, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 213, in __init__
    print('Using clip_transform', hparams['clip_backbone'])
                                  ~~~~~~~^^^^^^^^^^^^^^^^^
KeyError: 'clip_backbone'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 184, in <module>
    algorithm_class = algorithms.get_algorithm_class(args.algorithm)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 46, in get_algorithm_class
    raise NotImplementedError("Algorithm not found: {}".format(algorithm_name))
NotImplementedError: Algorithm not found: APLCLIP
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 185, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 114, in __init__
    momentum=self.hparams["momentum"]
             ~~~~~~~~~~~~^^^^^^^^^^^^
KeyError: 'momentum'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 237, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 127, in update
    all_acl = torch.zeros(all_y.size[0], 3, 512)
                          ~~~~~~~~~~^^^
TypeError: 'builtin_function_or_method' object is not subscriptable
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 237, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 136, in update
    acl_features = torch.cat([all_acl, image_features.unsqueeze(dim=1)], dim = 1)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 237, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 140, in update
    ctx = self.prompt_generator(acl_features, ctx_only=True)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 296, in forward
    bias = torch.cat(acps, dim=1)      # (batch, n_ctx, ctx_dim)
           ^^^^^^^^^^^^^^^^^^^^^^
TypeError: expected Tensor as element 0 in argument 0, but got list
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 237, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 149, in update
    prompts = torch.cat([
              ^^^^^^^^^^^
RuntimeError: Sizes of tensors must match except in dimension 2. Expected size 1 but got size 96 for tensor number 1 in the list.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 237, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 149, in update
    prompts = torch.cat([
              ^^^^^^^^^^^
RuntimeError: Sizes of tensors must match except in dimension 2. Expected size 1 but got size 96 for tensor number 1 in the list.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 254, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/misc.py", line 118, in accuracy
    p = network.predict(x)
        ^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 176, in predict
    all_acl = torch.zeros(x.size[0], 3, 512)
                          ~~~~~~^^^
TypeError: 'builtin_function_or_method' object is not subscriptable
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 254, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/misc.py", line 118, in accuracy
    p = network.predict(x)
        ^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 177, in predict
    acl_features = torch.cat([all_acl, image_feature.unsqueeze(dim=1)], dim = 1)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 254, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/misc.py", line 118, in accuracy
    p = network.predict(x)
        ^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 177, in predict
    acl_features = torch.cat([all_acl, image_feature.unsqueeze(dim=1)], dim = 1).cuda()
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 237, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 128, in update
    all_acl = torch.cat([data[2].cuda().float() for data in minibatches])
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 128, in <listcomp>
    all_acl = torch.cat([data[2].cuda().float() for data in minibatches])
                         ~~~~^^^
IndexError: tuple index out of range
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 237, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 124, in update
    print(minibatches[0].size())
          ^^^^^^^^^^^^^^^^^^^
AttributeError: 'tuple' object has no attribute 'size'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 237, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 129, in update
    all_acl = torch.cat([data[2].cuda().float() for data in minibatches])
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 129, in <listcomp>
    all_acl = torch.cat([data[2].cuda().float() for data in minibatches])
                         ~~~~^^^
IndexError: tuple index out of range
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 230, in <module>
    minibatches_device = [(x.to(device), y.to(device), z.to(device))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 231, in <listcomp>
    for x,y,z in next(train_minibatches_iterator)]
        ^^^^^
ValueError: not enough values to unpack (expected 3, got 2)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 230, in <module>
    minibatches_device = [(x.to(device), y.to(device), z.to(device))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 231, in <listcomp>
    for x,y,z in next(train_minibatches_iterator)]
        ^^^^^
ValueError: not enough values to unpack (expected 3, got 2)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 230, in <module>
    minibatches_device = [(x.to(device), y.to(device), z.to(device))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 231, in <listcomp>
    for x,y,z in next(train_minibatches_iterator)]
        ^^^^^
ValueError: not enough values to unpack (expected 3, got 2)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 230, in <module>
    minibatches_device = [(x.to(device), y.to(device), z.to(device))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 231, in <listcomp>
    for x,y,z in next(train_minibatches_iterator)]
        ^^^^^
ValueError: not enough values to unpack (expected 3, got 2)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 230, in <module>
    minibatches_device = [(x.to(device), y.to(device), z.to(device))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 231, in <listcomp>
    for x,y,z in next(train_minibatches_iterator)]
        ^^^^^
ValueError: not enough values to unpack (expected 3, got 2)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 230, in <module>
    minibatches_device = [(x.to(device), y.to(device), z.to(device))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 231, in <listcomp>
    for x,y,z in next(train_minibatches_iterator)]
        ^^^^^
ValueError: not enough values to unpack (expected 3, got 2)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 230, in <module>
    minibatches_device = [(x.to(device), y.to(device), z.to(device))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 231, in <listcomp>
    for x,y,z in next(train_minibatches_iterator)]
        ^^^^^
ValueError: not enough values to unpack (expected 3, got 2)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 229, in <module>
    minibatches_device = [(x.to(device), y.to(device), z.to(device))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 230, in <listcomp>
    for x,y,z in next(train_minibatches_iterator)]
        ^^^^^
ValueError: not enough values to unpack (expected 3, got 2)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 229, in <module>
    minibatches_device = [(x.to(device), y.to(device), z.to(device))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 230, in <listcomp>
    for x,y,z in next(train_minibatches_iterator)]
        ^^^^^
ValueError: not enough values to unpack (expected 3, got 2)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 229, in <module>
    minibatches_device = [(x.to(device), y.to(device), z.to(device))
                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 230, in <listcomp>
    for x,y,z in next(train_minibatches_iterator)]
        ^^^^^
ValueError: not enough values to unpack (expected 3, got 2)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 242, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 229, in __init__
    env_dataset = ImageFolder(path, #这里可能需要更改 
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 323, in __init__
    super().__init__(
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 161, in __init__
    samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 204, in make_dataset
    return make_dataset(directory, class_to_idx, extensions=extensions, is_valid_file=is_valid_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 93, in make_dataset
    prompts_disc = np.load(prompts_path)
                   ^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/numpy/lib/npyio.py", line 432, in load
    return format.read_array(fid, allow_pickle=allow_pickle,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/numpy/lib/format.py", line 787, in read_array
    raise ValueError("Object arrays cannot be loaded when "
ValueError: Object arrays cannot be loaded when allow_pickle=False
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 242, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 229, in __init__
    env_dataset = ImageFolder(path, #这里可能需要更改 
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 324, in __init__
    super().__init__(
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 162, in __init__
    samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 205, in make_dataset
    return make_dataset(directory, class_to_idx, extensions=extensions, is_valid_file=is_valid_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 94, in make_dataset
    prompts_disc = np.load(prompts_path)
                   ^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/numpy/lib/npyio.py", line 432, in load
    return format.read_array(fid, allow_pickle=allow_pickle,
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/numpy/lib/format.py", line 787, in read_array
    raise ValueError("Object arrays cannot be loaded when "
ValueError: Object arrays cannot be loaded when allow_pickle=False
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 242, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 229, in __init__
    env_dataset = ImageFolder(path, #这里可能需要更改 
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 323, in __init__
    super().__init__(
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 161, in __init__
    samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 204, in make_dataset
    return make_dataset(directory, class_to_idx, extensions=extensions, is_valid_file=is_valid_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 94, in make_dataset
    scores = prompts_disc["score"]
             ~~~~~~~~~~~~^^^^^^^^^
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 242, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 229, in __init__
    env_dataset = ImageFolder(path, #这里可能需要更改 
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 324, in __init__
    super().__init__(
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 162, in __init__
    samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 205, in make_dataset
    return make_dataset(directory, class_to_idx, extensions=extensions, is_valid_file=is_valid_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 95, in make_dataset
    scores = prompts_disc["score"]
             ~~~~~~~~~~~~^^^^^^^^^
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 242, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 229, in __init__
    env_dataset = ImageFolder(path, #这里可能需要更改 
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 324, in __init__
    super().__init__(
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 162, in __init__
    samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 205, in make_dataset
    return make_dataset(directory, class_to_idx, extensions=extensions, is_valid_file=is_valid_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 94, in make_dataset
    print(prompts_disc.type)
          ^^^^^^^^^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'type'. Did you mean: 'dtype'?
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 242, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 229, in __init__
    env_dataset = ImageFolder(path, #这里可能需要更改 
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 324, in __init__
    super().__init__(
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 162, in __init__
    samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 205, in make_dataset
    return make_dataset(directory, class_to_idx, extensions=extensions, is_valid_file=is_valid_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 95, in make_dataset
    scores = prompts_disc["score"]
             ~~~~~~~~~~~~^^^^^^^^^
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 242, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 229, in __init__
    env_dataset = ImageFolder(path, #这里可能需要更改 
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 325, in __init__
    super().__init__(
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 163, in __init__
    samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 206, in make_dataset
    return make_dataset(directory, class_to_idx, extensions=extensions, is_valid_file=is_valid_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 94, in make_dataset
    prompts_disc = dict(prompts_disc)
                   ^^^^^^^^^^^^^^^^^^
TypeError: iteration over a 0-d array
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 242, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 229, in __init__
    env_dataset = ImageFolder(path, #这里可能需要更改 
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 324, in __init__
    super().__init__(
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 162, in __init__
    samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 205, in make_dataset
    return make_dataset(directory, class_to_idx, extensions=extensions, is_valid_file=is_valid_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 95, in make_dataset
    scores = prompts_disc.get('score')
             ^^^^^^^^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'get'
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 242, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 229, in __init__
    env_dataset = ImageFolder(path, #这里可能需要更改 
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 324, in __init__
    super().__init__(
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 162, in __init__
    samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 205, in make_dataset
    return make_dataset(directory, class_to_idx, extensions=extensions, is_valid_file=is_valid_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 96, in make_dataset
    feature = prompts_disc.item().get['feature']
              ~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^
TypeError: 'builtin_function_or_method' object is not subscriptable
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 242, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 229, in __init__
    env_dataset = ImageFolder(path, #这里可能需要更改 
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 324, in __init__
    super().__init__(
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 162, in __init__
    samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 205, in make_dataset
    return make_dataset(directory, class_to_idx, extensions=extensions, is_valid_file=is_valid_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 100, in make_dataset
    prompts = feature[large_index, :]
              ~~~~~~~^^^^^^^^^^^^^^^^
IndexError: index 4 is out of bounds for axis 0 with size 4
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 242, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 229, in __init__
    env_dataset = ImageFolder(path, #这里可能需要更改 
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 328, in __init__
    super().__init__(
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 166, in __init__
    samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 209, in make_dataset
    return make_dataset(directory, class_to_idx, extensions=extensions, is_valid_file=is_valid_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 101, in make_dataset
    prompts[i,:] = feature[j, :]
                   ~~~~~~~^^^^^^
IndexError: index 4 is out of bounds for axis 0 with size 4
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 242, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 229, in __init__
    env_dataset = ImageFolder(path, #这里可能需要更改 
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 329, in __init__
    super().__init__(
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 167, in __init__
    samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 210, in make_dataset
    return make_dataset(directory, class_to_idx, extensions=extensions, is_valid_file=is_valid_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 102, in make_dataset
    prompts[i,:] = feature[j, :]
                   ~~~~~~~^^^^^^
IndexError: index 4 is out of bounds for axis 0 with size 4
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 242, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 229, in __init__
    env_dataset = ImageFolder(path, #这里可能需要更改 
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 332, in __init__
    super().__init__(
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 170, in __init__
    samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 213, in make_dataset
    return make_dataset(directory, class_to_idx, extensions=extensions, is_valid_file=is_valid_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 105, in make_dataset
    prompts[i,:] = feature[j, :]
                   ~~~~~~~^^^^^^
TypeError: list indices must be integers or slices, not tuple
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 242, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 229, in __init__
    env_dataset = ImageFolder(path, #这里可能需要更改 
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 332, in __init__
    super().__init__(
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 170, in __init__
    samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 213, in make_dataset
    return make_dataset(directory, class_to_idx, extensions=extensions, is_valid_file=is_valid_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 105, in make_dataset
    prompts[i,:] = feature[j, :]
                   ~~~~~~~^^^^^^
TypeError: list indices must be integers or slices, not tuple
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 111, in <module>
    dataset = vars(datasets)[args.dataset](args.data_dir, args.test_envs, hparams)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 242, in __init__
    super().__init__(self.dir, test_envs, hparams['data_augmentation'], hparams)
  File "/home/zhongyi.han/project/APLCLIP/domainbed/datasets.py", line 229, in __init__
    env_dataset = ImageFolder(path, #这里可能需要更改 
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 332, in __init__
    super().__init__(
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 170, in __init__
    samples = self.make_dataset(self.root, class_to_idx, extensions, is_valid_file)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 213, in make_dataset
    return make_dataset(directory, class_to_idx, extensions=extensions, is_valid_file=is_valid_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/folder.py", line 105, in make_dataset
    prompts[i,:] = feature[j, :]
                   ~~~~~~~^^^^^^
TypeError: list indices must be integers or slices, not tuple
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 253, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/misc.py", line 119, in accuracy
    p = network.predict(x, z)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 177, in predict
    prompts = self.prompt_generator(acl_features)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 296, in forward
    acp = self.meta_net(im_features[:, i, :]) 
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: mat1 and mat2 must have the same dtype
/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py:128: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  all_acl = torch.cat([torch.tensor(data[2]).cuda().float() for data in minibatches])
/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py:175: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  all_acl = torch.tensor(z).cuda()
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 253, in <module>
    acc = misc.accuracy(algorithm, loader, weights, device)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/lib/misc.py", line 119, in accuracy
    p = network.predict(x, z)
        ^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 177, in predict
    prompts = self.prompt_generator(acl_features)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 296, in forward
    acp = self.meta_net(im_features[:, i, :]) 
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: mat1 and mat2 must have the same dtype
/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  all_acl = torch.cat([torch.tensor(data[2]).cuda().type_as(image_features) for data in minibatches])
/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py:174: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  all_acl = torch.tensor(z).cuda().type_as(image_feature)
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 184, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 114, in __init__
    self.optimizer = torch.optim.SGD(
                     ^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/sgd.py", line 27, in __init__
    super().__init__(params, defaults)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 192, in __init__
    self.add_param_group(param_group)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 512, in add_param_group
    raise TypeError("optimizer can only optimize Tensors, "
TypeError: optimizer can only optimize Tensors, but one of the params is dict
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 184, in <module>
    algorithm = algorithm_class(dataset.input_shape, dataset.num_classes,
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 114, in __init__
    self.optimizer = torch.optim.SGD(
                     ^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/sgd.py", line 27, in __init__
    super().__init__(params, defaults)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 192, in __init__
    self.add_param_group(param_group)
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/optim/optimizer.py", line 512, in add_param_group
    raise TypeError("optimizer can only optimize Tensors, "
TypeError: optimizer can only optimize Tensors, but one of the params is list
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 156, in update
    text_features = self.text_encoder(tokenized_prompts, prompts)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 312, in forward
    x = prompts + self.positional_embedding.type(self.dtype)
        ~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
RuntimeError: The size of tensor a (77) must match the size of tensor b (512) at non-singleton dimension 1
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 156, in update
    text_features = self.text_encoder(prompts, tokenized_prompts)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 312, in forward
    x = self.positional_embedding.type(self.dtype) + prompts
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~
RuntimeError: The size of tensor a (77) must match the size of tensor b (4) at non-singleton dimension 1
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 156, in update
    text_features = self.text_encoder(prompts, tokenized_prompts)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 314, in forward
    x = self.transformer(x)
        ^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 203, in forward
    return self.resblocks(x)
           ^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 190, in forward
    x = x + self.attention(self.ln_1(x))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/clip/model.py", line 187, in attention
    return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/activation.py", line 1189, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/functional.py", line 5215, in multi_head_attention_forward
    raise RuntimeError(f"The shape of the 2D attn_mask is {attn_mask.shape}, but should be {correct_2d_size}.")
RuntimeError: The shape of the 2D attn_mask is torch.Size([77, 77]), but should be (4, 4).
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 152, in update
    prompts = torch.cat([
              ^^^^^^^^^^^
RuntimeError: Sizes of tensors must match except in dimension 2. Expected size 96 but got size 5 for tensor number 1 in the list.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 164, in update
    text_features = text_features[:, all_y, :] #dimension = [batchsize x domains, class_num, 512]
                    ~~~~~~~~~~~~~^^^^^^^^^^^^^
IndexError: too many indices for tensor of dimension 2
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 164, in update
    true_class_text_feature = torch.cat([text_features[:,i,:] for i in all_y])
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 164, in <listcomp>
    true_class_text_feature = torch.cat([text_features[:,i,:] for i in all_y])
                                         ~~~~~~~~~~~~~^^^^^^^
IndexError: too many indices for tensor of dimension 2
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 164, in update
    true_class_text_feature = torch.cat([text_features[:,i,:] for i in all_y])
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 164, in <listcomp>
    true_class_text_feature = torch.cat([text_features[:,i,:] for i in all_y])
                                         ~~~~~~~~~~~~~^^^^^^^
IndexError: too many indices for tensor of dimension 2
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 169, in update
    disc_input = self.grl(disc_input)
                 ^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/networks.py", line 100, in forward
    coeff = np.float(
            ^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/numpy/__init__.py", line 305, in __getattr__
    raise AttributeError(__former_attrs__[attr])
AttributeError: module 'numpy' has no attribute 'float'.
`np.float` was a deprecated alias for the builtin `float`. To avoid this error in existing code, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.
The aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:
    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations. Did you mean: 'cfloat'?
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 170, in update
    disc_out = self.discriminator(disc_input)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
           ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
           ^^^^^^^^^^^^^^^^^
RuntimeError: running_mean should contain 96 elements not 1024
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 171, in update
    disc_out = self.discriminator(disc_input)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
           ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
           ^^^^^^^^^^^^^^^^^
RuntimeError: running_mean should contain 96 elements not 1024
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 171, in update
    disc_out = self.discriminator(disc_input)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py", line 171, in forward
    return F.batch_norm(
           ^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/functional.py", line 2450, in batch_norm
    return torch.batch_norm(
           ^^^^^^^^^^^^^^^^^
RuntimeError: running_mean should contain 96 elements not 1024
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 168, in update
    disc_input = image_features + true_class_text_feature
                 ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~
RuntimeError: The size of tensor a (96) must match the size of tensor b (9216) at non-singleton dimension 0
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 169, in update
    disc_input = image_features + true_class_text_feature
                 ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~
RuntimeError: The size of tensor a (96) must match the size of tensor b (9216) at non-singleton dimension 0
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 169, in update
    disc_input = image_features + true_class_text_feature
                 ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~
RuntimeError: The size of tensor a (96) must match the size of tensor b (9216) at non-singleton dimension 0
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 168, in update
    disc_input = image_features + true_class_text_feature
                 ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~
RuntimeError: The size of tensor a (96) must match the size of tensor b (9216) at non-singleton dimension 0
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 169, in update
    disc_input = image_features + true_class_text_feature
                 ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~
RuntimeError: The size of tensor a (96) must match the size of tensor b (9216) at non-singleton dimension 0
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 166, in update
    true_class_text_feature = torch.cat([true_class_text_feature[i,all_y[i],:] for i in range(true_class_text_feature.size())])
                                                                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: 'torch.Size' object cannot be interpreted as an integer
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 169, in update
    disc_input = image_features + true_class_text_feature
                 ~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~
RuntimeError: The size of tensor a (512) must match the size of tensor b (49152) at non-singleton dimension 1
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 166, in update
    true_class_text_feature = torch.cat([true_class_text_feature[i,all_y[i],:].unsqueenze(0) for i in range(true_class_text_feature.size()[0])])
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 166, in <listcomp>
    true_class_text_feature = torch.cat([true_class_text_feature[i,all_y[i],:].unsqueenze(0) for i in range(true_class_text_feature.size()[0])])
                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
AttributeError: 'Tensor' object has no attribute 'unsqueenze'. Did you mean: 'unsqueeze'?
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/zhongyi.han/project/APLCLIP/domainbed/scripts/train.py", line 236, in <module>
    step_vals = algorithm.update(minibatches_device, uda_device)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/project/APLCLIP/domainbed/algorithms.py", line 177, in update
    disc_loss = F.cross_entropy(disc_out, disc_labels)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/functional.py", line 3029, in cross_entropy
    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument target in method wrapper_CUDA_nll_loss_forward)
