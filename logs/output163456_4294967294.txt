./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.001
	lr_g: 0.0001
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 1024
	momentum: 0.1
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "cause the presence of"
Number of context words (tokens): 4
ViT-B/16
Using ViT-B/16...
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         lr            step          step_time    
5.5962729454  1.0999903679  0.9960753532  1.0000000000  0.6486825596  0.6452830189  0.7332430603  0.7317073171  0.8285620270  0.8516320475  0.0000000000  0.0001000000  0             2.5691730976 
4.3414975389  0.3693323454  0.9992150706  1.0000000000  0.6675031368  0.6792452830  0.8869329722  0.8750000000  0.9154327081  0.9169139466  7.5353218210  0.0000999238  300           1.2122244978 
3.9635403744  0.4588032526  0.9968602826  0.9929078014  0.6783772480  0.6830188679  0.8974272173  0.8841463415  0.9285949325  0.9198813056  15.070643642  0.0000994763  600           1.2121910477 
4.0470454868  0.5128407980  0.9992150706  1.0000000000  0.6813048934  0.6867924528  0.9272173324  0.8810975610  0.9503126028  0.9347181009  22.605965463  0.0000985896  900           1.2124773383 
3.9769157823  0.5328000975  0.9992150706  1.0000000000  0.6762860728  0.6792452830  0.9549763033  0.9085365854  0.9644619941  0.9436201780  30.141287284  0.0000972718  1200          1.2129525558 
3.9658155076  0.5637270005  0.9992150706  1.0000000000  0.6750313676  0.6867924528  0.9634394042  0.8871951220  0.9759789404  0.9317507418  37.676609105  0.0000955343  1500          1.2126342988 
3.9481076813  0.5434005870  0.9976452119  1.0000000000  0.6838143036  0.6792452830  0.9722410291  0.8993902439  0.9809147746  0.9287833828  45.211930926  0.0000933928  1800          1.2131907121 
3.9885992813  0.5846425907  0.9992150706  1.0000000000  0.6787954831  0.6792452830  0.9871360867  0.8963414634  0.9911154985  0.9258160237  52.747252747  0.0000908662  2100          1.2134844764 
3.9158812968  0.5630761144  0.9960753532  0.9929078014  0.6729401924  0.6867924528  0.9800270819  0.8902439024  0.9904573873  0.9198813056  60.282574568  0.0000879768  2400          1.2130649122 
3.8944802539  0.5758879845  1.0000000000  1.0000000000  0.6813048934  0.6792452830  0.9769803656  0.8719512195  0.9888121092  0.9287833828  67.817896389  0.0000847504  2700          1.2114567749 
3.8479621172  0.5463898696  1.0000000000  1.0000000000  0.6909242995  0.6754716981  0.9878131347  0.8841463415  0.9884830536  0.9287833828  75.353218210  0.0000812157  3000          1.2132442474 
3.9862923686  0.5657280351  1.0000000000  0.9929078014  0.6813048934  0.6754716981  0.9911983751  0.9054878049  0.9970384995  0.9317507418  82.888540031  0.0000774039  3300          1.2134144354 
3.8433936532  0.5586049241  1.0000000000  0.9858156028  0.6725219573  0.6716981132  0.9884901828  0.8841463415  0.9937479434  0.9258160237  90.423861852  0.0000733489  3600          1.2134704113 
3.9653170395  0.5513521359  1.0000000000  1.0000000000  0.6670849017  0.6792452830  0.9935680433  0.9085365854  0.9970384995  0.9287833828  97.959183673  0.0000690867  3900          1.2488188720 
3.7332579708  0.5320281000  0.9992150706  1.0000000000  0.6700125471  0.6792452830  0.9908598510  0.8963414634  0.9967094439  0.9317507418  105.49450549  0.0000646551  4200          1.2134382312 
3.7736258936  0.5328481079  1.0000000000  1.0000000000  0.6704307821  0.6716981132  0.9908598510  0.8963414634  0.9930898322  0.9406528190  113.02982731  0.0000600935  4500          1.2245245727 
3.8293382374  0.5448018319  1.0000000000  1.0000000000  0.6796319532  0.6754716981  0.9962762356  0.8932926829  0.9976966107  0.9376854599  120.56514913  0.0000554423  4800          1.2148434560 
3.7688128090  0.5439263792  1.0000000000  1.0000000000  0.6884148892  0.6792452830  0.9884901828  0.8841463415  0.9937479434  0.9406528190  125.58869701  0.0000515280  5000          1.2135862088 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.001
	lr_g: 0.0001
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 1024
	momentum: 0.1
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "cause the presence of"
Number of context words (tokens): 4
ViT-B/16
Using ViT-B/16...
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         lr            step          step_time    
5.4785866737  1.0952688456  0.9960753532  1.0000000000  0.6896695943  0.7094339623  0.7102234259  0.6981707317  0.8246133597  0.8456973294  0.0000000000  0.0001000000  0             2.4352474213 
4.6420688979  0.4843794080  0.9929356358  0.9787234043  0.8021748223  0.8188679245  0.7995937712  0.7652439024  0.9095097071  0.9228486647  7.5353218210  0.0000999238  300           1.2129316759 
4.3845614179  0.6034802658  0.9945054945  1.0000000000  0.8306148055  0.8150943396  0.7840216655  0.7743902439  0.9170779862  0.9228486647  15.070643642  0.0000994763  600           1.2111874255 
4.3260053619  0.6816099309  0.9992150706  1.0000000000  0.8506900878  0.8264150943  0.8094109682  0.7774390244  0.9285949325  0.9139465875  22.605965463  0.0000985896  900           1.2101499160 
4.1537918313  0.6311474876  1.0000000000  1.0000000000  0.8582183187  0.8377358491  0.7880839540  0.7652439024  0.9420862126  0.9109792285  30.141287284  0.0000972718  1200          1.2227286577 
4.0807855527  0.6272879739  1.0000000000  1.0000000000  0.8385612714  0.8188679245  0.7654028436  0.7591463415  0.9358341560  0.9287833828  37.676609105  0.0000955343  1500          1.2103419518 
4.0642723099  0.6298759999  1.0000000000  1.0000000000  0.7954830615  0.7509433962  0.7274881517  0.6890243902  0.9183942086  0.9169139466  45.211930926  0.0000933928  1800          1.2131322845 
4.1586229293  0.6889698064  0.9992150706  1.0000000000  0.8887494772  0.8150943396  0.7782667569  0.7500000000  0.9582099375  0.9317507418  52.747252747  0.0000908662  2100          1.2141856352 
4.0533207210  0.6754258967  0.9984301413  1.0000000000  0.9422835634  0.8113207547  0.7985781991  0.7896341463  0.9848634419  0.9169139466  60.282574568  0.0000879768  2400          1.2119332377 
4.0684055630  0.6708294477  0.9992150706  1.0000000000  0.9565035550  0.8150943396  0.7931618145  0.7743902439  0.9743336624  0.9376854599  67.817896389  0.0000847504  2700          1.2143015869 
4.0284688123  0.6684386920  1.0000000000  1.0000000000  0.9096612296  0.7584905660  0.7494922139  0.7103658537  0.9598552155  0.9228486647  75.353218210  0.0000812157  3000          1.2142699035 
3.8736905622  0.6156285042  0.9968602826  1.0000000000  0.9820158929  0.8000000000  0.8043331077  0.7804878049  0.9819019414  0.9287833828  82.888540031  0.0000774039  3300          1.2144611343 
4.0813266468  0.6469155383  1.0000000000  1.0000000000  0.9740694270  0.7584905660  0.7823290454  0.7682926829  0.9858506088  0.9347181009  90.423861852  0.0000733489  3600          1.2135748704 
3.9012475808  0.6359658827  0.9976452119  1.0000000000  0.9912170640  0.8113207547  0.8182125931  0.7957317073  0.9917736097  0.9287833828  97.959183673  0.0000690867  3900          1.2135443894 
4.0621119928  0.6343190945  0.9984301413  1.0000000000  0.9949811794  0.7924528302  0.7982396750  0.7896341463  0.9944060546  0.9287833828  105.49450549  0.0000646551  4200          1.2135104354 
3.9368208663  0.6095293882  1.0000000000  1.0000000000  0.9895441238  0.7735849057  0.7914691943  0.7774390244  0.9937479434  0.9317507418  113.02982731  0.0000600935  4500          1.2141779462 
4.0365391246  0.6541192840  0.9992150706  1.0000000000  0.9933082392  0.7886792453  0.7982396750  0.7591463415  0.9950641658  0.9287833828  120.56514913  0.0000554423  4800          1.2133792734 
3.9408935452  0.6511054388  1.0000000000  1.0000000000  0.9958176495  0.8037735849  0.8009478673  0.7591463415  0.9934188878  0.9228486647  125.58869701  0.0000515280  5000          1.2136297071 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.001
	lr_g: 0.0001
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 1024
	momentum: 0.1
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "cause the presence of"
Number of context words (tokens): 4
ViT-B/16
Using ViT-B/16...
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         lr            step          step_time    
5.6068787575  1.0969921350  0.9945054945  0.9929078014  0.6783772480  0.7094339623  0.7227488152  0.7134146341  0.8091477460  0.8486646884  0.0000000000  0.0001000000  0             2.6034483910 
4.5038925489  0.3164654828  0.9897959184  0.9787234043  0.8046842325  0.8188679245  0.8591740014  0.8506097561  0.8134254689  0.8189910979  7.5353218210  0.0000999238  300           1.2119627674 
4.0577639039  0.4399599477  0.9984301413  1.0000000000  0.8327059808  0.8377358491  0.8953960731  0.8871951220  0.8650871997  0.9109792285  15.070643642  0.0000994763  600           1.2127510269 
4.1429630899  0.4754402443  0.9976452119  1.0000000000  0.8598912589  0.8528301887  0.9248476642  0.8902439024  0.8713392563  0.8961424332  22.605965463  0.0000985896  900           1.2127107310 
3.9995590687  0.4470845823  0.9992150706  1.0000000000  0.8724383103  0.8452830189  0.9380501016  0.9024390244  0.8295491938  0.8605341246  30.141287284  0.0000972718  1200          1.2119107866 
4.0652574205  0.4911864908  1.0000000000  1.0000000000  0.8820577164  0.8301886792  0.9458361544  0.8932926829  0.8325106943  0.8664688427  37.676609105  0.0000955343  1500          1.2100617639 
3.9789554882  0.4832119293  0.9976452119  1.0000000000  0.9176076955  0.8113207547  0.9654705484  0.8902439024  0.8496215860  0.8842729970  45.211930926  0.0000933928  1800          1.2130643066 
3.9558898338  0.5018594857  0.9984301413  1.0000000000  0.9410288582  0.8188679245  0.9752877454  0.9024390244  0.8397499177  0.8664688427  52.747252747  0.0000908662  2100          1.2116114505 
4.1178890292  0.5357085687  0.9976452119  1.0000000000  0.9502300293  0.8113207547  0.9800270819  0.9146341463  0.8506087529  0.8931750742  60.282574568  0.0000879768  2400          1.2106490699 
4.0694024324  0.5050844453  1.0000000000  1.0000000000  0.9611041405  0.7962264151  0.9820582261  0.8993902439  0.8446857519  0.8724035608  67.817896389  0.0000847504  2700          1.2144218262 
slurmstepd-gpu-02: error: *** JOB 163456 ON gpu-02 CANCELLED AT 2023-06-09T12:30:43 ***
