./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.001
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.2
	mlp_width: 512
	momentum: 0.1
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [0]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "cause the presence of"
Number of context words (tokens): 4
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         lr            step          step_time    
0.8879125118  1.0997791290  0.9858712716  0.9858156028  0.6206608114  0.5962264151  0.7606635071  0.7378048780  0.7499177361  0.7596439169  0.0000000000  0.0000100000  0             2.0790905952 
0.4152747368  0.4837949348  0.9976452119  1.0000000000  0.8197406943  0.8264150943  0.8842247800  0.8780487805  0.9151036525  0.9287833828  7.5353218210  0.0000099925  300           0.7426319170 
0.3024167334  0.3777325787  0.9968602826  1.0000000000  0.8368883312  0.8188679245  0.9163845633  0.8902439024  0.9434024350  0.9376854599  15.070643642  0.0000099481  600           0.7420902371 
0.2386805659  0.3614301937  0.9960753532  1.0000000000  0.8690924299  0.8301886792  0.9373730535  0.9176829268  0.9598552155  0.9376854599  22.605965463  0.0000098604  900           0.7428917766 
0.2033960522  0.3475704157  0.9937205651  0.9858156028  0.8732747804  0.8188679245  0.9536222072  0.8963414634  0.9753208292  0.9287833828  30.141287284  0.0000097299  1200          0.7435611947 
0.1553668603  0.3459536204  0.9960753532  1.0000000000  0.9155165203  0.8339622642  0.9661475965  0.9085365854  0.9802566634  0.9406528190  37.676609105  0.0000095579  1500          0.7455689692 
0.1305119478  0.3273228758  0.9960753532  0.9929078014  0.9163529904  0.8150943396  0.9773188896  0.9054878049  0.9878249424  0.9258160237  45.211930926  0.0000093458  1800          0.7443018421 
0.1064591457  0.3116826678  0.9952904239  1.0000000000  0.9569217900  0.8188679245  0.9878131347  0.9054878049  0.9947351102  0.9258160237  52.747252747  0.0000090957  2100          0.7431661240 
0.0746390316  0.2999515132  0.9945054945  0.9929078014  0.9686323714  0.8150943396  0.9928909953  0.9054878049  0.9960513327  0.9436201780  60.282574568  0.0000088096  2400          0.7457409588 
0.0575429157  0.2714876043  0.9897959184  0.9929078014  0.9803429527  0.8075471698  0.9949221395  0.9024390244  0.9973675551  0.9317507418  67.817896389  0.0000084901  2700          0.7460279679 
0.0447418532  0.2541614722  0.9905808477  1.0000000000  0.9895441238  0.8075471698  0.9969532837  0.9054878049  0.9986837776  0.9347181009  75.353218210  0.0000081402  3000          0.7459786018 
0.0298842881  0.2399981646  0.9929356358  1.0000000000  0.9891258887  0.8188679245  0.9979688558  0.9024390244  0.9986837776  0.9287833828  82.888540031  0.0000077628  3300          0.7460274927 
0.0228076066  0.2249973053  0.9905808477  0.9929078014  0.9966541196  0.8226415094  0.9986459039  0.9024390244  0.9993418888  0.9258160237  90.423861852  0.0000073613  3600          0.7451489226 
0.0164636245  0.2085679316  0.9890109890  0.9929078014  0.9970723547  0.8150943396  0.9996614760  0.9054878049  0.9996709444  0.9347181009  97.959183673  0.0000069393  3900          0.7457562478 
0.0139419868  0.1918394259  0.9890109890  1.0000000000  0.9995817650  0.8113207547  0.9996614760  0.9085365854  0.9996709444  0.9258160237  105.49450549  0.0000065005  4200          0.7461873452 
0.0113312410  0.1924219640  0.9866562009  0.9858156028  0.9995817650  0.8150943396  1.0000000000  0.9085365854  1.0000000000  0.9287833828  113.02982731  0.0000060489  4500          0.7470792389 
0.0087729433  0.1860036381  0.9874411303  0.9858156028  1.0000000000  0.8188679245  1.0000000000  0.9085365854  1.0000000000  0.9287833828  120.56514913  0.0000055883  4800          0.7463131229 
0.0075675497  0.1828666932  0.9866562009  0.9858156028  1.0000000000  0.8188679245  1.0000000000  0.9054878049  1.0000000000  0.9347181009  125.58869701  0.0000052008  5000          0.7427998412 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.001
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.2
	mlp_width: 512
	momentum: 0.1
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "cause the presence of"
Number of context words (tokens): 4
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         lr            step          step_time    
0.6533398628  1.0949343443  0.9897959184  0.9858156028  0.6030949394  0.5773584906  0.7643872715  0.7317073171  0.7525501810  0.7744807122  0.0000000000  0.0000100000  0             2.0475337505 
0.2284634430  0.5598697552  0.9984301413  1.0000000000  0.6449184442  0.6000000000  0.9041976980  0.8780487805  0.9318854886  0.9198813056  7.5353218210  0.0000099925  300           0.7392435439 
0.1376992289  0.4317801266  0.9992150706  1.0000000000  0.6695943120  0.6415094340  0.9346648612  0.9054878049  0.9552484370  0.9347181009  15.070643642  0.0000099481  600           0.7389695732 
0.0952067179  0.4160281202  1.0000000000  1.0000000000  0.6641572564  0.6264150943  0.9580230196  0.9115853659  0.9707140507  0.9287833828  22.605965463  0.0000098604  900           0.7388199512 
0.0670555501  0.4076918933  1.0000000000  1.0000000000  0.6595566708  0.6226415094  0.9722410291  0.9176829268  0.9828891083  0.9287833828  30.141287284  0.0000097299  1200          0.7403233846 
0.0493228971  0.4054309597  1.0000000000  1.0000000000  0.6700125471  0.6490566038  0.9830737982  0.9176829268  0.9884830536  0.9317507418  37.676609105  0.0000095579  1500          0.7405722499 
0.0397371596  0.3662333463  1.0000000000  1.0000000000  0.6779590130  0.6679245283  0.9895057549  0.9146341463  0.9921026654  0.9406528190  45.211930926  0.0000093458  1800          0.7402928217 
0.0264335887  0.3608063867  1.0000000000  1.0000000000  0.6708490171  0.6490566038  0.9928909953  0.9024390244  0.9963803883  0.9376854599  52.747252747  0.0000090957  2100          0.7400673493 
0.0220407825  0.3362842960  1.0000000000  0.9929078014  0.6683396069  0.6452830189  0.9969532837  0.9085365854  0.9980256663  0.9347181009  60.282574568  0.0000088096  2400          0.7410957702 
0.0138450208  0.3158062598  1.0000000000  0.9929078014  0.6750313676  0.6716981132  0.9983073798  0.9115853659  0.9980256663  0.9347181009  67.817896389  0.0000084901  2700          0.7434266504 
0.0106027333  0.2977102774  1.0000000000  0.9929078014  0.6775407779  0.6679245283  0.9996614760  0.9146341463  0.9993418888  0.9347181009  75.353218210  0.0000081402  3000          0.7413636899 
0.0089530535  0.2721649662  1.0000000000  1.0000000000  0.6767043078  0.6754716981  0.9993229519  0.9115853659  0.9996709444  0.9376854599  82.888540031  0.0000077628  3300          0.7389063795 
0.0068120236  0.2651457086  1.0000000000  0.9929078014  0.6871601840  0.6830188679  0.9993229519  0.9115853659  0.9996709444  0.9347181009  90.423861852  0.0000073613  3600          0.7417484093 
0.0062456888  0.2489194503  1.0000000000  1.0000000000  0.6796319532  0.6830188679  1.0000000000  0.9054878049  0.9996709444  0.9347181009  97.959183673  0.0000069393  3900          0.7416242464 
0.0052011872  0.2389760372  1.0000000000  1.0000000000  0.6817231284  0.6792452830  1.0000000000  0.9176829268  0.9996709444  0.9347181009  105.49450549  0.0000065005  4200          0.7417208322 
0.0048519830  0.2268483215  1.0000000000  1.0000000000  0.6825595985  0.6830188679  1.0000000000  0.9115853659  0.9996709444  0.9376854599  113.02982731  0.0000060489  4500          0.7391595705 
0.0045270418  0.2108472686  1.0000000000  1.0000000000  0.6712672522  0.6716981132  1.0000000000  0.9054878049  0.9996709444  0.9258160237  120.56514913  0.0000055883  4800          0.7401052610 
0.0044125528  0.2133866513  1.0000000000  1.0000000000  0.6821413634  0.6716981132  1.0000000000  0.9207317073  0.9996709444  0.9347181009  125.58869701  0.0000052008  5000          0.7394013143 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [2]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.001
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.2
	mlp_width: 512
	momentum: 0.1
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [2]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "cause the presence of"
Number of context words (tokens): 4
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         lr            step          step_time    
0.8284201026  1.0983463526  0.9897959184  0.9858156028  0.6223337516  0.5924528302  0.7572782668  0.7378048780  0.7525501810  0.7655786350  0.0000000000  0.0000100000  0             2.0150640011 
0.2850588927  0.2732919881  0.9992150706  1.0000000000  0.8331242158  0.8301886792  0.7968855789  0.7896341463  0.9160908193  0.9258160237  7.5353218210  0.0000099925  300           0.7398345240 
0.2104387389  0.1491886161  1.0000000000  1.0000000000  0.8419071518  0.8000000000  0.8337846987  0.7957317073  0.9460348799  0.9258160237  15.070643642  0.0000099481  600           0.7392734480 
0.1703512134  0.1398073259  1.0000000000  1.0000000000  0.8803847762  0.8264150943  0.8124576845  0.7865853659  0.9634748272  0.9436201780  22.605965463  0.0000098604  900           0.7402869701 
0.1368299159  0.1348455491  1.0000000000  1.0000000000  0.8916771225  0.7886792453  0.8080568720  0.7957317073  0.9710431063  0.9376854599  30.141287284  0.0000097299  1200          0.7395933922 
0.1008775206  0.1297358309  1.0000000000  1.0000000000  0.9289000418  0.8264150943  0.8300609343  0.8140243902  0.9868377756  0.9347181009  37.676609105  0.0000095579  1500          0.7405886889 
0.0770913816  0.1270814812  1.0000000000  1.0000000000  0.9305729820  0.8075471698  0.8209207854  0.8048780488  0.9930898322  0.9376854599  45.211930926  0.0000093458  1800          0.7382545416 
0.0549646849  0.1200377425  1.0000000000  1.0000000000  0.9740694270  0.8339622642  0.8195666892  0.8140243902  0.9973675551  0.9376854599  52.747252747  0.0000090957  2100          0.7384821041 
0.0412463284  0.1074658313  1.0000000000  1.0000000000  0.9874529486  0.8188679245  0.8168584970  0.8170731707  0.9986837776  0.9406528190  60.282574568  0.0000088096  2400          0.7403139257 
0.0270580518  0.1034626015  1.0000000000  1.0000000000  0.9928900042  0.8037735849  0.8165199729  0.8048780488  0.9986837776  0.9347181009  67.817896389  0.0000084901  2700          0.7410900402 
0.0217934494  0.0989732347  1.0000000000  1.0000000000  0.9958176495  0.8301886792  0.8219363575  0.8048780488  0.9993418888  0.9287833828  75.353218210  0.0000081402  3000          0.7395135983 
0.0181449744  0.1013454954  1.0000000000  1.0000000000  0.9983270598  0.8113207547  0.8178740691  0.7926829268  0.9996709444  0.9347181009  82.888540031  0.0000077628  3300          0.7403907267 
0.0139079039  0.0903074522  1.0000000000  1.0000000000  0.9995817650  0.8113207547  0.8094109682  0.7865853659  1.0000000000  0.9347181009  90.423861852  0.0000073613  3600          0.7409293445 
0.0101022027  0.0892139835  1.0000000000  1.0000000000  0.9995817650  0.8150943396  0.8117806364  0.7804878049  1.0000000000  0.9287833828  97.959183673  0.0000069393  3900          0.7405499403 
0.0076354357  0.0843901029  1.0000000000  1.0000000000  1.0000000000  0.8150943396  0.8127962085  0.7896341463  1.0000000000  0.9347181009  105.49450549  0.0000065005  4200          0.7399378681 
0.0057275517  0.0784393991  1.0000000000  1.0000000000  1.0000000000  0.8150943396  0.8107650643  0.7835365854  1.0000000000  0.9465875371  113.02982731  0.0000060489  4500          0.7407245239 
0.0050700983  0.0706452099  1.0000000000  1.0000000000  1.0000000000  0.8150943396  0.8165199729  0.7957317073  1.0000000000  0.9376854599  120.56514913  0.0000055883  4800          0.7418004402 
0.0046860254  0.0739101927  1.0000000000  1.0000000000  1.0000000000  0.8226415094  0.8131347326  0.7835365854  1.0000000000  0.9406528190  125.58869701  0.0000052008  5000          0.7438338876 
./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: APLCLIP
	checkpoint_freq: None
	data_dir: /home/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.1
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [3]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.001
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.2
	mlp_width: 512
	momentum: 0.1
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [3]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
Set self.clip_model.parameters.reguires_grad = False!
Set self.clip_model.parameters.reguires_grad = False!
Initial context: "cause the presence of"
Number of context words (tokens): 4
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         lr            step          step_time    
0.9080697894  1.0995012522  0.9897959184  0.9858156028  0.6206608114  0.5962264151  0.7582938389  0.7378048780  0.7459690688  0.7596439169  0.0000000000  0.0000100000  0             2.0927128792 
0.3214036962  0.3590835517  0.9992150706  1.0000000000  0.8301965705  0.8075471698  0.8916723087  0.8902439024  0.8729845344  0.8961424332  7.5353218210  0.0000099925  300           0.7422382736 
0.2323838900  0.2521128075  1.0000000000  1.0000000000  0.8444165621  0.8000000000  0.9204468517  0.9024390244  0.8716683119  0.8991097923  15.070643642  0.0000099481  600           0.7434479825 
0.1893936205  0.2395781911  1.0000000000  1.0000000000  0.8678377248  0.8150943396  0.9441435342  0.9024390244  0.8552155314  0.8842729970  22.605965463  0.0000098604  900           0.7440119918 
0.1562950320  0.2091240496  1.0000000000  1.0000000000  0.8900041824  0.8188679245  0.9607312119  0.9115853659  0.8562026983  0.8902077151  30.141287284  0.0000097299  1200          0.7435224660 
0.1176665201  0.1916986379  1.0000000000  1.0000000000  0.9251359264  0.8301886792  0.9739336493  0.8963414634  0.8634419217  0.8991097923  37.676609105  0.0000095579  1500          0.7439215358 
0.0886995615  0.1752360576  1.0000000000  1.0000000000  0.9477206190  0.8188679245  0.9851049425  0.8871951220  0.8588351431  0.8931750742  45.211930926  0.0000093458  1800          0.7431738146 
0.0652926575  0.1593931104  1.0000000000  1.0000000000  0.9694688415  0.8075471698  0.9925524712  0.9054878049  0.8562026983  0.8931750742  52.747252747  0.0000090957  2100          0.7409732262 
0.0473447613  0.1506991893  1.0000000000  1.0000000000  0.9769970724  0.8037735849  0.9949221395  0.8993902439  0.8562026983  0.8902077151  60.282574568  0.0000088096  2400          0.7418261933 
0.0358874462  0.1573123168  1.0000000000  1.0000000000  0.9878711836  0.8113207547  0.9959377116  0.8871951220  0.8598223100  0.8872403561  67.817896389  0.0000084901  2700          0.7415964739 
0.0256021022  0.1350741730  1.0000000000  1.0000000000  0.9803429527  0.7962264151  0.9986459039  0.9024390244  0.8443566963  0.8813056380  75.353218210  0.0000081402  3000          0.7417643460 
0.0187570279  0.1158003378  1.0000000000  1.0000000000  0.9949811794  0.7924528302  0.9986459039  0.9085365854  0.8466600856  0.8842729970  82.888540031  0.0000077628  3300          0.7404502892 
0.0129612357  0.1166514587  1.0000000000  1.0000000000  0.9991635299  0.8113207547  0.9996614760  0.9024390244  0.8453438631  0.8783382789  90.423861852  0.0000073613  3600          0.7394519567 
0.0097034271  0.1106147939  1.0000000000  1.0000000000  0.9987452949  0.8113207547  0.9989844279  0.9054878049  0.8486344192  0.8724035608  97.959183673  0.0000069393  3900          0.7415963626 
0.0081608608  0.1057496260  1.0000000000  1.0000000000  1.0000000000  0.8037735849  0.9996614760  0.9054878049  0.8492925304  0.8724035608  105.49450549  0.0000065005  4200          0.7413495318 
0.0071485838  0.0999611080  1.0000000000  1.0000000000  1.0000000000  0.8075471698  1.0000000000  0.9024390244  0.8463310299  0.8753709199  113.02982731  0.0000060489  4500          0.7413385177 
0.0058795379  0.0936304051  1.0000000000  1.0000000000  1.0000000000  0.7962264151  1.0000000000  0.9054878049  0.8433695295  0.8724035608  120.56514913  0.0000055883  4800          0.7430592481 
0.0055848562  0.0891378447  1.0000000000  1.0000000000  1.0000000000  0.8075471698  1.0000000000  0.8993902439  0.8440276407  0.8724035608  125.58869701  0.0000052008  5000          0.7415955079 
