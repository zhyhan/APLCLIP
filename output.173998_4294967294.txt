./logs
Environment:
	Python: 3.11.3
	PyTorch: 2.0.0+cu117
	Torchvision: 0.15.1+cu117
	CUDA: 11.7
	CUDNN: 8500
	NumPy: 1.24.3
	PIL: 9.5.0
Args:
	algorithm: CMSAN
	checkpoint_freq: None
	data_dir: /l/users/zhongyi.han/dataset
	dataset: VLCS
	holdout_fraction: 0.2
	hparams: {"clip_backbone": "ViT-B/16"}
	hparams_seed: 0
	output_dir: ./logs
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [1]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	backbone: clip
	batch_size: 32
	beta1: 0.5
	class_balanced: False
	clip_backbone: ViT-B/16
	clip_transform: True
	d_steps_per_g_step: 1
	data_augmentation: True
	grad_penalty: 0.0
	lambda: 1.0
	lr: 0.001
	lr_d: 0.01
	lr_g: 1e-05
	mlp_depth: 3
	mlp_dropout: 0.0
	mlp_width: 512
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	test_envs: [1]
	weight_decay: 0.0
	weight_decay_d: 0.0
	weight_decay_g: 0.0
Using clip_transform ViT-B/16
/home/zhongyi.han/.conda/envs/CLIP/lib/python3.11/site-packages/torch/nn/functional.py:2919: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
class_loss    disc_loss     env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         kl_loss       step          step_time    
0.3369660676  1.1775075197  0.9982332155  1.0000000000  0.5980207352  0.6000000000  0.7711348058  0.7484756098  0.8333950389  0.8637037037  0.0000000000  -0.226553231  0             2.2688999176 
0.0889951124  0.4977653934  1.0000000000  1.0000000000  0.6753063148  0.6849056604  0.9832444783  0.8810975610  0.9907441688  0.9511111111  8.4805653710  -0.202181839  300           1.1997676261 
0.0269651557  0.9775895802  1.0000000000  1.0000000000  0.6720075401  0.6849056604  0.9950495050  0.8902439024  0.9970381340  0.9466666667  16.961130742  -0.188097885  600           1.2000600505 
0.0164466414  1.0420753751  1.0000000000  1.0000000000  0.6710650330  0.6849056604  0.9980959634  0.8963414634  0.9981488338  0.9466666667  25.441696113  -0.183717057  900           1.1998563512 
0.0130747421  1.0778334616  1.0000000000  1.0000000000  0.6701225259  0.6773584906  0.9996191927  0.8963414634  0.9988893003  0.9407407407  33.922261484  -0.174666658  1200          1.2004164267 
0.0106993373  1.0857262069  1.0000000000  1.0000000000  0.6729500471  0.6773584906  1.0000000000  0.8978658537  0.9992595335  0.9466666667  42.402826855  -0.173409506  1500          1.2017267211 
0.0110692499  1.0940103002  1.0000000000  1.0000000000  0.6729500471  0.6735849057  1.0000000000  0.9009146341  0.9996297668  0.9496296296  50.883392226  -0.172107505  1800          1.2009546216 
0.0075400721  1.1008109544  1.0000000000  1.0000000000  0.6724787936  0.6735849057  1.0000000000  0.8978658537  0.9996297668  0.9466666667  59.363957597  -0.167426921  2100          1.2006925138 
0.0067618558  1.1004549948  1.0000000000  1.0000000000  0.6691800189  0.6716981132  1.0000000000  0.8963414634  1.0000000000  0.9451851852  67.844522968  -0.168817414  2400          1.2010641638 
0.0062111278  1.1068151422  1.0000000000  1.0000000000  0.6691800189  0.6698113208  1.0000000000  0.8948170732  1.0000000000  0.9451851852  76.325088339  -0.171256601  2700          1.2026359224 
0.0045196281  1.1037719023  1.0000000000  1.0000000000  0.6682375118  0.6716981132  1.0000000000  0.8948170732  1.0000000000  0.9451851852  84.805653710  -0.170606231  3000          1.2011298259 
0.0058656357  1.1017139276  1.0000000000  1.0000000000  0.6677662582  0.6716981132  1.0000000000  0.8917682927  1.0000000000  0.9451851852  93.286219081  -0.172517011  3300          1.2015870897 
0.0060546260  1.1002011514  1.0000000000  1.0000000000  0.6663524976  0.6735849057  1.0000000000  0.8902439024  1.0000000000  0.9466666667  101.76678445  -0.173748810  3600          1.2013553111 
0.0069228061  1.1027401145  1.0000000000  1.0000000000  0.6658812441  0.6735849057  1.0000000000  0.8887195122  1.0000000000  0.9466666667  110.24734982  -0.174933691  3900          1.2020848854 
0.0065528240  1.0994095234  1.0000000000  1.0000000000  0.6654099906  0.6735849057  1.0000000000  0.8871951220  1.0000000000  0.9481481481  118.72791519  -0.175594300  4200          1.2020223546 
0.0063849825  1.1044489543  1.0000000000  1.0000000000  0.6639962300  0.6735849057  1.0000000000  0.8902439024  1.0000000000  0.9496296296  127.20848056  -0.176546011  4500          1.2016292389 
0.0064270860  1.1008156701  1.0000000000  1.0000000000  0.6649387370  0.6698113208  1.0000000000  0.8871951220  1.0000000000  0.9437037037  135.68904593  -0.175803651  4800          1.2025027331 
0.0077767009  1.0998313099  1.0000000000  1.0000000000  0.6663524976  0.6698113208  1.0000000000  0.8887195122  1.0000000000  0.9422222222  141.34275618  -0.175762999  5000          1.2027776599 
